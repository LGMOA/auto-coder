
# Tokens 模块

高效统计文件和目录中的 token 数量，支持正则过滤和智能文件类型识别。

## 目录结构

```
src/autocoder/common/tokens/
├── __init__.py               # 模块入口，导出主要接口
├── counter.py                # 核心 token 计数器实现
├── file_detector.py          # 文件类型检测器
├── models.py                 # 数据模型定义
├── filters.py                # 文件过滤器实现
└── .ac.mod.md                # 本文档
```

## 快速开始

### 基本使用（设计示例）

```python
# 导入模块（设计示例）
from src.autocoder.common.tokens import TokenCounter, count_file_tokens, count_directory_tokens

# 1. 统计单个文件
result = count_file_tokens("path/to/file.py")
print(f"文件: {result.file_path}")
print(f"Token 数量: {result.token_count}")
print(f"字符数: {result.char_count}")
print(f"行数: {result.line_count}")

# 2. 统计整个目录
dir_result = count_directory_tokens(
    "path/to/directory",
    pattern=r".*\.py$"  # 只统计 Python 文件
)
print(f"总 Token 数: {dir_result.total_tokens}")
print(f"文件数量: {dir_result.file_count}")
print(f"跳过文件数: {dir_result.skipped_count}")

# 3. 使用 TokenCounter 类进行更多控制
counter = TokenCounter(
    tokenizer="cl100k_base",  # 指定 tokenizer
    encoding="utf-8",          # 指定编码
    chunk_size=1024*1024       # 1MB 块大小，用于大文件
)

# 批量处理
files = ["file1.py", "file2.js", "file3.md"]
results = counter.count_files(files)
for result in results:
    if result.success:
        print(f"{result.file_path}: {result.token_count} tokens")
    else:
        print(f"{result.file_path}: 失败 - {result.error}")
```

### 配置选项

```python
# TokenCounter 配置（设计示例）
config = {
    "tokenizer": "cl100k_base",     # tiktoken 编码器名称
    "encoding": "utf-8",             # 文件编码
    "chunk_size": 1024 * 1024,       # 大文件分块大小
    "max_file_size": 100 * 1024 * 1024,  # 最大文件大小限制 (100MB)
    "timeout": 30,                   # 单文件处理超时时间（秒）
    "parallel": True,                # 是否并行处理
    "max_workers": 4,                # 最大工作线程数
}

counter = TokenCounter(**config)
```

## 核心组件

### 1. TokenCounter

**功能：**
- **文件统计**：统计单个文件的 token 数量
- **目录统计**：递归统计目录下所有文件
- **批量处理**：高效处理多个文件
- **内存优化**：支持大文件分块处理

**主要方法：**
- `count_file(file_path: str) -> TokenResult`: 统计单个文件
- `count_directory(dir_path: str, pattern: str = None) -> DirectoryTokenResult`: 统计目录
- `count_files(file_paths: List[str]) -> List[TokenResult]`: 批量统计
- `set_tokenizer(tokenizer_name: str)`: 更改 tokenizer

### 2. FileTypeDetector

**功能：**
- **MIME 类型检测**：基于文件内容判断类型
- **扩展名匹配**：基于文件扩展名快速判断
- **内容分析**：通过文件头部字节识别二进制文件
- **编码检测**：自动检测文本文件编码

**主要方法：**
- `is_text_file(file_path: str) -> bool`: 判断是否为文本文件
- `detect_encoding(file_path: str) -> str`: 检测文件编码
- `get_mime_type(file_path: str) -> str`: 获取 MIME 类型

### 3. FileFilter

**功能：**
- **正则过滤**：支持正则表达式匹配文件名
- **类型过滤**：按文件类型过滤（文本/二进制）
- **大小过滤**：按文件大小范围过滤
- **组合过滤**：支持多个过滤条件组合

**主要方法：**
- `matches(file_path: str) -> bool`: 检查文件是否匹配过滤条件
- `add_pattern(pattern: str)`: 添加正则表达式模式
- `set_size_range(min_size: int, max_size: int)`: 设置大小范围

## 架构设计

```mermaid
graph TB
    %% 用户接口层
    API[公共 API<br/>count_file_tokens()<br/>count_directory_tokens()]
    
    %% 核心层
    Counter[TokenCounter<br/>核心计数逻辑]
    Detector[FileTypeDetector<br/>文件类型检测]
    Filter[FileFilter<br/>文件过滤器]
    
    %% 数据模型层
    Models[数据模型<br/>TokenResult<br/>DirectoryTokenResult]
    
    %% 外部依赖
    Tiktoken[tiktoken<br/>Token 编码器]
    FileSystem[文件系统<br/>pathlib/os]
    
    %% 依赖关系
    API --> Counter
    Counter --> Detector
    Counter --> Filter
    Counter --> Models
    Counter --> Tiktoken
    Detector --> FileSystem
    Filter --> FileSystem
    
    %% 样式
    classDef coreClass fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef modelClass fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef externalClass fill:#fff3e0,stroke:#ef6c00,stroke-width:2px
    
    class Counter,Detector,Filter coreClass
    class Models modelClass
    class Tiktoken,FileSystem externalClass
```

### 数据流

```mermaid
sequenceDiagram
    participant User as 用户
    participant API as API 接口
    participant Counter as TokenCounter
    participant Detector as FileTypeDetector
    participant Filter as FileFilter
    participant Tokenizer as Tiktoken
    
    User->>API: count_directory_tokens(path, pattern)
    API->>Counter: 创建计数器实例
    API->>Filter: 创建过滤器(pattern)
    
    loop 遍历目录文件
        Counter->>Filter: 检查文件是否匹配
        alt 文件匹配
            Counter->>Detector: 检测文件类型
            alt 是文本文件
                Counter->>Tokenizer: 编码并计数
                Tokenizer-->>Counter: 返回 token 数量
            else 非文本文件
                Counter->>Counter: 跳过文件
            end
        end
    end
    
    Counter-->>API: 返回统计结果
    API-->>User: DirectoryTokenResult
```

### 依赖说明

**内部依赖：**
- `models.py` ← `counter.py` ← `__init__.py`
- `file_detector.py` ← `counter.py`
- `filters.py` ← `counter.py`

**外部依赖：**
- `tiktoken`: OpenAI 的 token 计数库
- `pathlib`: Python 标准库，用于路径操作
- `re`: Python 标准库，用于正则表达式
- `mimetypes`: Python 标准库，用于 MIME 类型检测
- `chardet` (可选): 用于更准确的编码检测

## 实现指导

### 开发步骤

1. **实现文件类型检测器**
   - 创建 `file_detector.py`
   - 实现基于文件头的二进制检测
   - 实现基于扩展名的快速判断
   - 添加编码检测功能

2. **实现数据模型**
   - 创建 `models.py`
   - 定义 TokenResult 数据类
   - 定义 DirectoryTokenResult 数据类
   - 实现结果序列化方法

3. **实现文件过滤器**
   - 创建 `filters.py`
   - 实现正则表达式匹配
   - 实现文件大小过滤
   - 支持过滤器组合

4. **实现核心计数器**
   - 创建 `counter.py`
   - 实现单文件 token 计数
   - 实现大文件分块处理
   - 实现目录递归统计
   - 添加并行处理支持

5. **创建公共接口**
   - 更新 `__init__.py`
   - 导出便捷函数
   - 提供默认配置

### 技术要点

**性能优化：**
- 使用生成器处理大目录，避免内存溢出
- 实现文件分块读取，支持超大文件
- 使用线程池并行处理多个文件
- 缓存 tokenizer 实例，避免重复创建

**错误处理：**
- 文件权限错误：跳过并记录
- 编码错误：尝试多种编码或跳过
- 内存错误：自动降级到分块处理
- 超时处理：设置单文件处理超时

**扩展性考虑：**
- 支持自定义 tokenizer 接口
- 支持插件式文件类型检测器
- 预留钩子函数接口
- 支持自定义统计指标

### 测试策略

**单元测试：**
```python
# 测试示例（设计）
def test_single_file_count():
    """测试单文件 token 计数"""
    result = count_file_tokens("test.txt")
    assert result.success
    assert result.token_count > 0

def test_binary_file_detection():
    """测试二进制文件检测"""
    detector = FileTypeDetector()
    assert not detector.is_text_file("image.jpg")
    assert detector.is_text_file("script.py")

def test_pattern_filter():
    """测试正则过滤器"""
    filter = FileFilter(pattern=r".*\.py$")
    assert filter.matches("test.py")
    assert not filter.matches("test.js")
```

**集成测试：**
```python
# 集成测试示例（设计）
def test_directory_count_with_filter():
    """测试目录统计与过滤"""
    result = count_directory_tokens(
        "test_directory",
        pattern=r".*\.(py|js|md)$"
    )
    assert result.file_count > 0
    assert result.skipped_count >= 0
    assert result.total_tokens > 0
```

**性能测试：**
```python
# 性能测试示例（设计）
def test_large_file_performance():
    """测试大文件处理性能"""
    # 创建 100MB 测试文件
    large_file = create_test_file(size=100*1024*1024)
    
    start_time = time.time()
    result = count_file_tokens(large_file)
    elapsed = time.time() - start_time
    
    assert result.success
    assert elapsed < 10  # 应在 10 秒内完成
```

## 使用场景

### 1. 项目代码统计
```python
# 统计整个项目的 token 使用量（设计示例）
result = count_directory_tokens(
    "src/",
    pattern=r".*\.(py|js|ts|jsx|tsx)$"
)
print(f"项目总 Token 数: {result.total_tokens:,}")
print(f"平均每文件: {result.total_tokens / result.file_count:,.0f}")
```

### 2. API 成本估算
```python
# 估算 GPT API 调用成本（设计示例）
def estimate_api_cost(directory: str, cost_per_1k_tokens: float = 0.002):
    result = count_directory_tokens(directory)
    total_cost = (result.total_tokens / 1000) * cost_per_1k_tokens
    return {
        "total_tokens": result.total_tokens,
        "estimated_cost": f"${total_cost:.2f}",
        "file_count": result.file_count
    }
```

### 3. 文档长度控制
```python
# 检查文档是否超过 token 限制（设计示例）
def check_document_limits(file_path: str, max_tokens: int = 4000):
    result = count_file_tokens(file_path)
    if result.token_count > max_tokens:
        print(f"警告: 文件超过限制 ({result.token_count} > {max_tokens})")
        return False
    return True
```

## 注意事项

1. **文件编码**：默认使用 UTF-8，但会自动检测其他编码
2. **内存使用**：大文件会自动分块处理，避免内存溢出
3. **性能考虑**：目录统计支持并行处理，可通过 `max_workers` 参数调整
4. **错误处理**：遇到无法读取的文件会跳过并记录在结果中
5. **Token 计算**：不同的 tokenizer 可能产生不同的结果，确保使用一致的 tokenizer

## 未来扩展

- 支持更多 tokenizer（如 Hugging Face tokenizers）
- 添加增量统计功能（只统计变更的文件）
- 支持远程文件统计（通过 URL）
- 添加可视化统计报告生成
- 支持自定义文件类型检测规则
- 添加 token 使用量历史记录功能
