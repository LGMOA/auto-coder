{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-24 16:28:56.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbyzerllm.utils.connect_ray\u001b[0m:\u001b[36mconnect_cluster\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mJDK 21 will be used (/Users/allwefantasy/.auto-coder/jdk-21.0.2.jdk/Contents/Home)...\u001b[0m\n",
      "2024-05-24 16:28:56,095\tINFO worker.py:1564 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379...\n",
      "2024-05-24 16:28:56,112\tINFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<byzerllm.utils.client.byzerllm_client.ByzerLLM at 0x1112a3760>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autocoder.common.anything2images import Anything2Images\n",
    "from autocoder.common import AutoCoderArgs\n",
    "import byzerllm\n",
    "\n",
    "byzerllm.connect_cluster()\n",
    "llm = byzerllm.ByzerLLM()\n",
    "llm.setup_default_model_name(\"deepseek_chat\")\n",
    "llm.setup_template(\"deepseek_chat\",\"auto\")\n",
    "\n",
    "model = \"qianwen_vl_max_chat\"\n",
    "model = \"gpt4o_chat\"\n",
    "vl_model = byzerllm.ByzerLLM()\n",
    "vl_model.setup_default_model_name(model)\n",
    "vl_model.setup_template(model,\"auto\")\n",
    "\n",
    "llm.setup_sub_client(\"vl_model\",vl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMResponse(output='在一个宁静的小镇上，有一个古老的钟楼，每当夜幕降临，钟楼里就会传出奇异的钟声。镇上的孩子们都相信，那是时间守护者在召唤着他们。一天，勇敢的小明决定探索钟楼的秘密。他爬上蜿蜒的楼梯，到达了钟楼的最顶层。在那里，他发现了一个闪', input={'instruction': '接着前面的内容继续', 'history': [{'role': 'user', 'content': '\\n将一个100字的故事。\\n输出的内容请以 \"<MARKER></MARKER> 标签对包裹。'}, {'role': 'assistant', 'content': '<MARKER>在一个宁静的小镇上'}, {'role': 'user', 'content': '接着前面的内容继续'}, {'role': 'assistant', 'content': '，有一个古老的钟楼，每当夜'}, {'role': 'user', 'content': '接着前面的内容继续'}, {'role': 'assistant', 'content': '幕降临，钟楼里就会传出'}, {'role': 'user', 'content': '接着前面的内容继续'}, {'role': 'assistant', 'content': '奇异的钟声。镇上的孩子们'}, {'role': 'user', 'content': '接着前面的内容继续'}, {'role': 'assistant', 'content': '都相信，那是时间守护者在召'}, {'role': 'user', 'content': '接着前面的内容继续'}, {'role': 'assistant', 'content': '唤着他们。一天，勇敢的小'}, {'role': 'user', 'content': '接着前面的内容继续'}, {'role': 'assistant', 'content': '明决定探索钟楼的秘密。他'}, {'role': 'user', 'content': '接着前面的内容继续'}, {'role': 'assistant', 'content': '爬上蜿蜒的楼梯'}, {'role': 'user', 'content': '接着前面的内容继续'}, {'role': 'assistant', 'content': '，到达了钟楼的最顶层'}], 'max_length': 10}, metadata={'request_id': '10b6d90873942b74368a368e079c2521', 'input_tokens_count': 224, 'generated_tokens_count': 8, 'time_cost': 0.9898069869959727, 'first_token_time': 0, 'speed': 8.082383843621574})\n",
      "<MARKER>在一个宁静的小镇上，有一个古老的钟楼，每当夜幕降临，钟楼里就会传出奇异的钟声。镇上的孩子们都相信，那是时间守护者在召唤着他们。一天，勇敢的小明决定探索钟楼的秘密。他爬上蜿蜒的楼梯，到达了钟楼的最顶层。在那里，他发现了一个闪闪\n"
     ]
    }
   ],
   "source": [
    "@byzerllm.prompt()\n",
    "def tt()->str:\n",
    "    '''\n",
    "    将一个100字的故事。\n",
    "    输出的内容请以 \"<MARKER></MARKER> 标签对包裹。\n",
    "    '''\n",
    "\n",
    "s = tt.with_llm(llm).with_response_markers([\"<MARKER>\",\"</MARKER>\"]).options({\"llm_config\":{\"max_length\":10}}).run()\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LLMResponse(output='你好！有什么可以帮助你的吗？', input={'instruction': '你好', 'history': [{'role': 'user', 'content': '你好'}, {'role': 'assistant', 'content': '你好'}]}, metadata={'request_id': '2ab3623dd5c1b820471774af96a2a733', 'input_tokens_count': 20, 'generated_tokens_count': 7, 'time_cost': 1.9897276129340753, 'first_token_time': 0, 'speed': 3.518069485741176})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = llm.chat_oai(conversations=[\n",
    "    {\n",
    "    \"role\":\"user\",\n",
    "    \"content\": \"你好\"\n",
    "},\n",
    "  {\n",
    "    \"role\":\"assistant\",\n",
    "    \"content\": \"你好\"\n",
    "},\n",
    "    {\n",
    "    \"role\":\"user\",\n",
    "    \"content\": \"你好\"\n",
    "}])\n",
    "\n",
    "v[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nxxxx1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = '''\n",
    "xxx0\n",
    "<start>\n",
    "xxxx1<end>\n",
    "ttt\n",
    "'''\n",
    "start = \"<start>\"\n",
    "end = \"<end>\"\n",
    "start_index = output.find(start)\n",
    "end_index = output.find(end)\n",
    "output[start_index+len(start):end_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byzerllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
