{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/012-AutoCoder如何保障auto-merge下的代码安全.md", "relative_path": "012-AutoCoder如何保障auto-merge下的代码安全.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/012-AutoCoder如何保障auto-merge下的代码安全.md", "source_code": "# 012-AutoCoder如何保障auto_merge模式下的代码安全\n\n代码安全实际有两部分，一部分是代码不能被泄露，这个可以通过 [010-AutoCoder 如何在公司级别使用](./010-AutoCoder%20%E5%A6%82%E4%BD%95%E5%9C%A8%E5%85%AC%E5%8F%B8%E7%BA%A7%E5%88%AB%E4%BD%BF%E7%94%A8.md)\n来解决。\n\n另一部分是如果开启了 auto_merge 模式，因为模型大模型的不确定性，很有可能破坏用户已有代码。比如用户可能忘了提交代码，马上又运行 AutoCoder ，并且开启了 auto_merge，很可能就覆盖掉了用户的代码。\n\n所以我们需要一些措施来保障代码的安全，这里我们鼓励大家在开启 auto_merge 模式的时候，务必保证你的代码被 git 管控。\n如果是一个被git 管控的项目，那么 AutoCoder 会通过 git 来保证代码的安全：\n\n1. 在修改代码之前，我们会执行一次 commit 操作，确保用户原有的代码被保存，message消息类似 auto_coder_pre_文件名_文件md5。\n2. 在修改之后，我们会执行一次 commit 操作，确保留下这个commit后，可以随时进行回滚，对应的commit message消息是 “auto_coder_文件名_文件md5”。\n\n我们来举个例子。\n\n我们随意修改 AutoCoder 的一个文件，比如 git_utils.py ，我们在这个文件中新增一个方法叫：echo。\n注意，这里我们开启了 execute/auto_merge 模式。\n```yml\nsource_dir: /home/winubuntu/projects/ByzerRawCopilot \ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nmodel: deepseek_chat\nmodel_max_length: 2000\nmodel_max_input_length: 30000\nanti_quota_limit: 0\n\nskip_build_index: false\nexecute: true\nauto_merge: true\n\nquery: |\n   在 git_utils.py 中新增一个方法叫：echo,可以随意发挥。\n```\n\n我们执行下:\n\n```bash\nauto-coder --file ./examples/003_test_revert.yml \n```        \n\n用户看日志输出，应该看到两次 commit 提交：\n\n```shell\n2024-03-26 09:58:51.905 | WARNING  | autocoder.index.index:get_target_files_by_query:184 - Fail to find targed files for chunk 5. this may be caused by the model limit or the query is not suitable for the files.\n2024-03-26 09:58:56.908 | INFO     | autocoder.index.index:build_index_and_filter_files:213 - Target File: /home/winubuntu/projects/ByzerRawCopilot/src/git_utils.py reason: 由于用户请求在该文件中新增一个名为 'echo' 的方法，因此该文件是目标文件。\n2024-03-26 09:59:49.503 | INFO     | autocoder.dispacher.actions.action:process_content:238 - Auto merge the code...\n2024-03-26 09:59:49.513 | INFO     | autocoder.common.git_utils:commit_changes:21 - Committed changes with message: auto_coder_pre_003_test_revert.yml_dca749b7ffddde8136a334a627221d3a\n2024-03-26 09:59:49.513 | INFO     | autocoder.common.code_auto_merge:merge_code:59 - Upsert path: ./git_utils.py\n2024-03-26 09:59:49.513 | INFO     | autocoder.common.code_auto_merge:merge_code:63 - Merged 1 files into the project.\n2024-03-26 09:59:49.521 | INFO     | autocoder.common.git_utils:commit_changes:21 - Committed changes with message: auto_coder_003_test_revert.yml_dca749b7ffddde8136a334a627221d3a\n```\n\n可以系统自动做了两次提交。你可以通过 git 查看：\n\n```shell\ncommit 9813a6e5e52d92bb6151e0473f0abb7d4b8ddc71 (HEAD -> master)\nAuthor: WilliamZhu <allwefantasy@gmail.com>\nDate:   Tue Mar 26 09:59:49 2024 +0800\n\n    auto_coder_003_test_revert.yml_dca749b7ffddde8136a334a627221d3a\n\ncommit 5aef905e14759aa0b513f493c5a879c0b921447f\nAuthor: WilliamZhu <allwefantasy@gmail.com>\nDate:   Tue Mar 26 09:59:49 2024 +0800\n\n    auto_coder_pre_003_test_revert.yml_dca749b7ffddde8136a334a627221d3a\n```\n\n如果你发现代码有问题，你可以使用 auto-coder 进行回滚：\n\n```shell\nauto-coder revert --file ./examples/003_test_revert.yml\n```\n\n或者通过git 命令自己手动回滚即可。\n\n\n\n\n\n\n\n\n", "tag": "", "tokens": 1129, "metadata": {}}], "modify_time": 1716937503.7342753, "md5": "21ba3a8dc467354cc1d6a320e28fa712"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/036-AutoCoder_编码_prompt实践_1.md", "relative_path": "036-AutoCoder_编码_prompt实践_1.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/036-AutoCoder_编码_prompt实践_1.md", "source_code": "# 036-AutoCoder_编码prompt实践_1\n\n在使用 AutoCoder 编程中，其实存在两个大的阶段：\n\n1. 根据你的需求查找文件。\n2. 把这些文件作为上下文，然后结合需求，生成代码然后合并到源文件中。\n\n今天我们重点讲讲，如何让大模型找到你的文件并且应该如何描述你的需求从而达到最佳的生成效果。\n\n## 如何让 AutoCoder 更好的找到需要的代码文件\n\n第一阶段要求我们至少能找到：\n\n1. 要修改的文件\n2. 为了能够修改这些文件，我们还需要依赖哪些其他文件。\n\n这个时候我们在写需求的时候，就有些技巧了，下面是一个来自AutoCoder 的实际案例：\n\n```yaml\ninclude_file: \n   - ./common/diff.yml\n\nquery: |   \n   在 code_auto_generate 开头的文件里，我们需要检测下 llm 是不是有个 code_model 值存在。参考 \n   rest.py 中检测 vl_model 的方式。如果 code_model存在，那么我们需要使用 code_model 而不是模型的llm 来处理。\n   \n```\n\n这个其实是我蛮得意的一个写法。简单的一两行字里，我们提供了三个信息：\n\n1. 需要修改的文件是什么\n2. 修改逻辑是什么\n3. 具体的修改范例是什么\n\n一般而言，我们在实际使用过程中，1 是一定要提及的。 2，3 中的3是可选的。 如果2 写的够详细，那么就不需要3。 如果你的修改是有返利的，最好就能\n告诉AutoCoder 这个已经写过的逻辑是什么。\n\n从这个例子中，我们可以可以看到 AutoCoder 的高效率，我们只要描述上面三个点，就可以在无需我们自己打开一个代码文件的情况，自动精准的完成\n所有的代码修改。\n\n### 需要修改的文件是什么\n\n这里我们们用了几个技巧：\n\n1. 显式的告诉 AutoCoder 我们需要的文件的名字是 `code_auto_generate` 开头的文件里，这样正常情况下 AutoCoder 就能找到这几个需要修改的文件了，这里应该是 `code_auto_generate.py`, `code_auto_generat_diff.py` 以及 `code_auto_generate_strict_diff.py` 这几个文件。\n2. 你可以通过提及文件部分路径来更精确的定位，比如 `common/__init__.py` 这样的提法。\n3. 你可以通过函数名来定位，然后辅助一些代码来定位。函数名可以帮你快速找到文件，辅助代码可以让你更清晰的控制修改的范围。这个对于前端特别有用，比如我希望在那个form input 后面再加一个input框，这样就可以非常精准新增一个输入框了。\n\n此外，你还可以专门提某个文件，并且显示说，需要把这个文件依赖到的文件都找出来（能不能最后找出来就看大模型的智能程度了）。\n当然最后依然可能会遇到难处，就是你怎么说，AutoCoder就是没get到你的意思，这个时候你可以显示的写路径或者通过 `urls` 参数来指定文件路径，确保该文件一定\n被引用。\n\n### 你的需求（修改逻辑）应该如何描述\n\n修改逻辑一般通过三种方式来表达：\n\n1. 给定一个比较大概的修改描述，但是具体实现细节有以前的代码可以参考。\n2. 给定详细的修改逻辑，手把手教。\n3. 给定一个目标，然后简单描述一下，让 AutoCoder 自己去实现。\n\n此外，对于第二种情况，如果你自己一时讲不清楚，你也可以分多个步骤来迭代，每次改一点，最终通过多次来完成最后的业务逻辑实现。前面我给的案例，就是\n一个经典的 1 的方案，告诉你逻辑，以及参考例子，大模型很精确的完成了需求。\n\n对于第二种情况，我们另外一个实际的例子：\n\n```yaml\nenable_rag_search: | \n   byzerllm  使用 openai_tts模型的 python 代码\n\nquery: | \n   我们要在 audio.py 中实现一个新的类叫 PlayStreamAudioFromText，\n   该类有一个方法 run,\n   该方法输入是一个字符串generator，在方法内部会将文本转换为语音，并且播放出来。\n   \n   具体逻辑是：\n   1. PlayStreamAudioFromText 维护一个queue，一个线程池\n   1. 运行时，从generator中读取文本，然后将文本放入queue中\n   2. 从queue中取出文本，按中英文句号或者换行符对语句进行切割调用，\n      并行调用 openai_tts 模型将文本转换为语音，保存在 /tmp/wavs 目录下。\n      音频文件用 001.wav, 002.wav, 003.wav...的命名规则保存在一个目录下.\n   3. 使用一个独立的线程播放音频文件，播放完一个音频文件后，再播放下一个音频文件，直到播放完毕。\n```\n\n这里我们把业务逻辑写的相当详细，这样 AutoCoder 就能和你可控的完成这个需求了，而不是他自由发挥。此外，这里我们也用了 RAG 的方式\n从文档里召回一些示例代码，这样避免我们在写query的时候，自己手动添加这些示例代码。\n\n第三种情况，给定目标这个，我们也来看一个例子：\n\n```yaml\nquery: |\n  在 auto_coder.py 中添加一个 revert 子命令。\n  命令具体形态如下：\n  \n  ```shell\n  auto_coder revert --file {args.file}\n  ```\n\n  具体逻辑：\n\n  0. 使用Python git 包而不是使用shell命令，revert 功能放到 git_utils.py 中。\n  1. 找到 git message 等于 {args.file}的 git log,并 找到对应的 commit id。\n  2. 使用 git revert {commit_id} 撤销对应的 commit。\n  3. 如果撤销失败，需要打印 warning 信息。\n```\n\n在这个例子里，就是典型的，我先告诉你我要达到的一个效果是啥，接着我们给你加一些限制或者逻辑，从而在可控和大模型的自由发挥之间取得一个平衡。\n\n## 总结\n\n使用 AutoCoder 过程中，基本套路是上面描述的，尽量告知需要的文件有哪些，根据实际情况描述你的修改需求，因为是基于\n已有项目的，所以我们无需描述采用的技术等各种细节，只需要描述我们的需求即可。\n\n大家也可以多看看 AutoCoder  actions目录下的例子，里面都是实际的例子，可以帮助你更好的理解如何写需求。", "tag": "", "tokens": 1318, "metadata": {}}], "modify_time": 1716940982.7467074, "md5": "b13e45a5556a2a6670746abd971b028b"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/046_AutoCoder_Chat-Auto-Coder指南.md", "relative_path": "046_AutoCoder_Chat-Auto-Coder指南.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/046_AutoCoder_Chat-Auto-Coder指南.md", "source_code": "# Chat-Auto-Coder\n\nChat-Auto-Coder 是一款基于 AutoCoder 开发的交互式聊天工具,可以让你以对话的方式与 AI 进行沟通,可以无需打开编辑器，就能完成代码的开发。\n\n![](../images/046-01.png)\n\n\n\n## 功能特点\n\n- 支持添加/移除项目文件到当前会话中\n- 支持与 AI 就选定文件进行对话交流 \n- 支持查询项目索引，获得相关文件\n- 支持阅读项目\n- 支持配置 AI 生成代码\n- 命令行交互界面,操作便捷\n- 完善的自动补全功能\n\n\n## 使用方法\n\n* 将 Chat-Auto-Coder 集成到你的项目中\n* 在项目根目录下运行 `chat-auto-coder` 启动工具，进入命令行交互界面\n* 通过 `/conf project_type:xxx` 配置项目类型,支持后缀名，比如 .py, .java, .js多个按逗号分隔。 提供了两个快捷方式： py 和 ts\n* 运行 `/index/build` 根据 project_type 选择文件构建文件索引。\n* 通过 `/ask` 命令开始与 AI 对话,了解该项目\n* 通过 `/add_files` 命令添加需要讨论的项目文件  \n* 通过 `/chat` 可以随意聊天或者针对`/add_files` 内容提问。\n* 通过 `/coding` 让 AI 为你生成代码\n* 对上次commit的代码不满意，可以通过 `/revert` 命令进行回滚\n* 通过 `/conf code_model:xxxx`  配置代码生成模型。\n* 通过 `/conf human_as_model:true` 设置人类作为模型\n* 通过 `/conf skip_build_index:false` 设置自动开启索引，当你进行 /coding 时，会自动寻找相关文件\n* 使用其他命令如 `/index/build`， `/index/query` 可以构建和查询索引\n* `/exit` 退出程序\n* `/shell` 可以执行shell命令\n\n\n比如我想修改一个然后我只需要说出需求即可：\n\n![](../images/046-04.png)\n\n然后 AI 会自动修改代码，你可以看到修改结果：\n\n![](../images/046-03.png)\n\n你也可以打开编辑器自己再改改。\n\n## 设置代码生成模型\n\nChat-Auto-Coder 默认集成了 deespseek_chat 模型, 然而该模型现阶段在代码生成方面还存在问题，\n我们强烈推荐设置下面的模型中的某一个作为编程模型。\n\n1. Sonnet 3.5/ Opus 模型\n2. GPT4o\n\n具体做法：\n\n```shell\neasy-byzerllm deploy sonnet3.5 --token ${MODEL_CLAUDE_TOEKN} --alias sonnet_3_5_chat\n```\n\n然后你可以通过如下命令来来测试是否正常运行：\n\n```shell\neasy-byzerllm chat sonnet_3_5_chat hello\n```\n\n接着你可以通过 `/conf code_model:sonnet_3_5_chat` 来设置代码模型。\n\n[更多模型](https://github.com/allwefantasy/byzer-llm/blob/master/docs/zh/004_easy_byzerllm_%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97.md)\n\n## 安装\n\n快捷方式：\n\n```bash\npip install -U auto-coder\nchat-auto-coder\n```\n\n如果有问题，参考这篇更详细的：\n[000-AutoCoder_准备旅程](./000-AutoCoder_准备旅程.md)\n\n\n## 示例\n\n```shell\n/add_files main.py,utils.py\nAdded files: ['main.py', 'utils.py'] (关注你想修改的文件)\n/coding 在main中新添加一个hello方法\n... (AI 直接修改代码，然后你可以看到修改结果)\n/conf human_as_model: true\nSet human_as_model to true (这个时候可以拦截auto-coder 和大模型的交互)\n/index/query 查找所有调用了 request 库的文件\n... (返回查询结果,主要是你可能不知道哪些文件是你想改的，可以通过这个来自动找到，然后手动添加)\n/exit\nExiting...\n```\n\n## 常见配置\n\n1. human_as_model: 是否使用人类作为模型，默认为 false。 你可以通过 `/conf human_as_model: true` 来设置。[003-使用Web版本大模型，性感的Human As Model 模式](./003-%20AutoCoder%20使用Web版大模型，性感的Human%20As%20Model%20模式.md)\n2. code_model: 代码生成模型，默认为 deepseek_chat。 你可以通过 `/conf code_model: xxxx` 来设置其他通过 byzerllm 启动的模型。 \n\n## 了解项目，但不想改代码\n\n可以用 `/ask` 指令， 可以回答和项目相关的任何问题。\n\n或者通过 `/chat` 指令，可以随意聊天，或者针对`/add_files` 内容提问。\n\n## 对项目不熟悉，但是又想改项目怎办？\n\n默认我们是通过 `/add_files` 来添加 chat-auto-coder 关注的文件，这需要你清晰的知道你想修改的文件，以及为了修改这个文件，你还需要哪些文件。\n为了帮助大家更好的找到文件，我们提供了半自动挡的 `/index/query` 命令，你可以通过这个命令来查找你想要的文件。\n如果你想完全让 chat-auto-coder 自动找到你想要的文件，你可以通过如下配置开启：\n\n```shell\n/conf skip_build_index: false\n```\n\n当执行你的需求时，系统会自动寻找相关文件。全自动档速度较慢，并且可能存在找的文件不对。如果你想人工确认找到的文件，可以\n通过如下配置开启确认步骤：\n\n```shell\n/conf skip_confirm: false\n```\n\n记得，`/conf` 配置的这些参数即使重启后依然会一直有效。\n\n你可以通过如下指令查看当前所有的配置：\n\n```shell\n/conf\n```\n\n## 常见问题\n\nQ: 如何选择项目文件?\n\nA: 使用 `/add_files` 命令,多个文件用逗号分隔。文件路径支持自动补全。\n\nQ: AI 生成的代码如何合并?\n\nA: 默认使用 editblock 方式合并。你可以通过 `/conf auto_merge: xxx` 命令修改合并方式。\n\n\nQ: Chat-Auto-Coder 适合什么样的项目?\n\nA: 理论上支持任何编程语言的项目。但建议项目文件数不要过多,否则可能影响效率。\n  ", "tag": "", "tokens": 1367, "metadata": {}}], "modify_time": 1720265250.5082633, "md5": "1005a242da778de316b04eb6be7e1622"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/001-AutoCoder自动创建模板项目.md", "relative_path": "001-AutoCoder自动创建模板项目.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/001-AutoCoder自动创建模板项目.md", "source_code": "# 001-AutoCoder自动创建模板项目\n\nAutoCoder 系列教程开始了。作为程序员，那得先从创建一个项目开始。AutoCoder 提供了一个自动化创建项目的能力。\n\n不过这里大家要注意的是，因为每个模型能力不一样，甚至同一个模型每次给的答案都不一定稳定，所以**这部分功能并不稳定**。\n\n这里我们依然推荐 QwenMax/DeepSeek(也不太稳定) 来完成下面的工作。\n\n实际上，大部人都是对已有项目进行开发，新建项目相对来说比较少，而且很多项目其实都有工具可以完成自动化创建，这里只是展示 AutoCoder 的的一些\n问题拆解，自我规划，执行的能力。\n\n推荐直接进入 [002- 用 AutoCoder 添加和修改代码](./002-%20%E7%94%A8%20AutoCoder%20%E6%B7%BB%E5%8A%A0%E5%92%8C%E4%BF%AE%E6%94%B9%E4%BB%A3%E7%A0%81.md) 开始。\n\n## 实战一，创建一个Python项目\n\n实际上创建一个标准的符合 pip 规范的python项目，很多人并不会，一般可能都需要google下。这里用 AutoCoder 就是很好的开始。\n\n我们实战一的目标: 在 /tmp 目录下，创建一个符合 pip 标准的 python项目 t-py,注意不需要创建conda/venv环境。\n\n那如何让 AutoCoder 自动完成这个事情？新建目录 /tmp/t-py， 在里面新建一个 001_create_python_project.yml,内容如下：\n\n```yml\n\nsource_dir: /tmp/t-py\ntarget_file: /tmp/t-py/output.txt\n\nmodel: deepseek_chat\nexecute: true\n\nproject_type: \"copilot/.py\"\n\nquery: |\n  在 /tmp 目录下，创建一个符合 pip 标准的 python项目 t-py,注意不需要创建conda/venv环境\n```\n\n执行下这个文件：\n\n```bash\nauto-coder --file 001_create_python_project.yml\n```\n\n这个时候系统输出如下：\n\n```text\nIntent: UserIntent.CREATE_NEW_PROJECT\ntry to get the total steps...\ntotal steps to finish the user's question: 5\n=============================Collect AUTO STEPS===========================================\nuser: \n你熟悉各种编程语言以及相关框架对应的项目结构。现在，你需要\n根据用户的问题，根据提供的信息，对问题进行拆解，然后生成执行步骤，当执行完所有步骤，最终帮生成一个符合对应编程语言规范以及相关框架的项目结构。\n整个过程只能使用 python/shell。\n\n现在请参考下面内容：\n\n根据用户的问题，创建一个符合 pip 标准的 Python 项目不需要创建 conda/virtualenv 环境，但需要初始化项目结构，包括 setup.py、requirements.txt 和项目的目录结构。以下是详细的执行步骤：\n\n1. **创建项目目录**：\n   \\```bash\n   mkdir /tmp/t-py\n   cd /tmp/t-py\n   \\```\n\n2. **创建项目文件夹结构**：\n   在 `/tmp/t-py` 下创建 `t_py`（假设这是你的实际Python包名称）目录和 `src` 目录，以及其他可能需要的如 `tests` 目录等。\n   \\```bash\n   mkdir t_py\n   mkdir -p src/t_py\n   mkdir tests\n   \\```\n\n3. **在 `src/t_py` 下创建一个 __init__.py 文件**：\n   \\```bash\n   touch src/t_py/__init__.py\n   \\```\n   这个文件是标识该目录为 Python 包所必需的。\n\n4. **创建 setup.py 文件**：\n   在 `/tmp/t-py` 根目录下创建一个 `setup.py` 文件，用于定义项目信息及依赖项：\n   \\```bash\n   cat << EOF > setup.py\n   from setuptools import setup, find_packages\n\n   with open(\"README.md\", \"r\") as fh:\n       long_description = fh.read()\n\n   setup(\n       name=\"t-py\",\n       version=\"0.0.1\",\n       author=\"Your Name\",\n       author_email=\"your.email@example.com\",\n       description=\"A brief description of your project\",\n       long_description=long_description,\n       long_description_content_type=\"text/markdown\",\n       url=\"https://github.com/yourusername/t-py\",\n       packages=find_packages(where=\"src\"),\n       package_dir={\"\": \"src\"},\n       classifiers=[\n           \"Programming Language :: Python :: 3\",\n           \"License :: OSI Approved :: MIT License\",\n           \"Operating System :: OS Independent\",\n       ],\n       python_requires='>=3.6',\n   )\n   EOF\n   \\```\n   请根据实际情况修改上述 `setup.py` 中的项目信息。\n\n5. **创建 requirements.txt 文件**：\n   在 `/tmp/t-py` 根目录下创建一个 `requirements.txt` 文件，列出项目所需的依赖包：\n   \\```bash\n   touch requirements.txt\n   \\```\n   如果有具体的依赖包，可以打开此文件并添加它们的版本号，例如：\n   \\```\n   numpy==1.23.0\n   pandas==1.4.3\n   \\```\n\n总计共5步。\n\n每次生成一个执行步骤，然后询问我是否继续，当我回复继续，继续生成下一个执行步骤。\n\nassistant: \\```json\n{\n  \"current_step\": 1,\n  \"total_steps\": 5,\n  \"code\": \"mkdir /tmp/t-py\",\n  \"lang\": \"shell\",\n  \"cwd\": null,\n  \"env\": null,\n  \"timeout\": null,\n  \"ignore_error\": false\n}\n\\```\n请确认是否已执行此步骤并准备继续？\n\nuser: 继续\n\nassistant: \\```json\n{\n  \"current_step\": 2,\n  \"total_steps\": 5,\n  \"code\": \"cd /tmp/t-py\",\n  \"lang\": \"shell\",\n  \"cwd\": null,\n  \"env\": null,\n  \"timeout\": null,\n  \"ignore_error\": false\n}\n\\```\n请确认是否已执行此步骤并准备继续？\n\nuser: 继续\n\nassistant: \\```json\n{\n  \"current_step\": 3,\n  \"total_steps\": 5,\n  \"code\": \"mkdir t_py\\nmkdir -p src/t_py\\nmkdir tests\",\n  \"lang\": \"shell\",\n  \"cwd\": \"/tmp/t-py\",\n  \"env\": null,\n  \"timeout\": null,\n  \"ignore_error\": false\n}\n\\```\n请确认是否已执行此步骤并准备继续？\n\nuser: 继续\n\nassistant: \\```json\n{\n  \"current_step\": 4,\n  \"total_steps\": 5,\n  \"code\": \"touch src/t_py/__init__.py\",\n  \"lang\": \"shell\",\n  \"cwd\": \"/tmp/t-py\",\n  \"env\": null,\n  \"timeout\": null,\n  \"ignore_error\": false\n}\n\\```\n请确认是否已执行此步骤并准备继续？\n\nuser: 继续\n\nassistant: \\```json\n{\n  \"current_step\": 5,\n  \"total_steps\": 5,\n  \"code\": \"cat << EOF > setup.py\\nfrom setuptools import setup, find_packages\\n\\nwith open(\\\"README.md\\\", \\\"r\\\") as fh:\\n    long_description = fh.read()\\n\\nsetup(\\n    name=\\\"t-py\\\",\\n    version=\\\"0.0.1\\\",\\n    author=\\\"Your Name\\\",\\n    author_email=\\\"your.email@example.com\\\",\\n    description=\\\"A brief description of your project\\\",\\n    long_description=long_description,\\n    long_description_content_type=\\\"text/markdown\\\",\\n    url=\\\"https://github.com/yourusername/t-py\\\",\\n    packages=find_packages(where=\\\"src\\\"),\\n    package_dir={\\\"\\\": \\\"src\\\"},\\n    classifiers=[\\n        \\\"Programming Language :: Python :: 3\\\",\\n        \\\"License :: OSI Approved :: MIT License\\\",\\n        \\\"Operating System :: OS Independent\\\",\\n    ],\\n    python_requires='>=3.6',\\n)\\nEOF\",\n  \"lang\": \"shell\",\n  \"cwd\": \"/tmp/t-py\",\n  \"env\": null,\n  \"timeout\": null,\n  \"ignore_error\": false\n}\n\\```\n请确认是否已执行此步骤并准备继续？\n\nuser: 继续\n\nShell Command:\nmkdir /tmp/t-py\nOutput:\n\nError: mkdir: cannot create directory ‘/tmp/t-py’: File exists\n\n--------------------\nShell Command:\ncd /tmp/t-py\nOutput:\n\n--------------------\nShell Command:\nmkdir t_py\nmkdir -p src/t_py\nmkdir tests\nOutput:\n\n--------------------\nShell Command:\ntouch src/t_py/__init__.py\nOutput:\n\n--------------------\nShell Command:\ncat << EOF > setup.py\nfrom setuptools import setup, find_packages\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=\"t-py\",\n    version=\"0.0.1\",\n    author=\"Your Name\",\n    author_email=\"your.email@example.com\",\n    description=\"A brief description of your project\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/t-py\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.6',\n)\nEOF\nOutput:\n\n--------------------\n```\n\n自动创建项目的目录如下：\n\n![](../images/image2.png)\n\n注意，因为模型的差异性以及概率性，你最后的结果可能和我略有区别。但整体而言，是能够正确创建对应的目录结构的。\n\n## Case2，创建一个前端项目\n\n如果希望结果能够稳定重现，大家可以尝试开启搜索支持。因为目前 AutoCoder 仅支持 Bing 和 Google, 所以推荐大家使用 Bing, 可以到这里  https://www.microsoft.com/en-us/bing/apis/bing-web-search-api 申请一个免费的 Token,从而可以使用 Bing的API。\n\n```yml\n\nsource_dir: /tmp/t-project\ntarget_file: /tmp/t-project/output.txt \n\nmodel: deepseek_chat\nmodel_max_length: 2000\nmodel_max_input_length: 30000\nexecute: true\n\nsearch_engine: bing\nsearch_engine_token: ENV {{BING_SEARCH_TOKEN}}\n\nproject_type: \"copilot/.ts,.jsx\"\nquery: |\n  帮我在/tmp/目录下创建一个 typescript + reactjs 组成的项目，项目名字叫 t-project\n\n```\n\n你需要手动线创建下 t-project 目录，或者任何一个空目录，source_dir 是必须配置的目录。\n\n现在可以执行了：\n\n```bash\n\nauto-coder --file 001_create_python_project.yml\n```\n\n输出如下：\n\n```text\nIntent: UserIntent.CREATE_NEW_PROJECT\nsearch SearchEngine.BING for 帮我在/tmp/目录下创建一个 typescript + reactjs 组成的项目，项目名字叫 t-project...\nreraking the search result by snippets...\netch https://blog.csdn.net/weixin_42429718/article/details/117402097 and answer the quesion (帮我在/tmp/目录下创建一个 typescript + reactjs 组成的项目，项目名字叫 t-project) based on the full content...\ntry to get the total steps...\ntotal steps to finish the user's question: 2\n=============================Collect AUTO STEPS===========================================\nuser: \n你熟悉各种编程语言以及相关框架对应的项目结构。现在，你需要\n根据用户的问题，根据提供的信息，对问题进行拆解，然后生成执行步骤，当执行完所有步骤，最终帮生成一个符合对应编程语言规范以及相关框架的项目结构。\n整个过程只能使用 python/shell。\n\n现在请参考下面内容：\n\n根据用户的问题，以下是详细的执行步骤：\n\n总共分为两步来完成。\n\n\\**步骤 1：全局安装Create React App**\n\n\\```bash\n\\# 使用npm全局安装Create React App\nnpm install -g create-react-app\n\n\\# 或者，如果你的系统中已经配置了yarn，使用yarn全局安装\n\\yarn global add create-react-app\n\\```\n\n\\**步骤 2：在 /tmp/ 目录下创建名为 t-project 的 TypeScript + ReactJS 项目**\n\n\\```bash\n\\# 首先，确保你位于 /tmp/ 目录下\ncd /tmp/\n\n\\# 使用npx创建新项目，并指定TypeScript模板\nnpx create-react-app t-project --template typescript\n\n\\# 或者，如果你习惯使用yarn进行包管理\nyarn create react-app t-project --template typescript\n\\```\n\n以上步骤完成后，你将在 `/tmp/` 目录下得到一个名为 `t-project` 的React项目，该项目已集成TypeScript支持。\n\n每次生成一个执行步骤，然后询问我是否继续，当我回复继续，继续生成下一个执行步骤。\n\nassistant: \\```json\n{\n  \"current_step\": 1,\n  \"total_steps\": 2,\n  \"code\": \"# 使用npm全局安装Create React App\\nnpm install -g create-react-app\",\n  \"lang\": \"shell\",\n  \"cwd\": null,\n  \"env\": null,\n  \"timeout\": null,\n  \"ignore_error\": false\n}\n\\```\n\n请确认，是否已成功执行了第一步（全局安装Create React App），如果完成，请回复“继续”，我将为您生成下一个执行步骤。\n\nuser: 继续\n\nassistant: \\```json\n{\n  \"current_step\": 2,\n  \"total_steps\": 2,\n  \"code\": \"# 首先，确保你位于 /tmp/ 目录下\\ncd /tmp/\\n\\n# 使用npx创建新项目，并指定TypeScript模板\\nnpx create-react-app t-project --template typescript\",\n  \"lang\": \"shell\",\n  \"cwd\": \"/tmp/\",\n  \"env\": null,\n  \"timeout\": null,\n  \"ignore_error\": false\n}\n\\```\n\n请确认您已成功执行了上一步并现在位于 `/tmp/` 目录下。如果完成，请回复“继续”，我将为您生成最后一个执行步骤来创建名为 `t-project` 的 TypeScript + ReactJS 项目。\n\nuser: 继续\n\nShell Command:\n\\# 使用npm全局安装Create React App\nnpm install -g create-react-app\nOutput:\n\n\\--------------------\nShell Command:\n\\# 首先，确保你位于 /tmp/ 目录下\ncd /tmp/\n\n\\# 使用npx创建新项目，并指定TypeScript模板\nnpx create-react-app t-project --template typescript\nOutput:\n\n\\--------------------\n```\n\n可以看到，AutoCoder 会自动搜索文章，并且找到了 CSDN的一篇博文：https://blog.csdn.net/weixin_42429718/article/details/117402097 觉得很不错，所以采用了他的方案作为参考，最后产生具体的执行步骤。\n\n最后生成的目录：\n\n![](../images/image3.png)\n\n当然了，如果你自己找到合适的文章，也可以通过 urls 参数配置，那么AutoCoder 就可以通过参考你给的文章来完成项目的创建了。\n\n## 总结\n\n实际上，上面的生成效果并没有很理想。比如没有检测你是否具有相应的环境。比如是不是npm 没有安装？ 所以很多人执行可能会失败。随着模型能力的不断提升，包括后续我们会介入更加智能的搜索引擎，我相信这些很快会解决。\n\n下一期 002, 我们可以开始着手代码了！ ", "tag": "", "tokens": 3337, "metadata": {}}], "modify_time": 1716967658.9144986, "md5": "34d5987c05555f5ca650716e8a066ee9"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/028-AutoCoder_如何高效的生成修改多文件.md", "relative_path": "028-AutoCoder_如何高效的生成修改多文件.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/028-AutoCoder_如何高效的生成修改多文件.md", "source_code": "# 028-AutoCoder_如何高效的生成修改多文件\n\n我们大模型一次生成的token是有限的，并且一般会远远小于输入。这就意味着，如果你要生成的代码量很大，那么你可能需要分多次生成。\n\nAutoCoder 提供了一个很好的功能，可以帮助你高效的生成修改多文件。你可以通过以下步骤来实现：\n\n```yml\n## 代码生成的时候会每次生成一个文件，现在大模型无法一次性生成太多文件，所以需要多轮生成\nenable_multi_round_generate: true\n```\n\n这里分两种情况，\n\n## 如果你开启了 human_as_model\n\n也就是:\n\n```yml\nhuman_as_model: true\n```\n那么AutoCoder会在大模型生成代码阶段停下来，然后把prompt 放到 `target_file` 中，你需要复制黏贴到web端，然后拷贝对应全部内容(不仅仅是代码)到\nAutoCoder的命令行中，然后换行，输入 `EOF` 点击回车。这个时候 AutoCoder 会继续把新的prompt(通常是`继续` 这个词汇)给到你，然后你继续复制黏贴到web端，然后拷贝对应内容到AutoCoder的命令行中，然后换行，输入 `EOF` 点击回车。如此循环，直到你的所有文件都生成完成或者web侧告诉你所有的文件都生成完成，此时，你需要:\n\n1. 换行 ，输入 '\\_\\_完成\\_\\_' 点击回车\n2. 输入 `EOF` 点击回车\n\n即可完成最后的输入过程。\n\n'\\_\\_完成\\_\\_' 表示可以结束多轮对话。 `EOF` 表示结束当前的这一轮，继续下一轮。\n\n## 如果你没有开启 human_as_model\n\n那么所有的操作 AutoCoder 会自动完成。但是如果你的模型没有办法准确的结束(比如按照AutoCoder要求，最后给定'\\_\\_完成\\_\\_'), 那有可能会陷入死循环，不过幸运的事，AutoCoder 会限制最多生成10轮，如果超过10轮，AutoCoder 会自动结束。\n\n\n", "tag": "", "tokens": 426, "metadata": {}}], "modify_time": 1716039834.5228148, "md5": "f22759c4ae1b2459cd9f4b9088c04e66"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/005-AutoCoder 使用搜索引擎.md", "relative_path": "005-AutoCoder 使用搜索引擎.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/005-AutoCoder 使用搜索引擎.md", "source_code": "# 005-AutoCoder 使用搜索引擎\n\n在上一篇内容中，我们介绍了 AutoCoder 可以同时看源码，同时阅读文档，最后结合你的需求进行代码的迭代。\n\n实际上，这个文档有两种可能性：\n\n1. 接口或者SDK的文档\n\n2. 一些设计思路\n\n通常接口或者SDK文档，通过 urls 参数指定是OK的，AutoCoder >= 0.1.18 同时还支持指定本地文件，并且支持 PDF,Word 等文档。\n\n但是如果你希望 AutoCoder 使用搜索引擎，寻找一些思路也是可以的。为了开启搜索引擎能力，你需要做两件事：\n\n1. 申请Google或者Bing的搜索API Token\n2. 在 AutoCoder 参数中带上两个参数\n\n首先是申请 API Token, 我们推荐 Bing, 可以访问这里：https://www.microsoft.com/en-us/bing/apis/bing-web-search-api 获取 token。\n\n其次是开启搜索功能：\n\n```yml\nsource_dir: /tmp/t-py\ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nmodel: deepseek_chat\nmodel_max_length: 2000\nmodel_max_input_length: 30000\nanti_quota_limit: 0\n\nsearch_engine: bing\nsearch_engine_token: ENV {{BING_SEARCH_TOKEN}}\n\n## execute the prompt generated by auto-coder\nexecute: true\n## extract the code from the prompt generated by auto-coder \n## and overwrite the source code\nauto_merge: true\n\nproject_type: py\n\nhuman_as_model: true\n\nquery: >\n  修改 server.py ，在代码 app = FastAPI()后\n  增加 ray 的初始化连接代码。\n```\n\n这里，我们可以看到 search_engine 和 search_engine_token 参数。当你使用 ENV {{BING_SEARCH_TOKEN}} AutoCoder 会在你环境变量里找到 BING_SEARCH_TOKEN 的值。如果你不是通过环境变量，你直接复制你的token 替换 \"ENV {{BING_SEARCH_TOKEN}}\" 就行。\n\n注意， AutoCoder 需要你配置Model才会生效。此外，AutoCoder 还会利用大模型对搜索结果进行筛选，最终可能没有合适的，这个时候就搜索内容就不会影响后续的代码生成。\n\n如果你能明确知道你的搜索词，那你可以直接通过一个参数解决：\n\n```yml\nsearch: 如何使用FastAPI\nsearch_engine: bing\nsearch_engine_token: ENV {{BING_SEARCH_TOKEN}}\n```\n\n", "tag": "", "tokens": 533, "metadata": {}}], "modify_time": 1716937423.5589266, "md5": "f8d2f61d8dc1ba2350a7bb82c5533bc3"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/命令行版Devin 来了_Auto-Coder.md", "relative_path": "命令行版Devin 来了_Auto-Coder.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/命令行版Devin 来了_Auto-Coder.md", "source_code": "# 命令行版Devin 来了: Auto-Coder\n\n## 前言\n\n从上周四开始，极限十小时实现了第一个可用版本，在这期间，我们成功实现了自举，也就是利用 Auto-coder 的基础功能来帮助 Auto-coder的开发，所以才有如此神速。\n\n今天这篇文章，我们来介绍下 Auto-Coder 到底可以给程序员带来什么价值。\n\n## Github Copilot 够么？\n\nGithub Copilot 本质还是IDE工具的衍生，是一个更加“智能”的代码提示，而其提供的Copilot Chat 则更加只是把一个聊天框做到IDE而已，和集成一个搜索框到IDE工具没有任何区别，然还是一个古典产品的思维在做的一个产品。\n\n更细节的，我可以从三个维度做给大家做分析：\n\n第一个维度是 Github Copilot 的定位，我一直是 Github Copilot 的铁杆用户，但因为它的定位是只能代码提示，这决定了他需要追求响应延时而不是效果，所以他最大的问题是，它无法基于整个项目的源码去做新的代码实现（这样会导致延时增加到不可接受，并且成本太高）。\n\n第二个维度是 Github Copilot 无法模拟人类的开发行为，我们实际做开发的时候，一般都是基于已有功能，并且根据某种“文档”，“第三方代码”和“搜索引擎”来进行开发。\n\n比如 Byzer-LLM 要对接 Qwen-vl 多模态大模型，那么作为一个开发，我至少需要准备三个事情：\n\n1. 首先我们需要了解和参考Byzer-LLM 之前是怎么对接各种模型的代码\n2. 其次我要找到 Qwen-VL的API 文档了解 Qwen-VL 的API\n3. 我可能还需要搜索下参考下别人是怎么对接的，以及如果我使用了第三方SDK，我还需要第三方SDK的文档或者代码。\n\n只有获取了这些信息之后，我们才能写出一个靠谱的代码。但 Github copilot 能做到这些么？显然做不到。\n\n第三个维度是，我没有办法替换模型，也就是只能用 Github Copilot 背后的模型，哪怕我有 GPT-4/Claude3-Opus的 web订阅版，如果是公司在用，如何保证模型的私有化部署呢？\n\n所以 Github Copilot 的产品本质决定了他只是一个更加smart的提示工具，而不是模拟人去编程。这个虽然说不上逆AGI潮流，但确实不够 AI Native, 没有把AI 充分利用起来。\n\n基于上面的问题，所以有了 Auto-Coder。\n\n\n\n人类单纯编程部分，无非是\n\n1.  理解需求\n\n2. 搜索看别人怎么解决类似问题，理清思路\n\n3. 看已有项目的代码\n\n4. 看要用到的第三方库的源码或者文档\n\nAutoCoder 会在阅读已有代码，参考文档，自动进行搜索引擎获取相关信息，最后尝试去理解程序员的需求，有了这些信息后，才最后进行生成代码，最终\n能够生成足够好的代码。\n\n我们来看看 Auto-Coder 的一些典型场景。\n\n## Auto-Coder 的典型场景\n\n### Case 1: 给已有项目加一个功能\n\n第一个典型Case 是，就是我要给目前已有的一个项目加一个功能，大致需求是加一个命令参数，并且要有一个叫做HttpDoc类处理这个新加的参数。\n大家可以看看下面 query 部分来了解详细需求。\n\n```yml\n\nsource_dir: /home/winubuntu/projects/ByzerRawCopilot \ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nquery: |\n  新增一个命令行参数 --urls 可以指定按逗号分隔的多个http链接\n  实现一个 HttpDoc 类，获取指定的http链接，获取链接的内容，返回 SourceCode 对象列表\n  在 HttpDoc 类实现一个抽取正文的方法，llm.chat_oai 方法来完成\n\n```\n第一行 source_dir 指定了我当前所在的项目，第二行 target_file， AutoCoder 会将收集到项目详细以及用户的需求生成\n一个Prompt 保存到指定了输出这个文件里。\n\n接着，你只要运行：\n\n```bash\nauto-coder -f actions/add_urls_command_paraemeter.yml\n```\n\n就可以生成合适的Prompt到  output.txt 文件里。接着你就可以把这个文件拖拽到比如 GPT4/Claude/KimiChat 等 Web 里，他们会生成代码，你只要复制黏贴到项目里即可。\n\n当然，如果你希望能够系统自动完成代码生成，则可以新增两参数：\n\n```yml\nsource_dir: /home/winubuntu/projects/ByzerRawCopilot \ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nmodel: qianwen_chat\nmodel_max_length: 2000\nmodel_max_input_length: 100000\nanti_quota_limit: 5\n\nexecute: true\nauto_merge: true\n\nquery: |\n  新增一个命令行参数 --urls 可以指定按逗号分隔的多个http链接\n  实现一个 HttpDoc 类，获取指定的http链接，获取链接的内容，返回 SourceCode 对象列表\n  在 HttpDoc 类实现一个抽取正文的方法，llm.chat_oai 方法来完成\n```\n\n这里新增加了两个参数，一个是 model 配置一个 AutoCoder可以直接调用的大模型， 一个是 execute。这样 Auto-Coder 会自动调用模型回答你的问题，并且把结果保存到 output.txt 文件里。\n\n如果你对 AutoCoder 生成的代码足够放心，你可以配置 auto_merge 参数，这样 AutoCoder 会把修改自动合并代码到你已有的项目代码里去。\n\n### Case 2: 给已有项目加一个功能，除了已有项目，还需要参考一个文档才行\n\n第二个Case: 参考一个API文档，然后根据已有代码新增某个接口的对接。这个应该是程序员经常要做的事情。\n\n```yml\nsource_dir: /home/winubuntu/projects/byzer-llm/src/byzerllm/saas\ntarget_file: /home/winubuntu/projects/byzer-llm/output.txt\nurls: https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-vl-plus-api?disableWebsiteRedirect=tru\nquery: |\n  学习通义千问VL文档，然后参考 saas/qianwen 中的接口规范实现，实现一个 saas/qianyi_vl。\n\n```\n\n这里我们新增了一个 urls 参数，指定文档地址，然后系统会自动获取你现有的源码以及API文档，然后和你的问题一起存储到 output.txt 文件里，然后你就可以拖拽到比如 GPT4/Claude/KimiChat 等 Web 里，他们会生成代码，你只要复制黏贴到项目里即可。\n\n### Case 3: 给已有项目加一个功能，除了已有项目，还需要参考一个文档，还需要参考一个第三方库的源码\n\n第三个case, 我要使用某个库，但是这个库的文档比较少（或者不全），我需要基于这个库开发一个功能，能不能让大模型自己阅读那个库的源码，然后结合我现有的代码，实现一个功能？没问题！\n\n```yml\n\nsource_dir: /home/winubuntu/projects/byzer-llm/src/byzerllm/saas\ntarget_file: /home/winubuntu/projects/byzer-llm/output.txt\npy_packages: openai\nquery: |\n  参考 src/byzerllm/saas/qianwen 中的实现，重新实现 offical_openai。注意 offical_openai 中\n  使用的是openai 这个模块，你需要学习这个模块的使用方法，保证正确的使用其功能。\n```\n\n这里我指定 Auto-Coder 要特别关注 openai 这个 SDK库，然后我让他参考以前我实现对 qianwen的对接，用openai 这个库，实现对 OpenAI 模型的对接。最终系统会把 OpenAI, 我自己的项目，以及我的要求合并成一个prompt,然后放到 output.txt里。如果你有API，也可以配置下 model参数，然后系统会自动调用模型回答问题。\n\n### Case 4: 创建一个新项目\n第四个case，我想创建一个 reactjs+typescript 的项目，但是我忘了具体怎么弄了，能不能让大模型自动帮我创建？没问题的\n\n```yml\n\nsource_dir: /home/winubuntu/projects/ByzerRawCopilot \ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nmodel: qianwen_short_chat\nmodel_max_length: 2000\nanti_quota_limit: 5\n\nsearch_engine: bing\nsearch_engine_token: xxxxxxx\n\nproject_type: \"copilot\"\nquery: |\n  帮我在/tmp/目录下创建一个 typescript + reactjs 组成的项目，项目名字叫 t-project\n\n```\n\n这里需要额外配置两个：第一个是配置一个模型，第二个是配置一个搜索引擎。Auto-Coder 会按如下逻辑进行操作：\n\n1. 通过搜索引擎检索相关的操作。\n2. 大模型会对检索结果进行阅读，并且找到最合适的那篇内容\n3. 取到那篇文档，并且提取正文，进行理解\n4. 抽取解决这个问题需要的步骤，生成代码 \n\n5. 利用内置的Shell/Python 执行器按步骤执行。\n\n可以给大家看看内部的日志：\n\n```text\n用户尝试: UserIntent.CREATE_NEW_PROJECT\nsearch SearchEngine.BING for 帮我在/tmp/目录下创建一个 typescript + reactjs 组成的项目，项目名字叫 t-project...\nreraking the search result by snippets...\nfetch https://blog.csdn.net/weixin_42429718/article/details/117402097 and answer the quesion (帮我在/tmp/目录下创建一个 typescript + reactjs 组成的项目，项目名字叫 t-project) based on the full content...\nuser: \n你熟悉各种编程语言以及相关框架对应的项目结构。现在，你需要\n根据用户的问题，根据提供的信息，对问题进行拆解，然后生成执行步骤，当执行完所有步骤，最终帮生成一个符合对应编程语言规范以及相关框架的项目结构。\n整个过程只能使用 python/shell。\n\n环境信息如下:\n操作系统: linux 5.15.0-48-generic  \nPython版本: 3.10.11\nConda环境: byzerllm-dev \n支持Bash\n\n现在请参考下面内容：\n\n由于提供的上下文信息与在Linux环境下使用命令行创建一个TypeScript和ReactJS项目无关，我将基于一般操作步骤给出解答。\n\n要在Linux系统的 `/tmp/` 目录下创建一个由TypeScript和ReactJS组成的项目，并命名为`t-project`，请按照以下步骤操作：\n\n1. 首先，请确保您已全局安装了Node.js包管理器（npm）以及创建React应用的脚手架工具 `create-react-app`。如果尚未安装，可以通过以下命令安装：\n   \n   npm install -g create-react-app\n   \n\n2. 然后，由于`create-react-app`默认不支持 TypeScript，需要安装 `create-react-app` 的 TypeScript 版本，即 `react-scripts-ts`，但请注意，`react-scripts-ts` 已不再维护，最新推荐做法是直接使用 `create-react-app` 并通过 `--template typescript` 参数指定 TypeScript 模板：\n\n   \n   npx create-react-app t-project --template typescript\n   \n\n   这条命令会在 `/tmp/` 目录下创建名为 `t-project` 的新React项目，并配置为使用TypeScript。\n\n3. 创建完成后，进入项目目录并启动开发服务器：\n\n   \n   cd /tmp/t-project\n   npm start\n   \n\n这样就成功在 `/tmp/` 目录下创建了一个基于TypeScript和ReactJS的项目。[information is missing on]如何具体使用命令行在Linux系统中创建项目，因为上下文没有提供相关指导。\n\n用户的问题是：帮我在/tmp/目录下创建一个 typescript + reactjs 组成的项目，项目名字叫 t-project\n\n每次生成一个执行步骤，然后询问我是否继续，当我回复继续，继续生成下一个执行步骤。\n\nassistant: \n{\n  \"code\": \"npm install -g create-react-app\",\n  \"lang\": \"shell\",\n  \"total_steps\": 3,\n  \"current_step\": 1,\n  \"cwd\": \"/tmp\",\n  \"env\": {},\n  \"timeout\": null,\n  \"ignore_error\": false\n}\n\n请确认是否已安装全局的 `create-react-app` 工具。如果确认或执行完成，请回复“继续”。\n\nuser: 继续\n\nassistant: \n{\n  \"code\": \"npx create-react-app t-project --template typescript\",\n  \"lang\": \"shell\",\n  \"total_steps\": 3,\n  \"current_step\": 2,\n  \"cwd\": \"/tmp\",\n  \"env\": {},\n  \"timeout\": null,\n  \"ignore_error\": false\n}\n\n请在 `/tmp` 目录下执行此命令以创建基于 TypeScript 的 ReactJS 项目。如果项目创建完成，请回复“继续”。\n\nuser: 继续\n\nassistant: \n{\n  \"code\": \"cd t-project\",\n  \"lang\": \"shell\",\n  \"total_steps\": 3,\n  \"current_step\": 3,\n  \"cwd\": \"/tmp\",\n  \"env\": {},\n  \"timeout\": null,\n  \"ignore_error\": false\n}\n\n请在终端中切换到刚创建的 `t-project` 目录。若切换成功，请回复“继续”以进行下一步操作，即启动项目开发服务器。\n\nuser: 继续\n```\n\n\n可以看到，最终需要三个步骤就可以帮助用户解决这个问题。\n\n### 如果项目很大怎么办\n\n这个问题的本质是：\n1. Token 很贵\n2. 大模型一次能接受的信息有限\n\nAuto-Coder 可以对你的项目做索引，做完索引后，会自动根据你的问题找到可能需要修改的文件，基于这些文件，再找这些文件会使用哪些其他文件，然后只把这些文件的信息和你的问题一起形成prompt 然后让大模型来回答。\n\n开启索引也很简单，只要添加一个 skip_build_index 参数即可。\n\n值得注意的事， 索引的构建需要依赖有API的大模型,所以必须配置 model参数，不然不会生效。\n\n```yml\n\nsource_dir: /home/winubuntu/projects/byzer-llm/src/byzerllm/saas\ntarget_file: /home/winubuntu/projects/byzer-llm/output.txt\n\nskip_build_index: false\n\nmodel: qianwen_short_chat\nmodel_max_length: 2000\nanti_quota_limit: 5\n\nquery: |\n  参考 src/byzerllm/saas/qianwen 中的实现，重新实现 offical_openai。注意 offical_openai 中\n  使用的是openai 这个模块，你需要学习这个模块的使用方法，保证正确的使用其功能。\n```\n\n一旦开启索引，Auto-Coder 会自动构建索引，根据用户的需求描述,同时：\n\n1. 自动筛选相关的源码文件。\n2. 从筛选出来的源码文件，再筛选一级他们依赖的文件。\n\n一般情况，经过这样的筛选，应该也就几个或者十几个文件，也能满足大部分代码生成的需求。\n\n## 总结下\n\n使用 Auto-Coder,  他可以自己阅读你已经写的源码，阅读API文档，阅读第三方类库的代码，然后根据你的要求编写代码，添加新功能。也可以自动去搜索引擎，找到合适的文章进行阅读，然后自动帮你完成包括项目创建等在内的基础工作。使用起来也很方便，支持命令行以及通过 YAML 进行配置。\n\n", "tag": "", "tokens": 3231, "metadata": {}}], "modify_time": 1717749340.0463655, "md5": "01e27f9aa7e2a40cab35be243f665428"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/027-AutoCoder_我只要效果好_那么最佳组合是什么.md", "relative_path": "027-AutoCoder_我只要效果好_那么最佳组合是什么.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/027-AutoCoder_我只要效果好_那么最佳组合是什么.md", "source_code": "# 027-AutoCoder_我只要效果好_那么最佳组合是什么\n\n经过消耗至少 2000多万token的探索之后，我总结在 AutoCoder中 辅助编程的最佳模型组合：\n\n1. 索引构建 DeepSeek/Haiku  (对应 index_model 参数)\n2. 索引查询/AutoCoder 功能驱动 DeepSeek/GPT3.5 (对应 model 参数)\n3. 代码生成  Claude Opus / GPT-4o (对应 code_model参数，推荐human_as_model 设置为true)\n4. 知识库构建 OpenAI Embedding （Small） (对应 emb_model 参数)\n5. 图片转web GPT-4o (对应 vl_model 参数)\n\n此外，因为代码生成的token消耗量也很大， AutoCoder 提供了独有的 human_as_model 功能，允许你使用 web 版本的模型来完成代码生成，相当于包月，避免海量token的计费。\n\n\n如果你完全不考虑成本，那么最好的组合是：\n1. 索引构建/索引查询/AutoCoder功能驱动 用 GPT-3.5\n2. 代码生成用 Claude Opus (human_as_model 设置为false, 这个时候会走API直接生成代码，无需人工到web里去复制黏贴)\n3. 知识库构建 OpenAI Embedding （Big） (对应 emb_model 参数)\n4. 图片转web GPT-4o (对应 vl_model 参数)\n\n", "tag": "", "tokens": 309, "metadata": {}}], "modify_time": 1717319610.3795602, "md5": "06e37edce94d6a7c3f7c6d14f05fef0e"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/015 - AutoCoder 迭代粒度拆解技巧.md", "relative_path": "015 - AutoCoder 迭代粒度拆解技巧.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/015 - AutoCoder 迭代粒度拆解技巧.md", "source_code": "# 015 - AutoCoder 迭代粒度拆解技巧\n\n下面是我最近开发一个实际前后端应用的一个迭代流程：\n\n![](../images/image15-01.png)\n\n经过大概20多轮迭代加上一定的人工修改，完成了这个项目。大概花了一天多一点。\n\n而且实际我用下来，对于前后端项目非常好的一点，是可以左右互搏，比如先写好前端请求，基本就可以自动生成后端的API 接口（入参，数据库表等）。同理反过来，对接已经写好的后端API， 前端也是基本可以自动生成对应的 interface的。因为AutoCoder可以给到比较完整的上下文信息。\n\n为什么需要拆解到这么细，而不是“Hey, 大模型，给我一个完美的Web网站，支持上传下载，支持实时显示后端信息” 这么一句话？ Hmmm, 那是 Devin的目标，不是 AutoCoder的目标。\n\nAutoCoder 目前阶段是需要程序员来做Planing， Review/Revert 来持续推进的，最终达到最后目标。如果拆解的过细，可能就有点浪费token和时间。但是如果拆解粒度太粗，就会有很多问题。\n\n产出完全不符合预期\n如果你的粒度太粗，那么最终产出很可能和你想的完全不一样，而且大量新生成的代码也导致review 困难。大概率你需要revert重来，并且花费更多的重试成本。\n\n输入输出过大\n大模型的输入输出都是有限制的。如果你一个需求太粗了，可能会导致需要修改的文件特别多，需要参考的文件也更多，最终哪怕是开启了索引，也会涉及到很多文件，超出了大模型可以接受的输入。\n\n而如果修改的问题特别多，那么输出就会很大，就会导致超出最长限制而输出不完整，导致需要手工介入。\n\n后续难以人工微调\n如果粒度太初，一次性生成代码较多，研发很难一眼看出修改是不是符合预期，或者快速的根据新修改的代码做一些人工调整。\n\n当然，如何控制好每次需求的粒度，其实非常考验研发同学的架构思维和手感。需要实际迭代几次，才能慢慢找到其中的平衡点。\n\n\n\n下面让大家看看我写的几个示例：\n\n![](../images/image15-02.png)\n\n这个是项目起手，我构建了一个页面。\n\n![](../images/image15-03.png)\n\n\n接着我让AutoCoder 根据前面的页面，生成一个 python 后端。大模型能够很好的理解前后端的进行配合。\n\n![](../images/image15-04.png)\n\n还有比如这个，我想添加一个提交后，转圈圈的等待动画。这个就一句话搞定了。如果你你想调整自己的内容，可以随时revert 自己刚刚的操作，可以看这篇：\n[](./012-AutoCoder如何保障auto-merge下的代码安全.md)\n此外项目的代码使用 human_as_model 模式生成的，参考这篇：[](./003-%20AutoCoder%20使用Web版大模型，性感的Human%20As%20Model%20模式.md)\n\n最后，这篇文章其实介绍的非技术的东西，但是也是 AutoCoder 中对使用者最核心的要求，实际上很重要。\n\n", "tag": "", "tokens": 625, "metadata": {}}], "modify_time": 1711938548.0977168, "md5": "24b00406162f6efa6620edc03b372519"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/024-AutoCoder并行索引构建和查询.md", "relative_path": "024-AutoCoder并行索引构建和查询.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/024-AutoCoder并行索引构建和查询.md", "source_code": "# 024-AutoCoder并行索引构建和查询\n> 版本要求：\n> - auto-coder >= 0.1.52\n## 什么模型比较适合做索引构建和查询？\ndeepseek 和 haiku 都属于性价比较高的模型，可以用来做索引构建。\n索引查询则可以使用 gpt-3.5-turbo，这个模型的速度比较快，效果不错，适合做索引查询。\n\n## 并行度设置\n\n正常一个 python文件，一般使用 haiku 构建索引消耗时间为 2-7s 之间，而使用 deepseek 则消耗在10-30s 之间。\n如果用户第一次构建索引，会导致等待时间过长，所以我们可以使用并行的方式来构建索引。\n\n```yaml\n# 开启索引\nskip_build_index: false\n# 索引查询时的级别\nindex_filter_level: 1\n# 查询索引的并行度设置\nindex_filter_workers: 4\n\n# 单次构建最大的输入长度，建议设置较大值\nindex_model_max_input_length: 30000\n# 构建索引的并行度设置\nindex_build_workers: 4\n```\n\n如上所示，我们可以通过设置 `index_build_workers` 来设置构建索引的并行度，比如我这里设置为4。\n不过单单设置这个为4 可能还不够，我们还需要在部署模型的时候把并行度也匹配下：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 4 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n这里我部署了deepseek 模型，然后设置了 `--num_workers 4`，这样就可以保证构建索引和查询索引设置的并行度不会被 byzerllm 代理阻塞。一般而言，我们可以把 num_workers 的值设置的比 index_build_workers 大一点，这样可以保证索引构建的时候不会被阻塞。\n\n", "tag": "", "tokens": 461, "metadata": {}}], "modify_time": 1715214449.5964592, "md5": "b45c8ad95fb35484fbbb6fb2002cebc4"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/042_AutoCoder_顺手把文档也写了.md", "relative_path": "042_AutoCoder_顺手把文档也写了.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/042_AutoCoder_顺手把文档也写了.md", "source_code": "## 042_AutoCoder_顺手把文档也写了\n\n准备给 auto-coder 加一个自动创建下一个需求yaml文件的功能。分成了两个步骤：\n\n```yaml\ninclude_file: \n   - ./common/editblock.yml\n\nquery: |   \n   关注 auto_coder.py, command_args.py, lang.py 三个文件。添加一个 next 命令\n   \n   auto-coder next [name]   \n\n   具体功能为：\n\n   1. 在当前目录下，寻找是否有 actions 目录，如果没有则告诉用户 \"当前目录下没有 actions 目录(用英文逗号分隔)\" 并退出。\n   2. 如果有 actions 目录，获得 actions 所有以 三个数字开头的文件名，然后找到最大的序列号，然后在这个序列号上加 1，得到新的序列号。\n   3. 将得到的序列号，转换为 3 位的字符串，然后在 actions 目录下创建一个新的文件，文件名为这个序列号加上下划线加上 name，文件为上一个文件（序列号比他小一）的内容。   \n   \n```\n\n这次我们开启了 editblock 功能，可以让大模型以最少的生成token数，实现对多文件进行更改，相比diff 也有更加稳定，对模型要求也更低一些。 更多参考这篇文档[035-AutoCoder_auto_merge详解.md](./035-AutoCoder_auto_merge详解.md)。\n\n执行完上面的 yaml文件实际上基本功能就做完了，可以用了，用的过程发现，没有自动获取上一次需求的yaml文件内容，所以这次特意强调下：\n\n```yaml\ninclude_file: \n   - ./common/editblock.yml\n\nquery: |   \n   关注 auto_coder.py 中的next命令，新创建的文件的内容需要是上一个比他序列号小的那个文件的内容。   \n```\n\n执行：\n\n```bash\nauto-coder --file actions/064_修正内容.yml\n```\n\n本来到这里就可以收工了。但我想文档也让 auto-coder 写了把。于是\n\n```bash\nauto-coder next 生成next文档\n```\n自动创建了 `065_生成next文档.yml` 文件，点击打开，描述需求：\n\n```yaml\ninclude_file: \n   - ./common/local.yml\n\nquery: |   \n   关注 auto_coder.py 中的next命令，生成一份名字叫 `041_AutoCoder_快速生成下一个需求YAML文件.md` 的文档，\n   放在 docs/zh 目录下。\n   要说明解决的问题是什么，怎么解决，如何使用。   \n```\n\n再执行：\n\n```bash\nauto-coder --file ./actions/065_生成next文档.yml\n```\n\n最后我们这篇文档就出现了: [041_AutoCoder_快速生成下一个需求YAML文件.m](./041_AutoCoder_快速生成下一个需求YAML文件.md)\n\n看着还不错。", "tag": "", "tokens": 594, "metadata": {}}], "modify_time": 1717396432.9068897, "md5": "0038c094a0fa0ec5a7610fc1fe608754"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/014-AutoCoder使用Ollama.md", "relative_path": "014-AutoCoder使用Ollama.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/014-AutoCoder使用Ollama.md", "source_code": "# 014 - AutoCoder 使用 Ollama\n\nOllama 是一个很优秀的模型部署工具。 Byzer-LLM 则不仅仅支持类似 Ollama 的模型部署能力，还包括如下功能特性：\n\n1. 开源或者私有模型训练、模型调参等\n2. 训练和部署都支持分布式\n3. 同时支持Saas模型\n4. 支持诸如Prompt函数/类等将大模型和编程语言融合的一些高阶特性\n\n不过如果用户已经使用 Ollama 进行了模型的部署,我们依然可以使用 byzer-llm 对接Ollama, 这样 AutoCoder就可以无缝使用 Ollama了。 因为 ollama 支持 OpenAI 协议的接口，所以我们可以使用如下方式进行部署：\n\n```shell\nbyzerllm deploy  --pretrained_model_type saas/official_openai \\\n--cpus_per_worker 0.01 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=xxxxx saas.model=llama2  saas.base_url=\"http://localhost:11434/v1/\" \\\n--model ollama_llama_chat\n```\n\n这里的 `ollama_llama2_chat` 是一个模型的名字，可以自己定义，后续在 AutoCoder 中使用这个名字即可, 其他的则是一些资源方面的配置，因为\n我们用的是已经部署好的Ollama,所以 gpus设置为0, cpus 则设置一个较小的数值即可，并且设置下并发数num_workers，这里因为我是测试，所以设置为1。\n\n最后在 `saas.base_url` 配置下 Ollama 的地址。\n\n部署完成后可以测试下：\n\n```shell\nbyzerllm query --model ollama_llama2_chat --query 你好\n```\n\n输出如下：\n\n```\nCommand Line Arguments:\n--------------------------------------------------\ncommand             : query\nray_address         : auto\nmodel               : ollama_llama2_chat\nquery               : 你好\ntemplate            : auto\nfile                : None\n--------------------------------------------------\n2024-03-27 16:34:22,040\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 192.168.3.123:6379...\n2024-03-27 16:34:22,043\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at 192.168.3.123:8265\n\nHello! 😊 How are you today? Is there anything you'd like to chat about or ask? I'm here to help with any questions you may have.\n```\n\n\n", "tag": "", "tokens": 558, "metadata": {}}], "modify_time": 1713849439.0655332, "md5": "890294990d46a2d35b9ef063e734cb45"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/004-AutoCoder 边看代码 边看文档 写代码.md", "relative_path": "004-AutoCoder 边看代码 边看文档 写代码.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/004-AutoCoder 边看代码 边看文档 写代码.md", "source_code": "# 004-AutoCoder 边看代码 边看文档 写代码\n\n程序员单纯编程部分，无非是\n\n1.  理解需求\n\n2. 搜索看别人怎么解决类似问题，理清思路\n\n3. 看已有项目的代码\n\n4. 看要用到的第三方库的源码或者文档\n\n\n\nAutoCoder 会模拟程序的这种行为来完成代码的编写。我们来看看具体如何让 AutoCoder 去完成这件事。\n\n```yml\nsource_dir: /tmp/t-py\ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nmodel: deepseek_chat\nmodel_max_length: 2000\nmodel_max_input_length: 30000\nanti_quota_limit: 0\n\nexecute: true\nauto_merge: true\n\nproject_type: py\n\nhuman_as_model: true\n\nurls: >\n  https://raw.githubusercontent.com/allwefantasy/byzer-llm/master/README.md\n\nquery: >\n  在 read_root 方法前新添加一个方法，\n  对应的rest 路径为 /llm, 该接口接受连个参数：query 和 model。\n  调用 llm.chat_oai 方法，然后返回结果。\n  你需要参考前面 byzer-llm 的文档来完成 llm.chat_oai 方法的调用。\n  注意， ByzerLLM 的初始化要放到新方法里。\n\n```\n\n这里，相比之前，我们新增了一个 urls 参数，你可以指定一份多份文档。因为我准备给我们的web server 添加一个大模型服务的功能，所以我把 byzerllm 的文档提供给了他。\n\n此外,urls 也支持YAML的数组格式，方便你指定多个文档：\n\n```yml\nurls:\n  - https://raw.githubusercontent.com/allwefantasy/byzer-llm/master/README.md\n  - /tmp/t-py/README.md\n```\n\n注意, urls 支持 Rest 和 本地路径两种，并且支持多种文本格式，诸如PDF,Word，txt, md等等。\n\n这里因为文档比较长，我使用了 human_as_model 功能，从而可以使用一些具有超长窗口的 web 模型。\n\n现在，我们打开 server.py 文件，可以看到新增了一个/llm 接口。\n\n```yml\n\n\nimport os\nimport ray\nfrom fastapi import FastAPI\nfrom byzerllm.utils.client import ByzerLLM\n\n# 创建FastAPI应用实例\napp = FastAPI()\n\n\n@app.get(\"/hello\")\ndef hello():\n    return {\"message\": \"world\"}\n\n@app.get(\"/llm\")\ndef llm_chat(query: str, model: str):\n    llm = ByzerLLM()\n    llm.setup_default_model_name(model)\n\n    result = llm.chat_oai(conversations=[{\n        \"role\": \"user\",\n        \"content\": query\n    }])\n\n    return {\"response\": result[0].output}\n\n# 定义根路径的GET请求处理函数，返回 \"Hello, World!\"\n@app.get(\"/\")\ndef read_root():\n    return {\"message\": \"Hello, World!\"}\n    \nif __name__ == \"__main__\":\n    # 启动web服务，端口改为9001\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=9001)\n```\n\n代码很完美，但其实漏掉了一个连接 ByzerLLM 集群的语句，就一行代码，你其实可以手动修改下的。\n\n不过考虑到你可能更喜欢指挥大模型干活，所以你修改下query,再执行一把：\n\n```yml\n...\nquery: >\n  修改 server.py ，在代码 app = FastAPI()后\n  增加 ray 的初始化连接代码。\n```\n\n这个时候他就会修改代码了，你可以看到他在第10行正确的添加了代码。\n\n```python\nimport os\nimport ray\nfrom fastapi import FastAPI\nfrom byzerllm.utils.client import ByzerLLM\n\n# 创建FastAPI应用实例\napp = FastAPI()\n\n# 初始化ray连接\nray.init(address=\"auto\", namespace=\"default\", ignore_reinit_error=True)\n\n@app.get(\"/hello\")\ndef hello():\n    return {\"message\": \"world\"}\n\n@app.get(\"/llm\")\ndef llm_chat(query: str, model: str):\n    llm = ByzerLLM()\n    llm.setup_default_model_name(model)\n\n    result = llm.chat_oai(conversations=[{\n        \"role\": \"user\",\n        \"content\": query\n```\n\n好，轮到我们检验成果的时候了。启动这个web服务：\n\n```shell\npython /tmp/t-py/server/server.py\n```\n\n访问我们/llm 接口：\n\nhttp://127.0.0.1:9001/llm?query=你好&model=qianwen_chat\n\n返回：\n\n![](../images/image11.png)\n\n恭喜，你成功完成了一个大模型的服务！\n\n考虑到今天这篇文章不断，所以如何使用搜索引擎我们放在下一篇。\n", "tag": "", "tokens": 1041, "metadata": {}}], "modify_time": 1716937398.3721128, "md5": "2dacd519508f1e99e30d909f67679b5b"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/025-AutoCoder知识库写代码的两种上下文模式.md", "relative_path": "025-AutoCoder知识库写代码的两种上下文模式.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/025-AutoCoder知识库写代码的两种上下文模式.md", "source_code": "# 025-AutoCoder知识库写代码的两种上下文模式\n\n我们在 [019-AutoCoder对本地文档自动构建索引](./019-AutoCoder%E5%AF%B9%E6%9C%AC%E5%9C%B0%E6%96%87%E6%A1%A3%E8%87%AA%E5%8A%A8%E6%9E%84%E5%BB%BA%E7%B4%A2%E5%BC%95.md) 提及了\n如何两条命令就构建好一个RAG索引，也在 [022-AutoCoder多知识库支持.md](./022-AutoCoder%E5%A4%9A%E7%9F%A5%E8%AF%86%E5%BA%93%E6%94%AF%E6%8C%81.md)\n提及了多知识库的支持。\n\n但是对于召回的内容，如何给到大模型参考，实际上  AutoCoder 提供了两种上下文模式：\n\n1. enable_rag_context\n2. enable_rag_search\n\n## enable_rag_context\n\nenable_rag_context 会取第一个chunk 对应的文章，作为上下文，这个上下文会被作为一个普通的文件给到大模型。\n\n## enable_rag_search\n\nenable_rag_search 会对 topN 个chunk 根据问题进行回答，把回答结果作为上下文，这个上下文也会被作为一个普通的文件给到大模型。\n\n## 如果你使用 ray://xxxx:10001 的方式连接知识库\n\n如果你使用 ray://xxxx:10001 的方式连接知识库，你需要在服务器上先启动一个知识库服务service：\n\n```bash\nauto-coder doc serve --host 127.0.0.1 --port 8001 --model deepseek_chat --emb_model gpt_emb  --collection auto-coder\n```\n\n如果是企业使用，你还可以通过添加一个 api_key 启动鉴权：\n\n```bash\nauto-coder doc serve .... --api_key 123456\n```            \n\n然后客户端使用时，需要使用如下配置：\n\n```yaml\nray_address: \"ray://127.0.0.1:10001\"\n\nrag_url: http://127.0.0.1:8001/v1\nrag_token: 123456\nenable_rag_search: 你希望查找的东西的描述\ncollections: auto-coder\n```\n\n除了 enable_rag_search 参数，你可以把其他参数都放到一个独立的 YAML文件，然后通过 include_file 指令引入，避免重复。\n\n这里简单解释下， rag_url 是我们启动的知识库服务的地址，rag_token 是我们启动知识库服务的时候指定的 token，如果没有指定 token,那么在使用时可以随意指定，但不能为空。\n\n当前客户端模式的一些限制：\n\n1. collections 则可以指定知识库名称,当前一次只能支持一个知识库。\n2. 当前只支持 enable_rag_search 参数，不支持 enable_rag_context 参数。\n\n## 最佳使用实践\n\nenable_rag_context/enable_rag_search 两个参数都支持 bool 或者字符串参数。我们推荐使用字符串参数。比如：\n\n```yml\nenable_rag_search: | \n   byzerllm  使用 openai_tts模型的 python 代码\ncollections: byzerllm\n\nquery: | \n   我们要在 audio.py 中实现一个新的类叫 PlayStreamAudioFromText，\n   该类有一个方法 run,\n   该方法输入是一个字符串generator，在方法内部会将文本转换为语音，并且播放出来。\n   \n   具体逻辑是：\n   1. PlayStreamAudioFromText 维护一个queue，一个线程池\n   1. 运行时，从generator中读取文本，然后将文本放入queue中\n   2. 从queue中取出文本，按中英文句号或者换行符对语句进行切割调用，\n      并行调用 openai_tts 模型将文本转换为语音，保存在 /tmp/wavs 目录下。\n      音频文件用 001.wav, 002.wav, 003.wav...的命名规则保存在一个目录下.\n   3. 使用一个独立的线程播放音频文件，播放完一个音频文件后，再播放下一个音频文件，直到播放完毕。   \n```\n\n如果 enable_rag_search 被设置为 bool 值(true),那么 AutoCoder 会把 query 作为问题，到指定的 byzerllm 知识库中检索信息。但实际效果可能非常差。\n在上面的示例中，我们直接明确的告诉RAG，我需要你检索 \"byzerllm  使用 openai_tts模型的 python 代码\" 的代码，然后他会给我一个完整的示例代码，然后这个\n示例代码会被作为上下文，配合你的 query, 一起给到大模型。\n\n下面是上面示例生成一段代码:\n\n```python\ndef text_to_speech(self, text, file_path):\n        print(f\"Converting text to speech: {text}\")\n        t = self.llm.chat_oai(conversations=[{\n            \"role\":\"user\",\n            \"content\": json.dumps({\n                \"input\": text,\n                \"voice\": \"echo\",\n                \"response_format\": \"wav\"\n            }, ensure_ascii=False)\n        }])\n        temp_file_path = file_path + \".tmp\"\n        with open(temp_file_path, \"wb\") as f:\n            f.write(base64.b64decode(t[0].output))\n        shutil.move(temp_file_path, file_path)\n        print(f\"Converted successfully: {file_path}\") \n```\n\n大模型自身兵不知道 openai_tts 模型的具体实现，但是通过RAG检索给到的示例代码，他最后知道如何调用 openai_tts 模型，并且写出了很漂亮的代码。\n\n", "tag": "", "tokens": 1195, "metadata": {}}], "modify_time": 1717751258.0941246, "md5": "ee4612517899cf71d6ecb491d8d22319"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/038-AutoCoder_为什么你需要经过反复练习才能用好.md", "relative_path": "038-AutoCoder_为什么你需要经过反复练习才能用好.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/038-AutoCoder_为什么你需要经过反复练习才能用好.md", "source_code": "# 038-AutoCoder_为什么你需要经过反复练习才能用好\n\nauto-coder 自身的使用并不复杂，基本有一两天的概念学习后就能上手。但是真正要获得好的工作效率提升，需要经过反复的练习，为什么呢？\n应该 auto-coder 的真正门槛在于如何合理的向它表述你的需求。\n\n注意，专业的表述需求并不是一件容易的事情，你和auto-coder 之间需要反复磨合，慢慢找到感觉，你说的话auto-coder 一听就懂，精准的帮你\n完成项目的开发。\n\n这里我们提供一些练习的方向，帮助您尽快的找到感觉。\n\n## 需求三要素\n\n在你的需求里，需要能够提现下面三件事：\n\n1. 关注的文件\n2. 修改逻辑或者目标\n3. 修改范例参考\n\n![](../images/038-01.png)\n\n在上面的范例中，我提及了两个关于文件的信息：\n\n1. 你需要修改的文件是以 code_generate开头的。\n2. 你的参考范例文件是 rest.py, 这里面有我以前写的和这次需求类似的代码。\n\n此外，我两次提及了修改逻辑：\n1. 检测 llm 是否有 code_model 这个值存在（目标1）\n2. 如果有，使用 code_model 而不是 llm 来处理（目标2）\n\n你实际上可以合并成一句话，但是因为我实际在写的时候，并没有仔细去考量，所以我顺手就写成这样了。\n\n最后，我通过给出了参考范例，这样就无需我描述详细的修改，实现描述的简洁明了。\n\n只有拥有了这些信息，auto-coder 最后提交的代码才会是你想要的。\n\n## 对于一个项目新手该怎么办\n\n你可能第一次接触一个项目，所以你不知道怎么入手。实际上你也可以完全使用auto-coder 当做一个阅读工具来用。比如你可以这么用：\n\n```yaml\ninclude_file:\n  - ./base.yml\n\nindex_filter_level: 2\nquery: |\n  为什么 udiff_coder 进行 get_edits  和 apply_edits 时，diff @@ ... @@ 不需要包括\n  行号。\n\n```\n\n把生成的output.txt 文件拖拽到 web 版本的大模型里，就可以看到对于该问题的详细回答。不过阅读代码依然要遵循三要素描述\n\n1. 关注的文件/函数/变量 比如这里我们提及 udiff_coder 是一个文件名。然后我的问的问题也比较具体，就是针对两个方法的行为。\n\n你如果只是希望对这个项目有个概览，你可以使用 auto-coder 知识库的能力。这个我们在  000 篇告诉过你如何使用。\n\n## 比较新的功能，没有参考范例怎么办？\n\n有时候，你可能会遇到一些新的功能，这个时候你可能没有参考范例，不过我们依然可以让他基本一到两次就能提交给你一个满意的commit，\n\n我们来看一个实际的例子：\n```yaml\nquery: |      \n   你可以参考 byzerllm.utils.connect_ray import connect_cluster 中的相关代码，修改下面的代码，添加JDK环境的，来实现环境检测：\n   def detect_env() -> EnvInfo:\n        os_name = sys.platform\n        os_version = \"\"\n        if os_name == \"win32\":\n            os_version = sys.getwindowsversion().major\n        elif os_name == \"darwin\":\n            os_version = subprocess.check_output([\"sw_vers\", \"-productVersion\"]).decode('utf-8').strip()\n        elif os_name == \"linux\":\n            os_version = subprocess.check_output([\"uname\", \"-r\"]).decode('utf-8').strip()\n         \n        python_version = f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\"\n        \n        conda_env = os.environ.get(\"CONDA_DEFAULT_ENV\")\n        \n        virtualenv = os.environ.get(\"VIRTUAL_ENV\")\n        \n        has_bash = True\n        try:\n            subprocess.check_output([\"bash\", \"--version\"])\n        except:\n            has_bash = False\n            \n        return EnvInfo(\n            os_name=os_name,\n            os_version=os_version,\n            python_version=python_version,\n            conda_env=conda_env,\n            virtualenv=virtualenv,\n            has_bash=has_bash\n        )\n\n   然后在 byzerllm/apps/command.py 中的install方法中，通过环境检测，发现如果用户没有 JDK21,我们需要根据系统环境自动下载如下JDK。\n   下面是不同的平台JDK21的下载地址：\n\n   1. https://download.java.net/java/GA/jdk21.0.2/f2283984656d49d69e91c558476027ac/13/GPL/openjdk-21.0.2_linux-x64_bin.tar.gz\n   2. https://download.java.net/java/GA/jdk21.0.2/f2283984656d49d69e91c558476027ac/13/GPL/openjdk-21.0.2_macos-x64_bin.tar.gz\n   3. https://download.java.net/java/GA/jdk21.0.2/f2283984656d49d69e91c558476027ac/13/GPL/openjdk-21.0.2_windows-x64_bin.zip\n   \n   下载要求能展示进度以及下载完成后自动解压。\n```\n\n这个需求是 byzer-llm 项目中一个实际需求，我想添加一个环境检测的功能，然后根据环境检测，自动下载JDK21。\n\n根据前面的三要素，我们可以看到：\n\n1. 相关文件： “你可以参考 byzerllm.utils.connect_ray import connect_cluster 中的相关代码”然后在 byzerllm/apps/command.py 中的install方法中\"\n2. 修改逻辑：添加环境检测，然后根据环境检测，自动下载JDK21\n3. 参考范例：connect_ray.py 中的相关代码\n\n因为环境检测的代码我以前已经写过，所以直接贴给它。如果你没结果，你可以这么描述，让auto-coder自己发挥：\n\n```yaml\n你可以参考 byzerllm.utils.connect_ray import connect_cluster 中的相关代码，实现下面的方法来完成对当前操作系统的环境检测：\n   \n   def detect_env() -> EnvInfo:\n\n   其中 EnvInfo 对象是一个数据类，包含了当前操作系统的信息，python版本，conda环境，virtualenv环境，是否有bash等信息。           \n\n   然后在 byzerllm/apps/command.py 中的install方法中，通过环境检测，发现如果用户没有 JDK21,我们需要根据系统环境自动下载如下JDK。\n   下面是不同的平台JDK21的下载地址：\n\n   1. https://download.java.net/java/GA/jdk21.0.2/f2283984656d49d69e91c558476027ac/13/GPL/openjdk-21.0.2_linux-x64_bin.tar.gz\n   2. https://download.java.net/java/GA/jdk21.0.2/f2283984656d49d69e91c558476027ac/13/GPL/openjdk-21.0.2_macos-x64_bin.tar.gz\n   3. https://download.java.net/java/GA/jdk21.0.2/f2283984656d49d69e91c558476027ac/13/GPL/openjdk-21.0.2_windows-x64_bin.zip\n   \n   下载要求能展示进度以及下载完成后自动解压。\n```\n\nauto-coeder 也能顺利完成最终需求。\n\n## 你的需求似乎都比较细节，不能提一个笼统的需求么？\n\n我们在做实际的项目迭代，你会发现，我们是要能够比较精准控制我们最后的代码的，因为你的代码要能够满足你已有项目的一些风格以及一些规范，包括你需要较为准确的控制你的代码的逻辑，所以正常你应该尽可能提供详细的需求和实现逻辑。通过参考范例，可以极大的简化你的业务逻辑描述。\n\n你当然也可以提一个笼统的需求，但最后auto-coder 可能实现了你最后的目标，但是当你review commit 的时候，你可能会需要较多的修改满足项目自身的一些规范和需求。\n\n对于笼统的需求，我们倒也有一个例子：\n\n```yaml\nsource_dir: /home/winubuntu/projects/byzer-llm/saas\ntarget_file: /home/winubuntu/projects/byzer-llm/output.txt\n\nmodel: deepseek_chat\nmodel_input_max_length: 30000\n\nurls: https://raw.githubusercontent.com/allwefantasy/byzer-llm/master/README.md\n\nsearch_engine: bing\nsearch_engine_token: ENV {{BING_SEARCH_TOKEN}}\n\nquery: |\n  在 src/byzerllm 目录下新增一个 byzerllm.py 文件。在该文件中使用args 实现命令行支持。 参考 README.md 中的使用方法来增加命令行参数。\n  主要支持：\n  1. 部署模型相关参数\n  2. 运行推理相关阐述\n\n  比如部署模型，一般代码是这样的：\n  \n  \n  ray.init(address=\"auto\",namespace=\"default\",ignore_reinit_error=True)\n  llm = ByzerLLM()\n\n  llm.setup_gpus_per_worker(4).setup_num_workers(1)\n  llm.setup_infer_backend(InferBackend.transformers)\n\n  llm.deploy(model_path=\"/home/byzerllm/models/openbuddy-llama2-13b64k-v15\",\n            pretrained_model_type=\"custom/llama2\",\n            udf_name=\"llama2_chat\",infer_params={})\n  \n  此时你需要有 address, num_workers, gpus_per_worker, model_path, pretrained_model_type, udf_name, infer_params 这些参数可以通过命令行传递。\n\n  最终形态是： \n\n  byzerllm deploy --model_path /home/byzerllm/models/openbuddy-llama2-13b64k-v15 --pretrained_model_type custom/llama2 --udf_name llama2_chat --infer_params {}\n\n  同理推理是也是。比如一般推理代码是：\n\n  \n  llm_client = ByzerLLM()\n  llm_client.setup_template(\"llama2_chat\",\"auto\")\n\n  v = llm.chat_oai(model=\"llama2_chat\",conversations=[{\n      \"role\":\"user\",\n      \"content\":\"hello\",\n  }])\n\n  print(v[0].output)\n  \n  此时你需要有 model, conversations 这些参数可以通过命令行传递。\n\n  此时你的命令行形态是：\n  \n  byzerllm query --model llama2_chat --query \"hello\" --template \"auto\"\n```  \n\n我们要实现byzer-llm 一个命令行，可以看到，我只是把我最终想要的样子给到auto-coder看，然后还给了一些命令行对应的python API 的样子，希望他能举一反三。基本执行后效果就已经非常好了，你只需要对commit 再做一次小修改能满足整个功能需求。\n\n## 那能不能走到另外一个极端，比如我想控制一个变量的使用\n\n当然也是有的，这个有点相当于详细的实现设计：\n\n```yaml\nquery: | \n   我们要在 audio.py 中实现一个新的类叫 PlayStreamAudioFromText，\n   该类有一个方法 run,\n   该方法输入是一个字符串generator，在方法内部会将文本转换为语音，并且播放出来。\n   \n   具体逻辑是：\n   1. PlayStreamAudioFromText 维护一个queue，一个线程池\n   1. 运行时，从generator中读取文本，然后将文本放入queue中\n   2. 从queue中取出文本，按中英文句号或者换行符对语句进行切割调用，\n      并行调用 openai_tts 模型将文本转换为语音，保存在 /tmp/wavs 目录下。\n      音频文件用 001.wav, 002.wav, 003.wav...的命名规则保存在一个目录下.\n   3. 使用一个独立的线程播放音频文件，播放完一个音频文件后，再播放下一个音频文件，直到播放完毕。\n```\n\n我们把 PlayStreamAudioFromText 的详细实现逻辑都一一进行了描述。基本上auto-coder 就能很好的完成这个类。这个类相对来说也比较独立，所以我们没有提及范例以及其他文件。单纯让auto-coder 在一个已经有的空audio.py文件中实现这个类。\n\n控制粒度其实很细了，我明确要求他必须有个queue队列，然后描述如何操作这个队列。\n\n\n", "tag": "", "tokens": 2599, "metadata": {}}], "modify_time": 1717301689.19478, "md5": "9aa0d9ecfa4edafb3771a7704d646d5b"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/031-AutoCoder_正则表达式排除目录.md", "relative_path": "031-AutoCoder_正则表达式排除目录.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/031-AutoCoder_正则表达式排除目录.md", "source_code": "# 031-AutoCoder_正则表达式排除目录\n\n当系统在过滤文件时，会默认排除以下目录：\n\n```\n[\".git\", \".svn\", \".hg\", \"build\", \"dist\", \"__pycache__\", \"node_modules\"]\n```\n\n如果你还想排除一些其他目录或者文件，你可以使用 `exclude_files` 参数。下面是一个例子：\n\n```yaml\ninclude_file: \n   - ./common/remote.yml\n\nproject_type: .py\nexclude_files:      \n   - human://任何包含 common 的目录\n\nquery: |   \n   SuffixProject 支持 exclude_files 参数，用于排除不需要处理的目录。 exclude_files 参数\n   支持正则表达式，并且支持yaml的数组格式。\n```\n\n通过 project_type 我们会收集项目里所有.py后缀的文件，同时通过指定 exclude_files 参数，排除了所有包含 common 的目录。\n\nexclude_files 支持两种模式的配置:\n\n1. human:// 开头的字符串，AutoCoder 会根据后面的文本，自动生成正则表达式。比如上面的例子，会自动生成 `.*common.*` 的正则表达式。\n2. regex:// 开头的字符串，AutoCoder 会直接使用后面的文本作为正则表达式。比如 `regex://.*common.*`。\n\n你可以在日志中看到相关信息：\n\n```\n2024-05-19 14:54:12.498 | INFO     | autocoder.suffixproject:parse_exclude_files:58 - Generated regex pattern: .*/common/.*\n```\n\n以及哪些文件因此被排除：\n\n```\n2024-05-19 14:54:12.526 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/code_auto_execute.py\n2024-05-19 14:54:12.526 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/JupyterClient.py\n2024-05-19 14:54:12.526 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/screenshots.py\n2024-05-19 14:54:12.526 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/command_templates.py\n2024-05-19 14:54:12.526 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/__init__.py\n2024-05-19 14:54:12.527 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/types.py\n2024-05-19 14:54:12.527 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/ShellClient.py\n2024-05-19 14:54:12.527 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/cleaner.py\n2024-05-19 14:54:12.527 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/search.py\n2024-05-19 14:54:12.527 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/audio.py\n2024-05-19 14:54:12.527 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/code_auto_merge.py\n2024-05-19 14:54:12.527 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/code_auto_generate.py\n2024-05-19 14:54:12.527 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/git_utils.py\n2024-05-19 14:54:12.527 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/const.py\n2024-05-19 14:54:12.528 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/image_to_page.py\n2024-05-19 14:54:12.528 | INFO     | autocoder.suffixproject:should_exclude:67 - Excluding file: /Users/allwefantasy/projects/auto-coder/src/autocoder/common/llm_rerank.py\n```\n\n```\n\n\n\n", "tag": "", "tokens": 1253, "metadata": {}}], "modify_time": 1716990303.5786288, "md5": "9b02e8559f2252f9d445a9a55264bf6f"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/029-AutoCoder_混合项目如何开发.md", "relative_path": "029-AutoCoder_混合项目如何开发.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/029-AutoCoder_混合项目如何开发.md", "source_code": "# 029-AutoCoder_混合项目如何开发\n\n比如我有一个项目目录A，A里面有：\n\n1. web (这是一个vue前端项目)\n2. server (这是一个python后端项目)\n\n那我应该如何设置 project_type/source_dir 呢？\n\n你可以直接把 source_dir 可以设置为 A 项目，此时你可能面临下面三种情况：\n\n## 只修改web\n\n这个时候你可以将 project_type 设置为 `ts`。 如果 `ts`无法满足你，你可以手动设置需要收集的文件后缀名，比如 `.vue`之类的，多个可以用逗号隔开。\n如果你修改web的代码，需要参考后端的代码，你可以通过 urls 来设置一些后端代码的路径。\n\n## 只修改server\n\n这个时候你可以将 project_type 设置为 `py`(如果是java之类的，那么设置成 `.java`,多个可以用逗号隔开。)。同理如果需要参考前端代码，你可以通过 urls 来设置一些前端代码的路径。\n\n## web和server都要修改\n\n这个时候你可以将 project_type 设置为你需要关注的文件的后缀名，比如 `.ts,.vue,.py`。\n\n\n", "tag": "", "tokens": 235, "metadata": {}}], "modify_time": 1716039507.2597656, "md5": "92f8bc48a1f71a8dd106f7ddfce56017"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/017-AutoCoder指定专有索引模型.md", "relative_path": "017-AutoCoder指定专有索引模型.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/017-AutoCoder指定专有索引模型.md", "source_code": "# 017-AutoCoder指定专有索引模型\n\n> AutoCoder >= 0.1.27 特性\n\n如果你的项目很大，里面有海量的代码，如果用比较大的模型，速度和成本确实比较高。所以我们将构建索引的功能单独了出来，允许\n你单独指定一个模型来完成。\n\n我们来看下面一个例子：\n\n```yml\nsource_dir: /Users/allwefantasy/projects/tt/\ntarget_file: /Users/allwefantasy/projects/auto-coder/output.txt \nproject_type: ts\n\n\nmodel: deepseek_chat\nmodel_max_length: 2000\nmodel_max_input_length: 30000\nanti_quota_limit: 0\n\n\nindex_model: sparkdesk_chat\nindex_model_max_length: 2000\nindex_model_max_input_length: 6000\nindex_model_anti_quota_limit: 1\n\nskip_build_index: false\nexecute: true\nauto_merge: true\nhuman_as_model: true\n\nquery: |   \n   将html转换成 reactjs+typescript+tailwind 实现    \n```\n\n在这个例子里，我们通过 `skip_build_index` 参数开启了索引，然后默认模型我们用 QwenMax, 但是这个模型速度比较慢，比较贵，所以 我又通过参数 `index_model` 来指定一个专门的索引模型。 同样的，对于索引，你可以指定单次请求的最大输入和输出，以及为了防止模型的限速，你可以设置每次请求 后的停顿时间。\n\n这样你就可以使用一些私有的或者较便宜的模型来完成索引的构建。但是注意的是，很多模型能力实在太弱，构建索引需要该模型能够识别代码里面的包导入语句，变量，函数等，但是很多模型连这个都做不到。\n\n为此，我们提供了一些校验命令，方便你快速的测试哪些模型可以达到合适的效果。\n\n执行下面的命令可以构建索引：\n\n```bash\nauto-coder index --model kimi_chat --index_model sparkdesk_chat --project_type py --source_dir YOUR_PROJECT\n```\n\n此时项目会使用 `sparkdesk_chat` 模型来构建索引，构建完成后，你可以在项目的 .auto-coder/index.json 文件中看到索引的内容。\n\n接着你可以使用索引来查找文件：\n\n```bash\nauto-coder index-query --model kimi_chat --index_model sparkdesk_chat --source_dir YOUR_PROJECT --query \"添加一个新命令\"\n```\n\n之后就可以看看查找是否准确。你可以将 model 和 index_model 设置为相同的值。\n\n对于上面的命令，如果你想做更精细的控制，可以使用 `--file` 来指定 YAML 文件。\n", "tag": "", "tokens": 546, "metadata": {}}], "modify_time": 1716937536.618057, "md5": "a1e48034b80af4a64d56f04b32c36651"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/013-AutoCoder如何查看Token消耗.md", "relative_path": "013-AutoCoder如何查看Token消耗.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/013-AutoCoder如何查看Token消耗.md", "source_code": "# 013-AutoCoder如何查看Token消耗\n\n> auto-coder >= 0.1.21 可用\n\n可以执行如下命令：\n\n```shell\nauto-coder store --source_dir YOUR_PROJECT\n```\n\n注意，需要确保你的项目里有 .auto-coder 目录。\n\n输出结果：\n\n```\n+-----------------+----------------------+--------------------------+\n| project         |   input_tokens_count |   generated_tokens_count |\n+=================+======================+==========================+\n| ByzerRawCopilot |                51391 |                     3321 |\n+-----------------+----------------------+--------------------------+\n```", "tag": "", "tokens": 134, "metadata": {}}], "modify_time": 1711809217.9069142, "md5": "63fd75991aedcce264cbc3cc479e54d1"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/007-番外篇 AutoCoder里配置的model究竟用来干嘛.md", "relative_path": "007-番外篇 AutoCoder里配置的model究竟用来干嘛.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/007-番外篇 AutoCoder里配置的model究竟用来干嘛.md", "source_code": "# 007-番外篇 AutoCoder里配置的model究竟用来干嘛\n\nAutoCoder 最简化的配置是这样的：\n\n```yml\n\nsource_dir: /tmp/t-py\ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nproject_type: py\n\nquery: >\n  修改 server.py ，在代码 app = FastAPI()后\n  增加 ray 的初始化连接代码。\n```\n\n指定项目地址，指定 AutoCoder 根据你问题生成的prompt的存储地址，项目类型以及你的需求。\n\n这个时候，AutoCoder 是不需要大模型的。我们来看看那些参数是需要model配合的。\n\n## urls\n\n你可能会指定 urls ,让 AutoCoder 参考一些文档。实际上，对这个参数而言，model 是可选的，如果设置了model，那么我们会用model去对抓取的数据做清洗，从而获得更好的效果。如果没有设置，那么就是简单的去处html, 保留文本，但是可能会有很多干扰信息。\n\n## skip_build_index\n\n开启索引的话，是必须要配合 model 才行的。因为索引的构建需要model对文件做分析，查询的时候需要model做过滤。\n\n## searh_engine, search_engine_token\n\n搜索引擎的支持，model 也是必须的。因为要过滤搜索得到的文档，并且对文档做打分，避免文档太多太长或者不想管而影响最终效果。\n\n## human_as_model\n\n这个参数可以让 model 之完成一些基础功能，最后生成代码的部分交给 Web 版本模型。\n\n## execute\n\n这个参数会让 AutoCoder 让 model 直接执行Prompt，然后将结果写入到target_file 中。所以对这个参数而言，model也是必须的。\n\n\n\n下面是一个典型model配置：\n\n```yml\n\nmodel: qianwen_chat\nmodel_max_length: 2000\nmodel_max_input_length: 100000\nanti_quota_limit: 5\n```\n\n首先配置 model 的实例名称，也就是你通过 Byzer-LLM 部署的模型。接着你根据你模型的要求，配置模型的最大生成token,最大输入token。\n\n此外，因为很多模型都有限速，所以你可以设置调用一次后，多久在调用第二次，单位是秒。\n\n## 结论\n\n配置 model 后，可以让 AutoCoder 更加智能。如果你的项目已经很大，那么为了开启索引，则你必须使用model.\n\n对于model的要求，请至少保证该model 的窗口长度>32K,以保证在生产环境里获得一个不错的体验。", "tag": "", "tokens": 523, "metadata": {}}], "modify_time": 1711809217.9061852, "md5": "5651922e85262ffc77f0bcd5b8d8de65"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/037-AutoCoder_项目快速修bug实战.md", "relative_path": "037-AutoCoder_项目快速修bug实战.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/037-AutoCoder_项目快速修bug实战.md", "source_code": "# 037-AutoCoder_项目快速修bug实战\n\n## auto-coder 项目开发流程\n\n使用 auto-coder 的开发流程如下：\n\n1. 编写一个YAML文件来描述您的需求，Auto-Coder将生成代码并将代码合并到您的项目中。\n2. 检查Auto-Coder的提交，并在vscode或其他IDE中审查代码。\n3. 如果提交是基本满足需求，您可能仍需要使用github copilot或其他工具手动修改代码。\n4. 如果auto-coder提交的代码不满足你需求，您需要撤销提交并修改YAML文件，并 1 步骤重新开始。\n5. 重复上述步骤，直到您对auto-coder生成的代码满意。\n\n今天这个实例会展示如何使用上述流程快速修复一个bug。\n\n## Bug描述\n\n今天我们种子用户群给我提了个 Bug:\n\n![](../images/037-01.png)\n\n\n最后发现是因为我们使用了绝对路径，所以导致过滤不出文件：\n\n![](../images/037-02.png)\n\n\n我打算修正这个问题，并且实时直播给一个同学看我如何在三分钟内修正这个问题。\n\n## 修正流程（多图预警）\n\n在项目里新建一个 auto-coder yaml文件描述我的需求：\n\n![](../images/037-03.png)\n\n\n根据用户反馈的信息，我大概知道可以在加载索引的时候做个校验判断。所以我按图中进行描述，描述我想让 auto-coder 做的事情。\n\n接着通过让 auto-coder 自动完成bug修正了：\n\n```bash\nauto-coder --file actions/061_index_检测索引文件的有效性.yml \n```\n\n执行期间会弹出一个绿框，问我这些文件是不是能满足后续的修改需求。我点击 OK。\n\n因为我喜欢用 human_as_model 模式，所以 auto-coder 会给我一个文件，我把文件贴到大模型里：\n\n![](../images/037-04.png)\n\n\n然后整个大模型的回复(不单单是代码部分，而是完整的回复)我完整贴回给auto-coder:\n\n![](../images/037-05.png)\n\n\n这个时候auto-coder 会以diff模式提交代码（推荐 editblock, 也支持wholefile, wholefile 模式更稳定，但消耗的token也非常巨大）。auto-coder 运行完成后，\n你可以打开 vscode 的侧边栏看提交记录：\n\n![](../images/037-06.png)\n\n\n整个bug修复就改了两行代码，加的位置也非常准确，判断逻辑也非常准确。我review 后表示 LGTM。\n\n接着我们校验下（其实也可以顺带让他生成一个测试代码啦，可惜我做的时候搞的太急，忘了提这事了），我手工修改出一个错误出来：\n\n![](../images/037-07.png)\n\n\n然后再运行下：\n\n![](../images/037-08.png)\n\n\n可以看到正常的报错了，到此算是收工。当然，你可以再新建一个 yaml 然后添加测试用例。\n\n\nPS: 实际上我试了两次，一次用的 Opus,一次用的GPT4来做代码生成的部分, 前者需要人工做下编辑（合并过程中换行没搞对），后者一次过。如果你觉得不好，可以随时 `auto-coder revert --file actions/061_index_检测索引文件的有效性.yml` 撤销该次修改，然后修改，再次执行重复前面的流程。\n\n## 总结\n\nAuto-Coder 可以让你基本做到不用打开代码编辑器就能完成修改代码的需求，而且速度也足够快，具备生产落地的可能性，也可能会变革人们的开发流程和发生智能变化。比如自身工程师不断地写用auto-coder 通过文字来进行项目迭代，普通研发做review 和 测试开发（也用 auto-coder）等。可能还会有一些其他新的工种诞生。", "tag": "", "tokens": 767, "metadata": {}}], "modify_time": 1717565447.0046427, "md5": "0e892522cd01798993a6ad1cfac50404"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/026-AutoCoder模型的窗口长度控制.md", "relative_path": "026-AutoCoder模型的窗口长度控制.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/026-AutoCoder模型的窗口长度控制.md", "source_code": "# 026-AutoCoder模型的窗口长度控制\n\n不同的模型的窗口是不一样的，目前比较主流的是8k和32k,128k。 建议使用32k 起步。\n\n不同的SaaS模型，对输入的控制也是不一样的，比如 QwenMax 的输入是控制在6000个字符，而不是token。这个是特别需要注意的。\n此外，大部分模型都控制了请求频率，我们在使用 AutoCoder 的时候，这样就比较容易失败。\n\n为了避免上述这些情况导致的失败，我们推荐用一些更加宽松的模型，比如 deepseek v2 以及 GPT 3.5 或者 Haiku 来做AutoCoder 的驱动和索引模型。\n\n如果你无法使用上述模型，比如就只能使用 QwenMax,那么可以通过如下配置来避免失败：\n\n```yml\n# 当前模型的最大生成长度\nmodel_max_length: 2000\n# 当前模型的最大输入长度\nmodel_max_input_length: 6000\n# 当前模型每次请求后休息的时间，避免触发频率限制\nmodel_anti_quota_limit: 1\n\n# 功能如上，专门正对构建索引的配置\nindex_model_max_length: 2000\nindex_model_max_input_length: 6000\nindex_model_anti_quota_limit: 1\n```\n\n如果超出长度，AutoCoder 会自动进行切分，从而保证每次请求都不会超出长度限制，从而避免失败，但这也极大的影响了效率。\n\n## 如果你使用 DeepSeekV2 来做索引和驱动\n\n```yml\nmodel: deepseek_chat\nindex_model: deepseek_chat\n\nskip_build_index: false\nindex_filter_level: 1\nindex_model_max_input_length: 30000\nindex_filter_workers: 4\nindex_build_workers: 4\n```\n\n一个不错的配置。记得你在部署 deepseek 的时候，也要保证 并发度>4,否则容易出问题。\n\n下面是一个示例部署：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 30 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n这里我们把并发设置为 30。\n\n", "tag": "", "tokens": 517, "metadata": {}}], "modify_time": 1715853104.0095325, "md5": "f95bc0cc472360f673f4a09a94917d5d"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/043_AutoCoder_Windows安装特别说明.md", "relative_path": "043_AutoCoder_Windows安装特别说明.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/043_AutoCoder_Windows安装特别说明.md", "source_code": "# 043_AutoCoder_Windows安装\n\nWindows 下安装有两种方式。\n\n1. 直接安装\n2. 通过WSL安装\n\n> 推荐具备条件的，优先使用WSL安装。\n\n## 直接安装\n\n### 通过 Conda （推荐）\n\n1.安装 Conda 并且创建环境：\n\n    下载 Windows 版本的 [Miniconda 安装包](https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe)。安装\n\n    > 安装过程中，请务必选择将 miniconda 目录 添加到 Path环境变量中选项，否则你再 打开 PowerShell/CMD 会提示找不到 conda 命令。\n    \n    完成后在 shell 中执行下面的命令：\n    \n    conda create --name auto-coder python==3.11.9\n    conda activate auto-coder    \n\n2.设置环境变量\n    \n    高级系统设置->环境变量->RAY_ENABLE_WINDOWS_OR_OSX_CLUSTER，默认值给1。\n    高级系统设置->环境变量->PYTHONUTF8，默认值给1。 \n    高级系统设置->环境变量->PYTHONENCODING，默认值给utf-8。   \n\n3.配置pip源 \n    \n    pip config set global.index-url  https://pypi.tuna.tsinghua.edu.cn/simple\n\n4.安装auto-coder\n    \n    pip install -U auto-coder\n    ray start --head    \n\n5.安装完成后，可以通过下面的命令快速验证下：\n    \n    auto-coder -h    \n\n\n后续步骤可以回到： [000-AutoCoder_准备旅程.md](./000-AutoCoder_%E5%87%86%E5%A4%87%E6%97%85%E7%A8%8B.md) 即可。\n\n\n### 不通过 Conda\n\n1.安装python([3.10.11-python-3.10.11-amd64-full.exe](https://www.python.org/downloads/release/python-31011/))，客户端和服务端python和ray版本需保持一致。\n\n2.设置环境变量\n    \n    高级系统设置->环境变量->RAY_ENABLE_WINDOWS_OR_OSX_CLUSTER，默认值给1。\n    高级系统设置->环境变量->PYTHONUTF8，默认值给1。    \n\n3.配置pip源 \n    \n    pip config set global.index-url  https://pypi.tuna.tsinghua.edu.cn/simple    \n\n4.安装auto-coder\n    \n    pip install -U auto-coder\n    ray start --head    \n\n5.安装完成后，可以通过下面的命令快速验证下：\n    \n    auto-coder -h    \n\n\n后续步骤可以回到： [000-AutoCoder_准备旅程.md](./000-AutoCoder_%E5%87%86%E5%A4%87%E6%97%85%E7%A8%8B.md) 即可。\n\n## 通过WSL安装\n\n第一步，配置一个wsl，详细版本参见社区用户的一个文档[http://xuzhougeng.com/archives/building-a-wsl-based-python-data-science-environment](http://xuzhougeng.com/archives/building-a-wsl-based-python-data-science-environment)，如下是简明版。\n\n首先，得确保自己的操作系统符合要求，对于windows11直接就可以了，对于Windows 10 ，要求高于version 2004  或高于Build 19041，可以通过打开powershell运行如下命令确认。\n\n```Python\nGet-ComputerInfo -Property \"OsName\", \"OsVersion\", \"WindowsProductName\", \"WindowsEditionId\", \"OsHardwareAbstractionLayer\"\n```\n\n![image](../images/043-00.png)\n\n\n接着，在powershell中运行如下`wsl --install`, 跟着提示操作就可以，中间可能需要重启一下电脑。\n\n默认情况下，WSL使用宿主机的一般内存，例如你的电脑配置是16G内存，那么WSL最多就用8G。\n如果你内存比较小，可能会遇到 OOM，要在 windows 用户文件夹下建一个\n\" .wslconfig \" 文件，如何修改见详细版。\n\n第二步，在WSL中安装conda, 我用的是清华镜像提供的Miniconda\n\n```Bash\nwget https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-py38_4.12.0-Linux-x86_64.sh\nbash Miniconda3-py38_4.12.0-Linux-x86_64.sh\n```\n\n根据提示安装结束后，重新打开wsl，就可以进行下一步。\n\n第三步，安装auto-coder。\n\n```Bash\nconda create --name auto-coder python=3.10.11\nconda activate auto-coder\npip install -U auto-coder\nray start --head  --port 6380\n```\n\n需要注意的是，假设你运行`ray start`的事后，遇到了如下报错，就说明你的默认6379端口被占用了，比如说我就是，所以我换成了6380\n\n```Python\nRuntimeError: Failed to start GCS.  Last 1 lines of error files:\n[2024-06-04 10:09:50,260 C 15013 15013] (gcs_server) grpc_server.cc:124:  Check failed: server_ Failed to start the grpc server. \nThe specified port is 6379. This means that Ray's core components will not be able to function correctly. \nIf the server startup error message is `Address already in use`, \nit indicates the server fails to start because the port is already used by other processes (such as --node-manager-port, --object-manager-port, --gcs-server-port, and ports between --min-worker-port, --max-worker-port).\nTry running sudo lsof -i :6379 to check if there are other processes listening to the port.\n```\n\n后续步骤可以回到： [000-AutoCoder_准备旅程.md](./000-AutoCoder_%E5%87%86%E5%A4%87%E6%97%85%E7%A8%8B.md) 即可。\n\n### 常见问题\n\nWindows有个路径长度问题(win10/win11):\n\n![](../images/043-02.png)\n\n可以通过改注册表解决：\n\n1.win+r：regedit；\n2.HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem\\LongPathsEnabled，改为1\n\n\n### 另外一位网友补充\n\n在 Windows 11 上通过 Windows Subsystem for Linux (WSL) 安装 Ubuntu 22.04 的步骤如下：\n\n1. 启用 WSL 和虚拟机平台\n打开“开始”菜单，搜索并选择“启用或关闭 Windows 功能”。\n在弹出的窗口中，勾选“虚拟机平台”和“Windows Subsystem for Linux”选项，然后点击“确定”。\n系统会进行更改，可能需要几分钟，然后会提示你重启计算机。重启系统以使更改生效。\n\n2. 安装 Ubuntu 22.04\n\n#### 方法一：通过 Microsoft Store 安装\n\n1. 重启后，打开“开始”菜单，搜索并打开 Microsoft Store。\n2. 在 Microsoft Store 中搜索“Ubuntu 22.04”。\n3. 找到 Ubuntu 22.04 LTS 页面，点击“获取”按钮进行下载。\n4. 下载完成后，从“开始”菜单中打开 Ubuntu 22.04。\n5. 按照提示创建一个新的用户账户并进行一些小的配置设置。\n\n#### 方法二：使用命令行安装\n\n打开“开始”菜单，搜索并右键点击“命令提示符”或“PowerShell”，选择“以管理员身份运行”。\n输入以下命令以安装 WSL 和 Ubuntu 22.04：\n\n```bash\nwsl --install -d Ubuntu-22.04\n```\n\n系统会自动下载并安装所需的 Linux 组件和 Ubuntu 22.04 发行版。安装完成后，重启计算机。\n\n配置和使用 Ubuntu 22.04\n\n安装完成后，打开“开始”菜单，搜索并打开 Ubuntu 22.04。\n你将看到一个命令行终端，提示你创建一个新的 Linux 用户名和密码。输入并确认密码。\n现在，你可以在终端中运行几乎所有与物理 Ubuntu 22.04 机器上相同的命令。\n\n注意事项\n\n确保你的 WSL 安装保持最新，以获取最新的安全补丁和功能更新。可以使用以下命令更新 WSL 内核：\n\n```bash\nwsl --update\n```\n\n如果你内存比较小，可能会遇到 OOM，要在 windows 用户文件夹下建一个\n\" .wslconfig \" 文件:\n\n![image](../images/043-01.png)\n\n参考图片中的参数进行配置。\n\n后续步骤可以回到： [000-AutoCoder_准备旅程.md](./000-AutoCoder_%E5%87%86%E5%A4%87%E6%97%85%E7%A8%8B.md) 即可。\n\n\n## 注意事项\n\n1. Windows 7/10 可以正常安装。\n2. Windows 11 家庭版有用户反馈存在一定问题，Ray 无法正常启动 （推荐尝试后文中的 WSL） 。\n3. Python官方目前说 3.10.xx系列已经不支持 Windows 7，可以到这里下载一个第三方包： https://github.com/adang1345/PythonWin7/tree/master/3.10.11 （经过测试）， 或者 https://github.com/NulAsh/cpython/releases/tag/v3.10.1win7-1 (未经过测试)。 你也可以使用 3.11.9 版本，但是这个版本还没有经过更多测试。", "tag": "", "tokens": 2040, "metadata": {}}], "modify_time": 1721266456.1549573, "md5": "0667cc49e209391d1e8fa1a8e726b80b"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/044_AutoCoder_新手顺手指南.md", "relative_path": "044_AutoCoder_新手顺手指南.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/044_AutoCoder_新手顺手指南.md", "source_code": "# 044_AutoCoder_新手顺手指南\n\n当你完成了第一篇内容 [000-AutoCoder_准备旅程](./000-AutoCoder_准备旅程.md),你应该把你电脑的开发环境设置好了。\n\n\n设置好环境后，务必阅读下这篇文章：[037-AutoCoder_项目快速修bug实战](./037-AutoCoder_项目快速修bug实战.md)来了解 auto-coder 的基本使用流程，当你阅读完这篇内容之后，你应该在脑海里形成一个全新的开发流程。\n\n您的开发新方式是：\n\n1. 编写一个YAML文件来描述您的需求，Auto-Coder将生成代码并将代码合并到您的项目中。\n2. 检查Auto-Coder的提交，并在vscode或其他IDE中审查被提交的代码。\n3. 如果提交是基本满意，您可以选择使用github copilot或其他工具手动对代码进行微调，或者直接继续下一步工作。\n4. 如果提交的代码不满足你需求，您需要撤销提交并修改YAML文件，重新执行。\n5. 重复上述步骤，直到完成你的需求。\n\n好了。有了大概之后，我们就可以开始看看细节。\n\n## 如何把已有老项目auto-coder化\n\n如果你有一个老项目，你想把它auto-coder化，你可以使用 \n\n```bash\nauto-coder init --source_dir /path/to/your/project\n```\n\n更多内容参考这篇： [021-AutoCoder初始化项目](./021-AutoCoder初始化项目.md)。\n\n最重要的是会在你的项目里新增一个 actions 目录。 目录里有一个 `000_example.yml`。\n\n我们知道，我们是通过在 yml 文件描述自己的需求或者对代码的修改逻辑，然后auto-coder 会完成对项目源码的修改。这里\n你的第一步不是去修改 000_example.yml，而是从新建一个新的 yml 文件开始：\n\n```bash\nauto-coder next \"我的第一个修改\"\n```\n\n系统会自动在 actions 目录下创建一个新的文件 `001_我的第一个修改.yml`,并且会自动打开这个文件（vscode/idea都支持）。\n\n## 如何和 Yaml 文件交互\n\n001_我的第一个修改.yml 实际自动帮你拷贝了000_example.yml的内容,所以他的内容是这样的：\n\n```yaml\n\ninclude_file:\n  - ./base/base.yml\n  - ./base/enable_index.yml\n  - ./base/enable_wholefile.yml    \n\nquery: |\n  YOUR QUERY HERE\n```\n\n这个yaml文件引入了一些基础配置，诸如 base.yml, enable_index.yml, enable_wholefile.yml。\n\n1. base.yml 是一个基础配置，里面包含了一些基础的配置，比如你的项目的根目录，你的项目的语言，你使用的模型等。\n2. enable_index.yml 开启索引。\n3. enable_wholefile.yml 确定代码的生成和合并模式。\n\n你可能唯一需要修改是 project_type 字段，该字段默认为 py。 如果你的项目是其他语言，你需要修改这个字段,你可以直接使用后缀名。比如你是一个java项目，那么最后的配置看起来是这样的：\n\n```yaml\ninclude_file:\n  - ./base/base.yml\n  - ./base/enable_index.yml\n  - ./base/enable_wholefile.yml    \n\nproject_type: .java\nquery: |\n  YOUR QUERY HERE\n```\n\n除了该参数以外，强烈建议你遵循默认配置。\n\n你唯一要做的就是修改 query 字段。query 字段是一个多行字符串，你可以在这里写你的需求。\n\n如何去描述你想对项目的更改，参考 [038-AutoCoder_为什么你需要经过反复练习才能用好](./038-AutoCoder_为什么你需要经过反复练习才能用好.md) 以及 [036-AutoCoder_编码_prompt实践_1](./036-AutoCoder_编码_prompt实践_1.md)。\n\n## 运行 YAML 并且完成交互\n\n当你完成了你的需求描述，你可以使用下面的命令来运行你的YAML文件：\n\n```bash\nauto-coder --file ./actions/001_我的第一个修改.yml\n```\n\n该命令执行后，会需要你和进行两次交互：\n\n1. auto-coder 会根据你的描述，找到你需要修改以及为了完成这些修改还可能需要的文件，这是一个大绿屏。\n2. 到最后的代码生成环节，auto-coder 会停下来，等待你的输入。\n\n### 第一次交互\n\n我们来看看第一个交互，也就是那个大绿屏：\n\n![](../images/044-01.png)\n\n这大绿屏其实展示了两个部分：\n\n1. 根据你的 query 描述，auto-coder 找到的相关文件。 \n2. 文件名后面的其实还有一部分，就是告诉你为什么这个文件被选上了。\n\n默认都被勾选上，在这个环节你可能需要注意几个问题：\n\n1. 路径必须是绝对路径。如果给出的是相对路径，那么要么是模型抽经，要么就是auto-coder 没有正确的拿到目录信息。你可以到群里咨询这个问题。\n2. 这里罗列文件可能有没有实际存在的文件，但是auto-coder 会最终检查是不是存在，所以你不用担心这个问题。\n3. 为什么这个环节只允许用户去掉一些文件，而无法新增一些文件。减少文件时因为我们需要减少给大模型的输入，避免超出模型的限制。如果你想要auto-coder关注的文件没有在这里，那么你有两种选择，第一个是取消然后回去修改yaml, 手动 @下你的文件名，类名或者函数名。或者你可以通过 urls 参数手动执行必须关注的一些文件。\n\n你用 Tab 切换到 OK ，点击回车就会继续下一步,此时这个大绿框会消失,然后会展示绿框中的列表。\n\n稍等几秒，我们会进入第二次交互\n\n### 第二次交互\n\n![](../images/044-02.png)\n\n这个过程，auto-coder 会停下来，然后告诉你,我已经把我能收集到的信息都放到 output.txt 文件啦。\n你现在可以把这个output.txt 给到大模型。 然后你把这个output.txt 的内容拖拽到任何一个大模型的 web 页面力就可以：\n\n![](../images/044-03.png)\n\n然后他就开始卡卡卡给你生成代码。\n\n这里特别需要注意的一点是，把 output.txt 文件丢给大模型的时候，如果你发现大模型明显不知所谓，那么你可以打开这个output.txt 文件看看，是不是有正常\n的目录结构树以及被我们选上的文件的源码。如果没有，可能\n\n1. 是我们 project_type 配置错误，导致最后无法获取实际的文件\n2. 也可能source_dir 目录问题\n3. 还有可能是在我们的query,我们显示的写了文件路径，但是路径写错了\n\n代码生成完之后，你有两个选择：\n\n1. 让auto-coder 帮你完成代码的合并。\n2. 你自己完成后续的流程，比如代码合并。\n\n这个第二次交互带来的价值有两点，第一点是现在 web 版的大模型可选的非常多，可以包月，或者直接免费。\n第二点是，你可以看看生成结果靠不靠谱，毕竟有界面，看着舒服。\n\n#### auto-coder 帮你合并代码\n\n我们回国头来看你的选择,如果你选择让 auto-coder 帮你合并代码，那么你可以把 web 大模型的回复拷贝下，然后黏贴到 auto-coder 的命令行里。\n然后换一个新行，输入 EOF，然后再次回车,表示我结束啦。\n\n![](../images/044-04.png)\n\n这里特别要注意两点：\n\n1. 你要需要黏贴完整的回复，而不是代码部分。\n2. EOF 要在独立的一行，并且是最后一行。\n\n此时 auto-coder 会做完后续的事情，然后你就可以在 vscode/idea 里看到 commit 记录了。\n\n\n#### 你自己合并代码\n\n这个时候你 control + c 退出 auto-coder，然后根据大模型给出的答案，你自己对已有的代码做修改。\n\n### 如何控制这些交互\n\n1. skip_confirm 参考设置为 true, 可以跳过第一次交互,默认为 false。\n2. human_as_model 设置为 false, 可以跳过第二次交互,默认为 true。参考这里: [human_as_model](./003-%20AutoCoder%20使用Web版大模型，性感的Human%20As%20Model%20模式.md)\n3. auto_merge 可以控制生成的代码样式以及合并方式，默认为 wholefile。参考这里: [auto_merge](./035-AutoCoder_auto_merge详解.md)\n\n## 很重要的一点\n\n如果在公司推广 auto-coder ,推广者可以自己多调试各种参数，但是给到一线业务研发人员用的时候，推荐循序渐进：\n\n1. 一开始引导调整 project_type/auto_merge/query 这三个参数。\n2. 后续等推广者自己熟练了，有最佳实践，再慢慢推广其他参数给到一线业务研发人员。\n\n生成代码效果目前最好的是 GPT4o 或者 Claude Opus。如果企业只有API，建议搭建一个web服务出来（类似搭建了一个私有版本的web订阅版），然后给到一线业务研发人员配合 auto-coder 使用，也就是开启 human_as_model 模式,代码生成还是人工贴到web服务里来生成。\n\n等完全掌握auto-coder后，则可以走API， 你可以通过 code_model 参数，单独配置为代码生成部分配置一个好的模型。\nmodel 则可以配置为 deepseekv2,用来构建索引，查询索引，驱动 auto-coder 运行。这样可会获得最好的成本和效果的平衡。\n\n## 总结\n\n默认的设置对用户使用已经足够友好。用户唯一需要控制的是 auto_merge 参数，在 wholefile,diff,editblock 三者之间选择。\neditblock 适合针对多文件，每个文件修改不大的情况， wholefile 则适合新文件生成。用户目前需要自己判断下这个需求用哪个比较好。\n\n什么，这些参数要配置在哪里？ 放到 `001_我的第一个修改.yml` 里就可以了。后续如果你通过 auto-coder init 新增文件，他会自动拷贝上一个的配置过去。\n\n\n\n\n\n\n\n\n\n\n", "tag": "", "tokens": 2053, "metadata": {}}], "modify_time": 1717567709.5826838, "md5": "b67d5f391feea4e4dc3485b101d2fdc8"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/AutoCoder 快速使用指南.md", "relative_path": "AutoCoder 快速使用指南.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/AutoCoder 快速使用指南.md", "source_code": "# AutoCoder 快速使用指南\n\n昨天发了 命令行版Devin 来了: Auto-Coder 后有不少人私信我讨论。那么能让大家快捷的用起来是第一要务。所以从昨天晚上到今天，我光速给 Byzer-LLM/AutoCoder 发了两个新版本，来支持今天这篇文章。\n\n## 安装\n\n安装部分其实比较简单，安装如下 python 库：\n\n```bash\nconda create --name autocoder python==3.10.11\nconda activate autocoder\npip install -U auto-coder\nray start --head\n```\n\n现在，就可以开始使用 AutoCoder了。\n\n## 基于Web版本模型\n\n比如你手头有 Claude3, ChatGPT, Kimi等产品的Web端的订阅或者免费使用权限，而没有这些模型的 API 订阅， 那么这个时候 AutoCoder 等价于一个 Code Pack 工具，帮你把代码和问题一起打包成一个文本文件，方便你拖拽到这些产品的界面里，然后帮你进行代码生成。\n\n可以用 auto-coder 查看一些常见命令选项。\n\n```bash\nauto-coder -h\n```\n\n我来说一个实际案例，我想给 byzer-llm 项目增加一个命令行支持。下面是我写的yaml配置文件：\n\n```yml\nsource_dir: /home/winubuntu/projects/byzer-llm/saas\ntarget_file: /home/winubuntu/projects/byzer-llm/output.txt\n\nurls: https://raw.githubusercontent.com/allwefantasy/byzer-llm/master/README.md\n\nsearch_engine: bing\nsearch_engine_token: ENV {{BING_SEARCH_TOKEN}}\n\nquery: |\n  在 src/byzerllm 目录下新增一个 byzerllm.py 文件。在该文件中使用args 实现命令行支持。参考 README.md 中的使用方法来增加命令行参数。\n  主要支持：\n  1. 部署模型相关参数\n  2. 运行推理相关阐述\n\n  比如部署模型，一般代码是这样的：\n  \n  ```python\n  ray.init(address=\"auto\",namespace=\"default\",ignore_reinit_error=True)\n  llm = ByzerLLM()\n\n  llm.setup_gpus_per_worker(4).setup_num_workers(1)\n  llm.setup_infer_backend(InferBackend.transformers)\n\n  llm.deploy(model_path=\"/home/byzerllm/models/openbuddy-llama2-13b64k-v15\",\n            pretrained_model_type=\"custom/llama2\",\n            udf_name=\"llama2_chat\",infer_params={})\n  ```\n  此时你需要有 address, num_workers, gpus_per_worker, model_path, pretrained_model_type, udf_name, infer_params 这些参数可以通过命令行传递。\n\n  最终形态是：\n\n  byzerllm deploy --model_path /home/byzerllm/models/openbuddy-llama2-13b64k-v15 --pretrained_model_type custom/llama2 --udf_name llama2_chat --infer_params {}\n\n  同理推理是也是。比如一般推理代码是：\n\n  ```python\n  llm_client = ByzerLLM()\n  llm_client.setup_template(\"llama2_chat\",\"auto\")\n\n  v = llm.chat_oai(model=\"llama2_chat\",conversations=[{\n      \"role\":\"user\",\n      \"content\":\"hello\",\n  }])\n\n  print(v[0].output)\n  ```\n  此时你需要有 model, conversations 这些参数可以通过命令行传递。\n\n  此时你的命令行形态是：\n  \n  byzerllm query --model llama2_chat --query \"hello\" --template \"auto\"\n```\n\nurls 指定了大模型需要参考的文档，source_dir 自定了大模型需要阅读的代码，而 target_file 则指定了生成的prompt的位置。query则是我具体要大模型帮我做的事情。现在执行这个配置文件\n\n```bash\nauto-coder --file test.yml\n```\n\n然后将 output.txt 拖拽到大模型web界面，点击执行，大模型就开始干活了。\n\n![](../images/image1.png)\n\n可以看到，他做的很细致，会告诉你新的文件路径是什么，以及对应的代码。你只需要拷贝黏贴到你的项目里即可。\n\n## 基于大模型 API\n\n我们推荐你申请 Qwen https://dashscope.console.aliyun.com/model 免费token量大，效果也还不错。你申请了 Token之后，使用如下命令在你本机部署它：\n\n```bash\n\nbyzerllm deploy  --pretrained_model_type saas/qianwen \\\n--infer_params saas.api_key=xxxxxxx saas.model=qwen-max \\\n--model qianwen_chat\n```\n\n运行完成后，你相当于有个叫做 qianwen_chat 的模型实例了。可以通过下面命令来验证是否部署成功：\n\n```bash\n\nbyzerllm query --model qianwen_chat --query \"你好\"\n```\n\n如果能正常输出，就表示成功。如果失败，你需要先卸载，再重新部署。卸载的方式：\n\n```bash\nbyzerllm undeploy --model qianwen_chat\n```\n\n准备好了模型你可以做两件事：\n\n1. 让大模型直接执行，然后把结果写到 target_file 里。\n2. 解锁一些新功能，比如索引等，urls 内容整理和抽取等\n\n我们一个一个来看。\n\n首先第一个例子是，我希望使用刚才部署的模型实例 qianwen_chat 帮优化一个程序问题。但是因为这个项目非常大，而 qianwen_chat的最大输入是 6000个字符，所以我不能把项目所有文件都给到大模型，需要智能减少大模型的输入。下面是一个比较合理的配置：\n\n```yml\n\nproject_type: py\nsource_dir: /home/winubuntu/projects/byzer-llm\ntarget_file: /home/winubuntu/projects/byzer-llm/output.txt\n\nmodel: qianwen_chat\nmodel_max_length: 2000\nmodel_max_input_length: 6000\nanti_quota_limit: 5\n\nskip_build_index: false\n\nsearch_engine: bing\nsearch_engine_token: ENV {{BING_SEARCH_TOKEN}}\n\nquery: |\n  优化byzerllm.py 中StoreNestedDict，使其能解析标准的 KEY=\"VALUE\" 或者 KEY=VALUE 的字符串\n```\n\n在这里，我们设置了我们要用的模型，以及最大的输出和输入。此外，我们还通过 skip_build_index 开启了索引功能。\n\n这样，当我们第一次运行这个文件的时候，他会对你的项目文件构建索引，然后过滤出和你当前问题相关的代码进行prompt的生成。\n\n```bash\n\nauto-coder --file optimize_command_line.yml\n```\n\n在这个命令里，我们仅仅是利用 qwen_chat 模型生成合适大小的prompt（构建索引，过滤合适的代码，如果配置了urls,会对urls内容进行格式化抽取等等）。如果你希望 qwen_chat 也能直接生成代码，可以加一个参数：\n\n```bash\n\nauto-coder --file optimize_command_line.yml --execute\n```\n\n这个时候 target_file 里的内容就是已经给你生成好的代码而不是prompt了。\n\n所以可以看到，我们可以通过我们配置的模型来让 auto-coder更加只能得生成prompt，然后真正写代码的，还是让 web 版本的模型。我们当然也可以直接让你配置的模型直接完成代码书写，这个可以通过 --execute 参数控制。\n\n## 让大模型同时阅读你的代码，第三方包的代码，以及API文档，然后回答你的问题和编写代码\n\n```yml\nsource_dir: /home/winubuntu/projects/byzer-llm/src/byzerllm/saas\ntarget_file: /home/winubuntu/projects/byzer-llm/output.txt\npy_packages: openai\nurls: https://raw.githubusercontent.com/allwefantasy/byzer-llm/master/README.md\n\nquery: |\n  参考 src/byzerllm/saas/qianwen 中的实现，重新实现 offical_openai。注意 offical_openai 中\n  使用的是openai 这个模块，你需要学习这个模块的使用方法，保证正确的使用其功能。\n```\n\n这里，你的源码是通过 source_dir 配置的，你的第三方包是通过 py_packages 配置的，你的文档是通过 urls 配置的。最后你让大模型基于这些信息，回答你的问题(query)。模型能力强的，生成效果很惊艳。如果项目太大，你可以像前面一样配置一个模型，实现只能过滤代码：\n\n```yml\nsource_dir: /home/winubuntu/projects/byzer-llm/src/byzerllm/saas\ntarget_file: /home/winubuntu/projects/byzer-llm/output.txt\npy_packages: openai\nurls: https://raw.githubusercontent.com/allwefantasy/byzer-llm/master/README.md\n\nmodel: qianwen_chat\nmodel_max_length: 2000\nmodel_max_input_length: 6000\nanti_quota_limit: 5\n\nskip_build_index: false\n\nquery: |\n  参考 src/byzerllm/saas/qianwen 中的实现，重新实现 offical_openai。注意 offical_openai 中\n  使用的是openai 这个模块，你需要学习这个模块的使用方法，保证正确的使用其功能。\n```\n\n其实，我们还支持对搜索引擎的集成，可以让大模型为了完成你的目标，具备下面的能力：\n\n1. 阅读你的项目源码\n\n2. 阅读第三方库\n\n3. 阅读你提供的文档链接\n\n4. 找搜索要更多的一些参考文档\n\n这个我们后续会单独一篇来介绍。\n\n## 结束语\n\n还等什么，赶快动手吧。遇到任何问题可以在github留言。", "tag": "", "tokens": 2039, "metadata": {}}], "modify_time": 1711809217.9072099, "md5": "b5e53a214eedbcd16df5c7257ac09220"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/016-AutoCoder 如何从图片生成ReactJS+TypeScript+TailWind 代码.md", "relative_path": "016-AutoCoder 如何从图片生成ReactJS+TypeScript+TailWind 代码.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/016-AutoCoder 如何从图片生成ReactJS+TypeScript+TailWind 代码.md", "source_code": "# 016-AutoCoder 如何从图片生成ReactJS+TypeScript+TailWind 代码\n\n> AutoCoder >= 0.1.58 特性\n\n需要额外安装：\n\n```shell\npip install playwright\nplaywright install\n```\n\n这个功能在什么场景下有用呢，比如你有个 reactjs + typescript + tailwind 的项目，然后你有个页面想参考某个其他的网站，这个时候你就可以截个图，然后根据该图生成对应的 reactjs + typescript + tailwind 代码了。\n\n默认情况，AutoCoder 会把图片转成 html/tailwind 格式，然后再通过 query 使用 model 将其转换成你最终需要的形态。\n所以你可以只生成 html/tailwind 代码 或者是生成 reactjs + typescript + tailwind 代码。\n\n## 只生成 HTML + Tailwind\n\n我们看下面的例子：\n\n```yml\nsource_dir: /Users/allwefantasy/projects/tt/\ntarget_file: /Users/allwefantasy/projects/auto-coder/output.txt \nproject_type: copilot/ts\n\nimage_file: /Users/allwefantasy/projects/auto-coder/screenshots/www_elmo_chat.png\nimage_mode: direct\nimage_max_iter: 1\n\nmodel: deepseek_chat\nmodel_max_length: 2000\nmodel_max_input_length: 30000\nanti_quota_limit: 5\n\nvl_model: gpt4o_chat\n\nskip_build_index: false\nexecute: true\nauto_merge: true\nhuman_as_model: true\n\nquery: |   \n   将html转换成 reactjs+typescript+tailwind 实现 ,并且保存在 /Users/allwefantasy/projects/tt/ 合适的目录下。\ntt 已经是 reactjs + typescript + tailwind 项目\n```\n\n/Users/allwefantasy/projects/tt/ 是一个已经存在的 reactjs + typescript + tailwind 项目。我们希望新的代码生成到这个项目合适的目录。\n\n这里的配置文件相比以前，几个区别：\n\n1. 指定网页图片地址，这里通过 image_file 指定。\n2. 项目配置类型为： copilot/ts 而不是 ts\n3. 额外指定一个多模态模型，可以通过 vl_model 指定,推荐 gpt_4o\n4. 多模态模型需要通过 byzerllm 进行部署。\n5. image_max_iter 可以设置迭代次数，迭代次数越多，理论上生成的代码越接近原图，你也可以查看中间生成的 html 文件，看看是否满足你的需求。\n\n## 生成 reactjs + typescript + tailwind\n\n此时只需要改一行配置即可：\n\n```yml\nproject_type: ts\n```\n\n我们把project_type 从 copilot/ts 改成 ts,这样中间会让我们勾选相关的一些文件，并且进行代码转换。\n\n如果你希望转成诸如vue等，你只需要在 query中说明即可。\n\n## 结果\n\n\n最后生成的 ReactJS 代码则是这样的：\n\n```jsx\nimport React from 'react';\nimport tw from 'tailwind-styled-components';\n\nconst Container = tw.div`\n  bg-gradient-to-b from-orange to-yellow\n  text-white\n  font-Arial\n  flex\n  flex-col\n  items-center\n  pt-50\n  min-h-screen\n`;\n\nconst Title = tw.h1`\n  text-3xl\n  mb-20\n`;\n\nconst Subtitle = tw.h2`\n  text-2xl\n  mb-30\n`;\n\nconst InstallButton = tw.button`\n  bg-green\n  text-white\n  border-none\n  px-10 py-2\n  font-semibold\n  cursor-pointer\n  rounded-lg\n  mb-20\n`;\n\nconst GoldRibbon = tw.div`\n  bg-gold\n  text-black\n  font-medium\n  px-5 py-2\n  relative\n  mb-10\n`;\n\nconst RibbonBeforeAfter = tw.div`\n  absolute\n  w-full\n  h-2\n  bg-black\n  top-1/2\n  transform -translate-y-1/2\n`;\n\nconst RibbonBefore = tw(RibbonBeforeAfter)`\n  left[-10px]\n  right[100%]\n`;\n\nconst RibbonAfter = tw(RibbonBeforeAfter)`\n  left[100%]\n  right[-10px]\n`;\n\nconst FooterText = tw.p`\n  text-base\n  mb-20\n`;\n\nconst ElmoChat: React.FC = () => (\n  <Container>\n    <Title>\n      Elmo{' '}\n      <span role=\"img\" aria-label=\"Electric Spark\">\n        ⚡\n      </span>\n    </Title>\n    <Subtitle>\n      Elmo is your AI chrome extension to create summaries, insights and extended knowledge.\n    </Subtitle>\n    <InstallButton onClick={() => window.open('https://chrome.google.com/webstore/detail/elmo/your-extension-id')}>\n      Install from Chrome Web Store\n    </InstallButton>\n    <p>Free • No Account Needed • Supports Multiple Languages</p>\n\n    <GoldRibbon>Featured on Chrome Web Store</GoldRibbon>\n    <GoldRibbon>Developed by Established Publisher</GoldRibbon>\n\n    <FooterText>Summaries and Bullet Points</FooterText>\n  </Container>\n);\n\nexport default ElmoChat;\n```\n\n然后继续使用 AutoCoder 修改和迭代该页面即可。", "tag": "", "tokens": 1139, "metadata": {}}], "modify_time": 1717303519.5391395, "md5": "c6c05c41a344e69c0044a3acb5731460"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/048_AutoCoder如何辅助项目代码阅读.md", "relative_path": "048_AutoCoder如何辅助项目代码阅读.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/048_AutoCoder如何辅助项目代码阅读.md", "source_code": "\n# 048_AutoCoder如何辅助项目代码阅读\n\n## 背景\n\nAutoCoder 提供了一系列工具和功能来辅助开发者进行项目代码阅读，本文将介绍其中的两个主要功能：/ask 和 agent/project_reader。\n\n## chat-auto-coder 中的/ask 命令\n\nchat-auto-coder 是个命令行交互工具，其中的\n\n`/ask` 命令允许用户直接向 AI 提问，获取与当前项目相关的信息。\n\n### 基本用法\n\n以下是一个使用 `/ask` 命令的示例：\n\n```bash\n/ask 这个项目是用来做啥的？\n```\n\n为了能够使用该功能,需要先构建索引：\n\n```bash\n/index/build\n```\n\n特别注意：\n\n索引构建并不会对整个项目所有文件进行索引，而是根据 project_type 来选择性的构建索引，chat-auto-coder会读取 ./actions/base/base.yml 文件中的 project_type 来决定构建索引的文件类型\n\n你可以手动在当前会话里进行更改，\n\n前端项目你可以通过如下方式设置：\n\n```bash\n/conf project_type:ts\n```\n\n其他项目可以通过后缀设置,比如你可以同时设置索引.java,.scala后缀名的文件：\n\n```bash\n/conf project_type:.java,.scala\n```\n\n`/ask` 命令会自动解析问题并返回相应的答案。\n\n### 更多用法\n\n还有一些常见的问题：\n\n1. 罗列 xxxx 中所有的类函数\n2. IndexManager 类都被哪些文件使用了？\n3. 帮我阅读下planner.py 为啥里面需要用 IndexManager?\n4. 该项目是如何完成自动代码合并的\n\n还有比如，做统计\n\n```bash\n/ask 该项目有多少行python代码\n```\n\n输出:\n\n```\n=============RESPONSE==================\n\n\n该项目有61228行Python代码。\n```\n\n自动做一些推导：\n\n```\n/ask 这个项目需要编译么?\n```\n\n输出：\n\n```\n=============RESPONSE==================\n\n\nThe project does not require traditional compilation. Instead, it uses the setup.py file for packaging the project into a distributable format that can be installed using pip.\n```\n或者编译：\n\n```\n/ask 帮我编译下项目\n```\n\n## chat-auto-coder 中的/chat 命令\n\n### 基本用法\n\n此外，你也可以使用 `/chat` 来对通过 `/add_files` 添加的活动文件来进行提问。`/chat` 是不需要\n索引支持的。\n\n```bash\n/add_files chat_auto_coder.py\n/chat 这个文件里都有哪些指令？\n```\n\n这里，我们添加了一个文件 `chat_auto_coder.py`，然后使用 `/chat` 对这个问题进行提问。\n\n此外，如果你想单纯的问大模型一些通用知识，可以直接使用 `/chat` 命令。\n\n```bash\n/chat python 如何移除指定前缀\n```\n你可以清理掉当前的活动文件，从而使得响应更快：\n\n```bash\n/remove_files /all\n``` \n\n### 基于知识库的用法\n\n最后 `/chat` 是支持知识库的，如果你已经启动了知识库，那么可以通过如下方式开启你对话需要用的知识库名称：\n\n```bash\n/conf collection:<collection_name>\n```\n\n此时 /chat 会结合知识库以及当前的活动文件来和你进行对话。\n\n\n\n## auto-coder 中的 agent/project_reader\n\n`agent/project_reader` 是另一个强大的工具，chat-auto-coder中的 `/ask` 也是基于该工具做的二次封装，\n用于分析和提取项目中的关键信息。\n\n### 示例\n\n使用 `agent/project_reader` 可以生成一个包含项目关键文件和函数的详细报告：\n\n```bash\nauto-coder index --file ./actions/base/base.yml\nauto-coder agent project_reader --file ./actions/base/base.yml --query \"帮我找到 exclude_dirs函数，并且给出完整代码\"\n```\n\n## 一些比较好的实践\n\n比如你可能好奇， chat-auto-coder 项目是如何实现代码自动合并的，此时你可以进入 auto-coder项目，然后直接问题：\n\n```bash\n/ask 该项目是如何完成自动代码合并的？\n```\n\n输出：\n\n```\n=============RESPONSE==================\n\n\n该项目通过`CodeAutoMerge`和`CodeAutoMergeDiff`两个类来完成自动代码合并。`CodeAutoMerge`类主要通过`merge_code`方法实现代码合并，该方法首先读取文件内容，计算MD5值，然后尝试将代码解析并写入到指定路径的文件中。如果文件不存在，则创建新文件。合并完成后，如果未强制跳过Git操作，则提交更改到Git仓库。\n\n`CodeAutoMergeDiff`类则通过`merge_code`方法实现更复杂的代码合并，它首先获取代码差异，然后应用这些差异到文件中。如果差异应用成功，则更新文件内容；如果失败，则抛出异常。同样，如果未强制跳过Git操作，合并完成后也会提交更改到Git仓库。\n\n这两个类都依赖于`byzerllm`和`autocoder`库中的其他工具和方法，如`git_utils`用于Git操作，`code_utils`用于代码处理等。通过这些工具和方法的组合，实现了自动化的代码合并功能。\n```\n\n\n另外，我们也可以更加手动挡一些，循序渐进的提问，比如第一次我先找到一些可能和我问题相关的文件：\n\n```bash\n/index/query 自动代码合并相关的文件\n```\n\n这个是找到的内容：\n```\nindex_filter_level:1, total files: 6 filter files by query: 自动代码合并相关的文件\n\n+-----------------------------------------------------------+--------------------------------------------------------------+\n| file_path                                                 | reason                                                       |\n+===========================================================+==============================================================+\n| /Users/allwefantasy/projects/auto-                        | This file contains functions related to automatic code       |\n| coder/src/autocoder/common/code_auto_merge.py             | merging, including parsing text and merging code.            |\n+-----------------------------------------------------------+--------------------------------------------------------------+\n| /Users/allwefantasy/projects/auto-                        | This file focuses on merging code edit blocks, facilitating  |\n| coder/src/autocoder/common/code_auto_merge_editblock.py   | efficient code integration and modification.                 |\n+-----------------------------------------------------------+--------------------------------------------------------------+\n| /Users/allwefantasy/projects/auto-                        | This file deals with strict diff handling in automatic code  |\n| coder/src/autocoder/common/code_auto_merge_strict_diff.py | merging, ensuring precise code integration.                  |\n+-----------------------------------------------------------+--------------------------------------------------------------+\n| /Users/allwefantasy/projects/auto-                        | This file handles automatic code merging with diff           |\n| coder/src/autocoder/common/code_auto_merge_diff.py        | processing, including applying hunks and merging code.       |\n+-----------------------------------------------------------+--------------------------------------------------------------+\n| /Users/allwefantasy/projects/auto-                        | Contains functions related to building and querying indexes, |\n| coder/src/autocoder/index/index.py                        | which might be involved in code merging processes.           |\n+-----------------------------------------------------------+--------------------------------------------------------------+\n| /Users/allwefantasy/projects/auto-                        | Includes functions for managing files and directories, which |\n| coder/src/autocoder/chat_auto_coder.py                    | could be relevant for code merging.                          |\n+-----------------------------------------------------------+--------------------------------------------------------------+\n```\n\n这样我可以人工选一些文件（有的时候系统自动找的文件太多，会导致最后问询失败），然后在 /ask 里让他主动阅读者几个文件的源码来回答我的问题：\n\n```bash\n/ask 直接阅读下下面几个文件：code_auto_merge.py,code_auto_merge_editblock.py,code_auto_merge_strict_diff.py,code_auto_merge_diff.py，然后告诉我这些文件是如何实现自动代码合并的？\n```\n\n这个时候的输出：\n\n```\n=============RESPONSE==================\n\n\n这些文件通过解析文本，提取代码块，并使用不同的方法将这些代码块合并到项目中。`code_auto_merge.py` 使用 `parse_whole_text_v2` 方法解析文本，`code_auto_merge_editblock.py` 支持编辑块的合并，而 `code_auto_merge_strict_diff.py` 和 `code_auto_merge_diff.py` 则通过解析 diff 格式的文本来处理代码差异。这些文件都依赖于 `byzerllm` 和 `autocoder.common` 中的工具和方法来实现自动代码合并。\n```\n\n接着你可以继续这么问：\n\n```bash\n/ask `code_auto_merge.py` 使用 `parse_whole_text_v2` 方法解析文本,该方法的具体实现是怎么样的?\n```\n\n这次的输出:\n\n```\n=============RESPONSE==================\n\n\n`parse_whole_text_v2` 方法的实现主要分为以下几个步骤：\n\n1. 首先，将输入的文本按行分割，存储在列表 `lines` 中。\n2. 定义了几个辅助函数来识别文本中的标记：\n   - `start_marker` 函数用于检测是否为代码块的开始标记，即行首是否为 '```'，并且下一行是否以 '##File:' 开始。\n   - `inline_start_marker` 函数用于检测内联代码块的开始标记，即行首是否为 '```'，并且该行不等于 '```'。\n   - `end_marker` 函数用于检测代码块的结束标记，即行首是否为 '```'，并且该行等于 '```'。\n3. 遍历每一行文本，根据不同的标记进行处理：\n   - 如果检测到代码块的开始标记，增加 `start_marker_count`。\n   - 如果检测到内联代码块的开始标记，增加 `inline_start_marker_count`，并将该行添加到 `block` 列表中。\n   - 如果检测到代码块的结束标记，并且 `start_marker_count` 为 1，`inline_start_marker_count` 为 0，则提取文件路径和内容，并将其添加到 `path_and_code_list` 列表中。\n   - 如果检测到内联代码块的结束标记，减少 `inline_start_marker_count`。\n   - 如果 `start_marker_count` 大于 0，将当前行添加到 `block` 列表中。\n4. 最后，返回包含所有解析出的文件路径和内容的列表 `path_and_code_list`。\n\n这个方法主要用于从文本中提取出格式化的代码块，包括文件路径和代码内容，以便后续处理。\n```\n\n## 暂时无法解决的一些问题\n\n1. 可以指定文件检测语法错误，比如说： 看看 planner.py 中是否有语法错误， 但无法做到：项目中有哪些代码文件有语法错误（需要遍历整个项目文件，代价太大）。\n2. 无法任意查找一个文件，只能查找已经构建索引的文件。所以构建取决于 project_type 和是否执行了 /index/build 或者  auto-coder index 命令。\n3. 无法查找一个函数的调用者，只能查找一个函数的定义。\n\n## 问题\n\n因为采用了 Cot 来处理，有的时候会超出deepseek chat的最大窗口，会失败，可以换个问法。一般如果超出窗口长度，报错如下：\n\n```\n2024-06-23 22:16:09,104 ERROR serialization.py:414 -- Failed to unpickle serialized exception\nTraceback (most recent call last):\n  File \"/opt/miniconda3/envs/byzerllm/lib/python3.10/site-packages/ray/exceptions.py\", line 49, in from_ray_exception\n    return pickle.loads(ray_exception.serialized_exception)\nTypeError: APIStatusError.__init__() missing 2 required keyword-only arguments: 'response' and 'body'\n```\n\n\n## 结论\n\n通过使用 `/ask` 或者 `agent/project_reader`，开发者可以更高效地理解和分析项目代码，提高开发和维护效率。\n", "tag": "", "tokens": 2461, "metadata": {}}], "modify_time": 1719224063.2185557, "md5": "456350ba8c1feeccea1ccef0302defb2"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/023-AutoCoder中模型部署经验谈.md", "relative_path": "023-AutoCoder中模型部署经验谈.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/023-AutoCoder中模型部署经验谈.md", "source_code": "# 023-AutoCoder中模型部署经验谈\n\n我们知道，为了对接各种模型，我们提供了 `byzerllm` 部署工具。\n\n一个典型的SaaS部署脚本如下：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=qwen-max \\\n--model qianwen_chat\n```\n\n一个典型的私有模型部署如下：\n\n```bash\nbyzerllm deploy --pretrained_model_type custom/auto \\\n--infer_backend vllm \\\n--model_path /home/winubuntu/models/openbuddy-zephyr-7b-v14.1 \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 1 \\\n--num_workers 1 \\\n--infer_params backend.max_model_len=28000 \\\n--model zephyr_7b_chat\n```\n\n现在我们来仔细看看上面的参数。\n\n## 0. `--model`\n\n给当前部署的实例起一个名字，这个名字是唯一的，用于区分不同的模型。你可以理解为模型是一个模板，启动后的一个模型就是一个实例。\n比如同样一个 SaaS模型，我可以启动多个实例。每个实例里可以包含多个worker实例。\n\n## 1. `--pretrained_model_type`\n\n定义规则如下：\n\n1. 如果是SaaS模型，这个参数是 `saas/xxxxx`。 如果你的 SaaS 模型（或者公司已经通过别的工具部署的模型），并且兼容 openai 协议，那么你可以使用 `saas/openai`，否则其他的就要根据官方文档的罗列来写。 参考这里： https://github.com/allwefantasy/byzer-llm?tab=readme-ov-file#SaaS-Models\n\n    下面是一个兼容 openai 协议的例子,比如 moonshot 的模型：\n\n    ```bash\n    byzerllm deploy --pretrained_model_type saas/official_openai \\\n    --cpus_per_worker 0.001 \\\n    --gpus_per_worker 0 \\\n    --num_workers 2 \\\n    --infer_params saas.api_key=${MODEL_KIMI_TOKEN} saas.base_url=\"https://api.moonshot.cn/v1\" saas.model=moonshot-v1-32k \\\n    --model kimi_chat\n    ```\n\n    还有比如如果你使用 ollama 部署的模型，就可以这样部署：\n\n    ```bash\n    byzerllm deploy  --pretrained_model_type saas/openai \\\n    --cpus_per_worker 0.01 \\\n    --gpus_per_worker 0 \\\n    --num_workers 2 \\\n    --infer_params saas.api_key=token saas.model=llama3:70b-instruct-q4_0  saas.base_url=\"http://192.168.3.106:11434/v1/\" \\\n    --model ollama_llama3_chat\n    ```\n \n2. 如果是私有模型，这个参数是是由 `--infer_backend` 参数来决定的。 如果你的模型可以使用 vllm/llama_cpp 部署，那么 `--pretrained_model_type` 是一个固定值 `custom/auto`。 如果你是用 transformers 部署，那么这个参数是 transformers 的模型名称, 具体名称目前也可以参考 https://github.com/allwefantasy/byzer-llm。 通常只有多模态，向量模型才需要使用 transformers 部署，我们大部分都有例子，如果没有的，那么也可以设置为 custom/auto 进行尝试。\n\n\n## 2. `--infer_backend`\n\n目前支持 vllm/transformers/deepspeed/llama_cpp 四个值。 其中 deepspeed 因为效果不好，基本不用。推荐vllm/llama_cpp 两个。\n\n## 3. `--infer_params`\n\n对于 SaaS 模型，所有的参数都以 `saas.` 开头，基本兼容 OpenAI 参数。 例如 `saas.api_key`, `saas.model`,`saas.base_url` 等等。\n对于所有私有模型，如果使用 vllm 部署，则都以 `backend.` 开头。 具体的参数则需要参考 vllm 的文档。 对于llama_cpp 部署，则直接配置 llama_cpp相关的参数即可，具体的参数则需要参考 llama_cpp 的文档。\n\nvllm 常见参数：\n\n1. backend.gpu_memory_utilization GPU显存占用比例 默认0.9\n2. backend.max_model_len 模型最大长度 会根据模型自动调整。 但是如果你的显存不够模型默认值，需要自己调整。\n3. backend.enforce_eager 是否开启eager模式(cuda graph, 会额外占用一些显存来提数) 默认True\n4. backend.trust_remote_code 有的时候加载某些模型需要开启这个参数。 默认False\n\nllama_cpp 常见参数：\n\n1. n_gpu_layers 用于控制模型GPU加载模型的层数。默认为 0,表示不使用GPU。尽可能使用GPU，则设置为 -1, 否则设置一个合理的值。（你可以比如从100这个值开始试）\n2. verbose 是否开启详细日志。默认为True。\n\n## 4. `--model_path`\n\n`--model_path` 是私有模型独有的参数， 通常是一个目录，里面包含了模型的权重文件，配置文件等等。\n\n## 5. `--num_workers`\n\n`--num_workers` 是指定部署实例的数量。 以backend  vllm 为例，默认一个worker就是一个vllm实例，支持并发推理，所以通常可以设置为1。 如果是SaaS模型，则一个 worker 只支持一个并发，你可以根据你的需求设置合理数目的 worker 数量。\n\nbyzerllm 默认使用 LRU 策略来进行worker请求的分配。\n\n你可以通过 `byzerllm stat` 来查看当前部署的模型的状态。\n\n比如：\n\n```bash\nbyzerllm stat --model gpt3_5_chat\n```\n\n输出：\n```\nCommand Line Arguments:\n--------------------------------------------------\ncommand             : stat\nray_address         : auto\nmodel               : gpt3_5_chat\nfile                : None\n--------------------------------------------------\n2024-05-06 14:48:17,206\tINFO worker.py:1564 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379...\n2024-05-06 14:48:17,222\tINFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at 127.0.0.1:8265\n{\n    \"total_workers\": 3,\n    \"busy_workers\": 0,\n    \"idle_workers\": 3,\n    \"load_balance_strategy\": \"lru\",\n    \"total_requests\": [\n        33,\n        33,\n        32\n    ],\n    \"state\": [\n        1,\n        1,\n        1\n    ],\n    \"worker_max_concurrency\": 1,\n    \"workers_last_work_time\": [\n        \"631.7133535240428s\",\n        \"631.7022202090011s\",\n        \"637.2349605050404s\"\n    ]\n}\n```\n解释下上面的输出：\n\n1. total_workers: 模型gpt3_5_chat的实际部署的worker实例数量\n2. busy_workers: 正在忙碌的worker实例数量\n3. idle_workers: 当前空闲的worker实例数量\n4. load_balance_strategy: 目前实例之间的负载均衡策略\n5. total_requests: 每个worker实例的累计的请求数量\n6. worker_max_concurrency: 每个worker实例的最大并发数\n7. state: 每个worker实例当前空闲的并发数（正在运行的并发=worker_max_concurrency-当前state的值）\n8. workers_last_work_time: 每个worker实例最后一次被调用的截止到现在的时间\n\n\n## 6. `--cpus_per_worker`\n\n`--cpus_per_worker` 是指定每个部署实例的CPU核数。 如果是SaaS模型通常是一个很小的值，比如0.001。\n\n\n## 7. `--gpus_per_worker`\n\n`--gpus_per_worker` 是指定每个部署实例的GPU核数。 如果是SaaS模型通常是0。\n\n", "tag": "", "tokens": 1805, "metadata": {}}], "modify_time": 1714979585.7411265, "md5": "837f241448a923695ed4f59af2d7a699"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/033-AutoCoder_最好用将网页转换为图片的工具.md", "relative_path": "033-AutoCoder_最好用将网页转换为图片的工具.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/033-AutoCoder_最好用将网页转换为图片的工具.md", "source_code": "# 033-AutoCoder_最好用将网页转换为图片的工具.md\n\n我发现 Chrome 插件能真正的把完整网页转换成图片的工具没几个好用的，基本都做不到完整转换。因为 Auto-Coder 可以实现把图片转成任意\n前端技术栈，那干脆我们提供一个前置的小工具，帮大家完成网页转成图片。\n\n使用：\n\n```bash\nauto-coder screenshot --urls https://www.baidu.com  --output /tmp/jack3/\n```\n\n下面是一个超长网页图：\n\n![](../images/033-01.png)\n\n\n\n\n\n\n\n\n\n\n\n", "tag": "", "tokens": 126, "metadata": {}}], "modify_time": 1716465299.2314522, "md5": "0b6bdf77b9dbaf53ccbab6ad0f711554"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/006-AutoCoder 开启索引，减少上下文.md", "relative_path": "006-AutoCoder 开启索引，减少上下文.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/006-AutoCoder 开启索引，减少上下文.md", "source_code": "# 006-AutoCoder 开启索引，减少上下文\n\n到今天为止，我们发现，AutoCoder 实际上会收集以下数据：\n\n1. 通过 source_dir 指定的源码目录\n2. 通过 urls 指定的文档\n3. 通过search_engine 指定的搜索引擎检索结果\n4. 你的需求描述\n5. 第三方包（目前仅支持python）\n\n\n实际上当你在一个积累了很多年的项目上，你会发现项目代码有几十万行，尤其是 Java 代码，这导致大部门模型的上下文窗口无法满足需求。\n\n如果直接把所有源码都带上，确实也有点太浪费了，正确的做法应该是：\n\n1. 根据用户的需求描述，自动筛选相关的源码文件。\n\n2. 从筛选出来的源码文件，再筛选一级他们依赖的文件。\n\n一般情况，经过这样的筛选，应该也就几个或者十几个文件，也能满足大部分代码生成的需求。\n\n但是如何筛选这些文件呢？必须要构建索引。现在让我们看看如何在 AutoCoder 中构建索引。\n\n```yml\n\nsource_dir: /tmp/t-py\ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nmodel: deepseek_chat\nmodel_max_length: 2000\nmodel_max_input_length: 30000\nanti_quota_limit: 0\n\nproject_type: py\n\nskip_build_index: false\n\nquery: >\n  修改 server.py ，在代码 app = FastAPI()后\n  增加 ray 的初始化连接代码。\n```\n\n为了能够开启索引功能，需要保证如下两个参数开启：\n\n1. skip_build_index 设置为 false\n2. model 参数必须设置\n\n现在，让我们执行下上面的query:\n\n```shell\n\nauto-coder --file ./examples/from-zero-to-hero/006_index_cache.yml\n```\n\n此时，会在终端出现如下信息：\n\n```\ntry to build index for /tmp/t-py/server/server.py md5: ad3f4e16f2a2804f973bdd67868eac5d\nparse and update index for /tmp/t-py/server/server.py md5: ad3f4e16f2a2804f973bdd67868eac5d\nTarget Files: [TargetFile(file_path='/tmp/t-py/server/server.py', reason=\"该文件包含了初始化 FastAPI 实例，并且用户要求在 'app = FastAPI()' 之后增加 ray 的初始化连接代码\")]\nRelated Files: []\n```\n\n可以看到，因为我们是python项目，所以系统会收集 .py 结尾的文件，然后对每一个文件构建索引。\n\n打开 /tmp/t-py 目录：\n\n```\n\n(byzerllm-dev) (base) winubuntu@winubuntu:~/projects/ByzerRawCopilot$ ll /tmp/t-py\ntotal 164\ndrwxrwxr-x   4 winubuntu winubuntu   4096  3月 22 19:37 ./\ndrwxrwxrwt 251 root      root      151552  3月 22 19:09 ../\ndrwxrwxr-x   2 winubuntu winubuntu   4096  3月 22 19:38 .auto-coder/\ndrwxrwxr-x   2 winubuntu winubuntu   4096  3月 21 19:50 server/\n```\n\n可以看到有个 .auto-coder 目录，里面就是有我们的索引文件。\n\n接着你应该看到，根据用户的query,我们找到了目标文件TargetFile(file_path='/tmp/t-py/server/server.py'，并且给出了为什么是这个文件的原因：\n\n```\n\n该文件包含了初始化 FastAPI 实例，\n并且用户要求在 'app = FastAPI()' \n之后增加 ray 的初始化连接代码\n```\n\n接着他会找这个文件依赖的文件，因为我们这个项目只有一个文件，所以找不到其他的文件了。\n\n这样，我们就能大大缩小最后给到大模型去生成代码的上下文了。\n\n", "tag": "", "tokens": 806, "metadata": {}}], "modify_time": 1716937436.241538, "md5": "37f73c9a6b4a82aab0730ce167badd04"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/003- AutoCoder 使用Web版大模型，性感的Human As Model 模式.md", "relative_path": "003- AutoCoder 使用Web版大模型，性感的Human As Model 模式.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/003- AutoCoder 使用Web版大模型，性感的Human As Model 模式.md", "source_code": "# 003- AutoCoder 使用Web版大模型，性感的Human As Model 模式\n\n前面我们提到，如何解决你没有API版大模型，或者你的API版大模型太弱，而你只有Web版本的诸如 Kimi/GPT4 的情况下，改如何让AutoCoder帮助你完成编程？\n\n我们有两个办法，第一个是去掉 execute/auto_merge 两个参数。这个时候你可以在 target_file 里找到你的prompt,拖拽到 Web版里就行。生成的代码，你基本上可以直接复制黏贴过来。\n\n但是，你可能希望比如自动合并等环节还是让 AutoCoder做掉，该怎么办呢？这里我们有一个新参数叫 human_as_model。我们来看看怎么用：\n\n```yml\nsource_dir: /tmp/t-py\ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nmodel: deepseek_chat\nmodel_max_length: 2000\nmodel_max_input_length: 30000\nanti_quota_limit: 0\n\nsearch_engine: bing\nsearch_engine_token: ENV {{BING_SEARCH_TOKEN}}\n\n## execute the prompt generated by auto-coder\nexecute: true\n## extract the code from the prompt generated by auto-coder \n## and overwrite the source code\nauto_merge: true\n\nproject_type: py\n\nhuman_as_model: true\n\nquery: >\n  在 read_root 方法前新添加一个方法，\n  对应的rest 路径为 /hello, \n  输出返回值为  \"world\"  \n```\n这里相比以前，就多了一个 human_as_model 参数，我们把他设置为 true。现在我们希望添加一个新的http 接口。执行下我们的需求：\n\n```shell\nauto-coder --file ./examples/from-zero-to-hero/003_human_as_model.yml\n```\n\n这个时候终端会自动进入交互模式：\n\n![](../images/image5.png)\n\n他提示你，他已经把问题保存到了 output.txt 文件里了，你来回答下这个问题。我这个时候把问题贴给 Claude:\n\n![](../images/image6.png)\n\nCalude 生成了下面的代码：\n\n![](../images/image7.png)\n\n现在我们复制下，注意要复制整个回复，而不是复制代码，复制完成后是这样子的：\n\n![](../images/image8.png)\n\n在最后EOF 表示你黏贴完了，再按回车。\n\n这个时候你再打开 server.py 文件，内容已经更新了：\n\n![](../images/image9.png)\n\n我们新增了一个新的接口。\n\n当你的项目完成度越高，模型的生成能力也会越强，因为有更多的新，从而可以直接帮你完成业务代码的生成。\n\n## 如何应对多文件修改(只针对 auto_merge: wholefile 的情况)\n\n因为大模型生成长度通常是有限制的，而且远远小于（尤其是在web版里）输入。这个时候，你可以开启下面的参数：\n\n```yaml\nenable_multi_round_generate: true\n```\n\n此时，\n\n1. 当你把prompt黏贴到web版里，只会生成一个文件的的代码，你把生成的代码你黏贴到 auto-coder 里，然后以新行 EOF  结尾\n2. 此时AutoCoder 不会结束，会继续给你新的Prompt 让你黏贴\n3. 这样循环往复，直到你的需求完成。\n4. 有的时候大模型没有在适当的时候说\"\\_\\_完成\\_\\_\", 你可以手动输入 \"\\_\\_完成\\_\\_\" 或者 \"\\_\\_EOF\\_\\_\"或者 `/done` 结束这个循环，最后再加一个换行加 EOF 来完成最后的提交。\n\n## 更多控制\n\n我们提供了一些控制选项，方便你更好的控制 human_as_model 的行为：\n\n1. 在输入的最后一行，输入 `/break` 会终止请求。\n2. 在输入的最后一行，输入 `/clear` 清理之前的黏贴，你可以重新进行黏贴。\n3. 最后一行输入 EOF 或者 `/eof` 则表示结束当前的输入。\n\n## 总结\n\n我们也可以看到，单次需求不要太大，否则会导致结果不可控。未来开发代码，会是这么一个流程：\n\n![](../images/image10.png)\n\n每个需求都是一段文本，每个人都看得懂。做了多少变更，每个变更是什么，基本上都文本化了，对于产品，或者研发而言都是很大的利好。\n\n同时配合 Git 的版本管理，保证AutoCoder的安全性，通过 Test 测试集，保证 AutoCoder的正确性。\n\n下期，我们来介绍下，如何引入文档和搜索引擎，让AutoCoder 在阅读你已经写好的代码同时，再参考文档和自己去做搜索，来完成最终需求的代码编写。\n", "tag": "", "tokens": 954, "metadata": {}}], "modify_time": 1719633866.091841, "md5": "7051ae37623ddbb765013210af2fb4bf"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/040_AutoCoder_辅助阅读代码的一些技巧和场景.md", "relative_path": "040_AutoCoder_辅助阅读代码的一些技巧和场景.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/040_AutoCoder_辅助阅读代码的一些技巧和场景.md", "source_code": "# 040_AutoCoder_辅助阅读代码\n\n在 [038-AutoCoder_为什么你需要经过反复练习才能用好](./038-AutoCoder_为什么你需要经过反复练习才能用好.md) 我们其实已经举了一个例子，\n用来说明如何让 AutoCoder帮你解决一些项目的疑惑。\n\n```yaml\ninclude_file:\n  - ./base.yml\n\nindex_filter_level: 2\nquery: |\n  为什么 udiff_coder 进行 get_edits  和 apply_edits 时，diff @@ ... @@ 不需要包括\n  行号。\n  \n```\n\nauto-coder 会帮你找到 udfiff_coder文件爱你，并且定位到 `get_edits` 和 `apply_edits` 这两个函数，然后进行解读。所以 auto-coder 特别适合有针对性的分析需求。\n\n我们再看另外一个例子：\n\n```yaml \ninclude_file: \n   - ./common/local.yml\n\nquery: |   \n   帮我找到所有包含 import os 的文件，并且确认这些文件中所有路径的写法是不是兼容了 windows 和 linux。\n   不用生成代码，告诉我那些文件是不兼容的。\n```\n\n因为用户想知道任何包含了 import os 的文件里面，有没有遗漏了比如使用  \"/tmp/abc\" 这种的特定于平台的写法。 auto-coder 会帮你找到所有包含 `import os` 的文件，并且定位到这些文件中的路径写法，然后告诉你哪些文件是不兼容的。\n\n下面是一个解读结果：\n\n![](../images/040-01.png)\n\n当然了，你可以反复的试下不同的 query，来获得一个可能更准确的效果。比如上面的话，你还可以这样问：\n\n```yaml\nquery: |   \n   帮我找到所有包含 import os 的文件，并且确认这些文件中所有路径的写法是不是兼容了 windows 和 linux。比如里面如果有 /tmp/abc 这种写法，就是不兼容的的，因为 windows 下面是 \\tmp\\abc，代码里面应该使用 os.path.join 来处理或者 pathlib 处理。\n   不用生成代码，告诉我那些文件是不兼容的。\n```\n\n这个描述其实效果理论上会比前面的更好一些。\n\nauto-coder 暂时还不适合用来做架构方面的分析，比如把整个项目绘制一个类图，并且画出类之间的关系。但是对于代码的分析，特别是对于代码中的某个问题的定位，auto-coder 是非常有用的。\n\n当然了，还可以结合 auto-coder 知识库，之前我们在 [](./000-AutoCoder_准备旅程.md) 中介绍了如何把文档放进知识库。你也可以完全把整个项目\n都放进知识库，然后再发起提问，但这种模式的准确率和前面有针对性局部问题会有所差别，两者结合可能效果更好。\n  \n\n\n", "tag": "", "tokens": 563, "metadata": {}}], "modify_time": 1717328110.627379, "md5": "ca923b7805a9f80ce5c4fcc952313702"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/022-AutoCoder多知识库支持.md", "relative_path": "022-AutoCoder多知识库支持.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/022-AutoCoder多知识库支持.md", "source_code": "# 022-AutoCoder多知识库支持.md\n\n在 [019-AutoCoder对本地文档自动构建索引](019-AutoCoder对本地文档自动构建索引.md) 中，我们提到了如何通过 AutoCoder 对本地文档进行索引构建。在实际工作中，我们可能会遇到多个知识库的情况，\n\n这可能是因为：\n\n1. 业务线不同，知识库不同。\n2. 所有内容都放在一个知识库中，会导致查询精度下降。\n\n这时我们可以通过 AutoCoder 的多知识库支持功能，将多个知识库的索引合并到一起，系统会自动\n选择最匹配的知识库进行查询。\n\n## 如何配置多知识库支持\n\n\n默认 byzerllm 构建的知识库名字叫default. \n\n### 创建一个新的知识库\n\n```bash\nbyzerllm storage collection --name llm --description \"使用ByzerLLM框架来部署和管理多种模型，包括如何启动、停止及配置Ray服务和Byzer-LLM的存储模板。此外，还介绍了如何通过Byzer-LLM部署各种预训练模型到云服务，涵盖了从OpenAI到自定义模型的部署，以及如何配置和调用这些模型的详细过程。这些操作使用户能够根据需求灵活部署和测试多种大规模语言模型。\"\n```\n其中description非常重要，因为这个描述会在使用时，让 AutoCoder 根据你的问题，参考该信息选择最匹配的知识库。\n\n如果你后续觉得描述不好，你可以按相同的方式更新描述。\n\n### 构建知识库\n\n```bash\nauto-coder doc build --source_dir /Users/allwefantasy/projects/doc_repo/deploy_models \\\n--model gpt3_5_chat \\\n--emb_model gpt_emb \\\n--collection llm\n```\n\n这个时候会将数据写入新的知识库llm中。\n\n### 使用多知识库\n\n```bash\nauto-coder doc query --file ~/model.yml --query \"如何启动 deepseek_chat\" --collections llm,default\n```\n我们通过 `--collections` 参数指定了我们要查询的多个知识库，这样 AutoCoder 就会选择最匹配的知识库进行查询。\n\n### 当前限制\n\n1. chat/serve 模式下，只支持一个知识库。\n2. query 模式下，目前只会自动选择一个知识库来查询，目前还不能跨知识库联合查询。\n\n\n", "tag": "", "tokens": 496, "metadata": {}}], "modify_time": 1714566897.9387796, "md5": "885a26f3aff9cf4c4971ae28ede85808"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/020-AutoCoder参数大全.md", "relative_path": "020-AutoCoder参数大全.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/020-AutoCoder参数大全.md", "source_code": "\n# AutoCoder 参数大全\n\nAutoCoder 是一个强大的代码自动生成工具,可以根据用户的需求自动生成相应的代码。本文档详细介绍了 AutoCoder 支持的各种可选参数,方便用户进行查阅和使用。\n\n## 通用参数\n\n- `--source_dir`: 项目源代码目录路径\n- `--git_url`: 用于克隆源代码的 Git 仓库 URL\n- `--target_file`: 生成的源代码的输出文件路径  \n- `--query`: 用户查询或处理源代码的指令\n- `--template`: 生成源代码使用的模板。默认为 'common'\n- `--project_type`: 项目类型。可选值:py、ts、py-script、translate 或文件后缀名。默认为 'py'\n- `--execute`: 是否执行生成的代码。默认为 False\n- `--package_name`: 仅适用于 py-script 项目类型。脚本的包名。默认为空。\n- `--script_path`: 仅适用于 py-script 项目类型。Python 脚本路径。默认为空。\n- `--model`: 使用的模型名称。默认为空  \n- `--model_max_length`: 模型生成代码的最大长度。默认为 2000。仅在指定模型时生效。\n- `--model_max_input_length`: 模型的最大输入长度。默认为 6000。仅在指定模型时生效。\n- `--vl_model`: 要使用的多模态模型的名称。默认为空\n- `--sd_model`: 要使用的稳定扩散模型的名称。默认为空  \n- `--emb_model`: 要使用的嵌入模型的名称。默认为空\n- `--index_model`: 用于构建索引的模型名称。默认为空\n- `--index_model_max_length`: 索引模型生成代码的最大长度。默认为 0,表示使用 `--model_max_length` 的值  \n- `--index_model_max_input_length`: 索引模型的最大输入长度。默认为 0,表示使用 `--model_max_input_length` 的值\n- `--index_model_anti_quota_limit`: 每次索引模型 API 请求后等待的秒数。默认为 0,表示使用 `--anti_quota_limit` 的值\n- `--index_filter_level`: 索引过滤级别,0:仅过滤 query 中提到的文件名,1. 过滤 query 中提到的文件名以及可能会隐含会使用的文件 2. 从 0,1 中获得的文件,再寻找这些文件相关的文件。默认为 0。\n- `--index_filter_workers`: 用于通过索引过滤文件的工作线程数。默认为 1。\n- `--file`: YAML配置文件路径。默认为空。\n- `--ray_address`: 要连接的 Ray 集群的地址。默认为 'auto'  \n- `--anti_quota_limit`: 每次 API 请求后等待的秒数。默认为 1 秒  \n- `--skip_build_index`: 是否跳过构建源代码索引。默认为 False\n- `--print_request`: 是否打印发送到模型的请求。默认为 False\n- `--py_packages`: 添加到上下文的 Python 包,仅适用于 py 项目类型。默认为空。\n- `--human_as_model`: 是否使用人工作为模型。默认为 False\n- `--urls`: 要爬取并提取文本的 URL,多个 URL 以逗号分隔。默认为空。\n- `--urls_use_model`: 是否使用模型处理 urls 中的内容。默认为 False\n- `--search_engine`: 要使用的搜索引擎。支持的引擎:bing、google。默认为空\n- `--search_engine_token`: 搜索引擎 API 的令牌。默认为空  \n- `--enable_rag_search`: 是否开启使用搜索的检索增强生成。默认为 False\n- `--enable_rag_context`: 是否开启使用上下文的检索增强生成。默认为 False\n- `--auto_merge`: 是否自动将生成的代码合并到现有文件中。默认为 False。\n- `--image_file`: 要处理的图像文件路径。默认为空\n- `--image_max_iter`: 图像转 html 的最大迭代次数。默认为 1  \n- `--enable_multi_round_generate`: 是否开启多轮对话生成。默认为 False\n\n## 子命令参数\n\n### revert \n\n- `--file`: 要撤销更改的文件路径\n\n撤销指定文件所做的更改。\n\n### store\n\n- `--source_dir`: 项目源代码目录路径\n- `--ray_address`: 要连接的Ray集群的地址。默认为'auto'\n\n一些统计信息,比如 token 使用等。\n\n### index\n\n- `--file`: YAML配置文件路径\n- `--model`: 使用的模型名称。默认为空\n- `--index_model`: 用于构建索引的模型名称。默认为空\n- `--source_dir`: 项目源代码目录路径\n- `--project_type`: 项目类型。可选值:py、ts、py-script、translate或文件后缀名。默认为'py' \n- `--ray_address`: 要连接的Ray集群的地址。默认为'auto'\n\n构建源代码索引。\n\n### index-query\n\n- `--file`: YAML 配置文件路径\n- `--model`: 使用的模型名称。默认为空\n- `--index_model`: 用于构建索引的模型名称。默认为空\n- `--source_dir`: 项目源代码目录路径\n- `--query`: 用户查询或处理源代码的指令\n- `--index_filter_level`: 索引过滤级别,0:仅过滤 query 中提到的文件名,1. 过滤 query 中提到的文件名以及可能会隐含会使用的文件 2. 从 0,1 中获得的文件,再寻找这些文件相关的文件。默认为 2。\n- `--ray_address`: 要连接的 Ray 集群的地址。默认为 'auto'\n\n根据索引查询相关文件。\n\n### doc \n\n- `--urls`: 要爬取并提取文本的 URL,多个 URL 以逗号分隔。默认为空。\n- `--model`: 使用的模型名称。默认为空\n- `--target_file`: 生成的源代码的输出文件路径。默认为空。\n- `--file`: YAML 配置文件路径。默认为空。\n- `--source_dir`: 项目源代码目录路径\n- `--human_as_model`: 是否使用人工作为模型。默认为 False\n- `--urls_use_model`: 是否使用模型处理 urls 中的内容。默认为 False\n- `--ray_address`: 要连接的 Ray 集群的地址。默认为 'auto'\n\n对文档进行一些操作,诸如获取 html 的正文内容。\n\n#### doc build\n\n- `--source_dir`: 项目源代码目录路径。默认为空。\n- `--model`: 使用的模型名称。默认为空\n- `--emb_model`: 要使用的嵌入模型的名称。默认为空\n- `--file`: YAML 配置文件路径。默认为空。\n- `--ray_address`: 要连接的 Ray 集群的地址。默认为 'auto'\n- `--required_exts`: doc 构建所需的文件扩展名。默认为空字符串\n\n#### doc query\n\n- `--query`: 用户查询或处理源代码的指令。默认为空。\n- `--source_dir`: 项目源代码目录路径。默认为 '.'\n- `--model`: 使用的模型名称。默认为空\n- `--emb_model`: 要使用的嵌入模型的名称。默认为空\n- `--file`: YAML 配置文件路径。默认为空。\n- `--ray_address`: 要连接的 Ray 集群的地址。默认为 'auto'  \n- `--execute`: 是否执行生成的代码。默认为 False\n\n#### doc chat\n\n- `--file`: YAML 配置文件路径。默认为空。  \n- `--model`: 使用的模型名称。默认为空\n- `--emb_model`: 要使用的嵌入模型的名称。默认为空\n- `--ray_address`: 要连接的 Ray 集群的地址。默认为 'auto'\n- `--source_dir`: 项目源代码目录路径。默认为 '.'\n\n#### doc serve\n\n- `--file`: YAML 配置文件路径。默认为空。\n- `--model`: 使用的模型名称。默认为空  \n- `--emb_model`: 要使用的嵌入模型的名称。默认为空\n- `--ray_address`: 要连接的 Ray 集群的地址。默认为 'auto' \n- `--source_dir`: 项目源代码目录路径。默认为 '.'\n- `--host`: 服务绑定的主机。默认为空。\n- `--port`: 服务绑定的端口。默认为 8000。\n- `--uvicorn_log_level`: Uvicorn 日志级别。默认为 'info'。\n- `--allow_credentials`: 是否允许凭证。默认为 False。\n- `--allowed_origins`: 允许的来源列表。默认为 ['*']。\n- `--allowed_methods`: 允许的 HTTP 方法列表。默认为 ['*']。  \n- `--allowed_headers`: 允许的 HTTP 头列表。默认为 ['*']。\n- `--api_key`: API 密钥。默认为空。\n- `--served_model_name`: 服务的模型名称。默认为空。  \n- `--prompt_template`: 提示模板。默认为空。\n- `--ssl_keyfile`: SSL 密钥文件路径。默认为空。\n- `--ssl_certfile`: SSL 证书文件路径。默认为空。\n- `--response_role`: 响应角色。默认为 'assistant'。\n\n以上就是 AutoCoder 支持的各种可选参数的详细介绍。用户可以根据自己的需求选择合适的参数来使用 AutoCoder,从而提高代码生成的效率和质量。如有任何疑问,欢迎随时联系我们。", "tag": "", "tokens": 2105, "metadata": {}}], "modify_time": 1713921193.6502833, "md5": "cb4c25e375cf5b28360fdef61abdc7c2"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/002- 用 AutoCoder 添加和修改代码.md", "relative_path": "002- 用 AutoCoder 添加和修改代码.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/002- 用 AutoCoder 添加和修改代码.md", "source_code": "# 002- 用 AutoCoder 添加和修改代码\n\n实际上 AutoCoder 最适合的场景是修改代码，因为它最原始的功能是把源码+你指定的文档+搜索引擎搜集到的资料和你的需求生成一个prompt给到大模型。\n\n此外纠正大家一个观点， AutoCoder 不是一句话给你创建一个网站抑或一个大型的项目。我们是帮助研发或者产品更快的迭代产品。\n\n不过我们还是从最开始项目啥都没有开始，现在，我们用 AutoCoder 创建一个 web server\n\n```yml\nsource_dir: /tmp/t-py\ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nmodel: deepseek_chat\nmodel_max_length: 2000\nmodel_max_input_length: 30000\nanti_quota_limit: 0\n\nsearch_engine: bing\nsearch_engine_token: ENV {{BING_SEARCH_TOKEN}}\n\n## execute the prompt generated by auto-coder\nexecute: true\n## extract the code from the prompt generated by auto-coder \n## and overwrite the source code\nauto_merge: true\n\nproject_type: py\n\nquery: >\n  在 /tmp/t-py/server/server.py \n  中使用 FastAPI 创建一个web服务。\n  服务的根路径返回 \"Hello, World!\"。\n```\n\n注意，这里我们开启了 auto_merge 参数。这个参数会修改你的项目，所以总体来说还是慎用。如果这个参数没有开启，可以在 target_file 里找到生成的代码，可以自己手动复制黏贴下。\n\n执行下：\n\n```shell\n\nauto-coder --file ./examples/from-zero-to-hero/002_fastapi_hello_word.yml\n```\n\n日志比较简单：\n\n```\n2024-03-21 17:55:08.944 | INFO     | autocoder.dispacher.actions.action:process_content:225 - Auto merge the code...\n2024-03-21 17:55:08.945 | INFO     | autocoder.common.code_auto_merge:merge_code:51 - Upsert path: /tmp/t-py/server/server.py\n2024-03-21 17:55:08.945 | INFO     | autocoder.common.code_auto_merge:merge_code:55 - Merged 1 files into the project.\n```\n\n结果：\n\n```\n(byzerllm-dev) (base) winubuntu@winubuntu:/tmp/t-py$ tree\n.\n└── server\n    └── server.py\n\n1 directory, 1 file\n```\n\n执行下代码：\n\n```\n(byzerllm-dev) (base) winubuntu@winubuntu:/tmp/t-py$ python /tmp/t-py/server/server.py\nINFO:     Started server process [1333520]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\nINFO:     127.0.0.1:33516 - \"GET / HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:33516 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n```\n\n访问下：\n\n![](../images/image4.png)\n\n成功搞定。\n\n现在，让我们来修改下代码。\n\n```yml\nsource_dir: /tmp/t-py\ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nmodel: qianwen_chat\nmodel_max_length: 2000\nmodel_max_input_length: 6000\nanti_quota_limit: 5\n\nsearch_engine: bing\nsearch_engine_token: ENV {{BING_SEARCH_TOKEN}}\n\n## execute the prompt generated by auto-coder\nexecute: true\n## extract the code from the prompt generated by auto-coder \n## and overwrite the source code\nauto_merge: true\n\nproject_type: py\n\nquery: >\n  修改 server.py 中的端口，改成 9001\n```\n\n执行下：\n\n```shell\n\nauto-coder --file ./examples/from-zero-to-hero/002_fastapi_modify_port.yml\n```\n\n此时打开 server.py 后，可以看到端口已经被改成 9001了\n\n```python\nfrom fastapi import FastAPI\n\n# 创建FastAPI应用实例\napp = FastAPI()\n\n# 定义根路径的GET请求处理函数，返回 \"Hello, World!\"\n@app.get(\"/\")\ndef read_root():\n    return {\"message\": \"Hello, World!\"}\n\nif __name__ == \"__main__\":\n    # 启动web服务，端口改为9001\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=9001)\n```\n\n这里你可能好奇，为啥前面我可以直接说修改 server.py 而不需要写完整路径了。因为这次项目里有python文件了，有上下文，所以大模型是可以自动推测的，你不需要写那么完整。\n\n今天内容就到这里。下一期我们看看如何解决你没有大模型API，或者你的API版大模型太弱，在只有Web版本的诸如 Kimi/GPT4 的情况下，AutoCoder如何帮助你完成编程。\n\n这里剧透下：\n\n\n1. AutoCoder 之负责生成Prompt,你可以拖拽文件到Web版大模型里去。\n\n2. AutoCoder 在需要用到的大模型的地方，会询问你，这个时候你可以把结果贴到web版里，然后再把结果贴回来，帮助AutoCoder完成整个流程。", "tag": "", "tokens": 1156, "metadata": {}}], "modify_time": 1716937368.177736, "md5": "13d5ca3b0c13002c632a938ee2195a22"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/为什么不是GithubCopilot,不是Devin而是AutoCoder.md", "relative_path": "为什么不是GithubCopilot,不是Devin而是AutoCoder.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/为什么不是GithubCopilot,不是Devin而是AutoCoder.md", "source_code": "# 为什么不是GithubCopilot,不是Devin而是AutoCoder\n\n我之前常说，不要逆AGI潮流去做一些事情，但也要对当前的大模型的边界有清晰的了解。\n\nGithub Copilot 本质还是IDE工具的衍生，是一个更加“智能”的代码提示，而其提供的Copilot Chat 则更加只是把一个聊天框做到IDE而已，和集成一个搜索框到IDE工具没有任何区别，然还是一个古典产品的思维在做的一个产品。\n\n更细节的，我可以从三个维度做给大家做分析：\n\n第一个维度是 Github Copilot 的定位，我一直是 Github Copilot 的铁杆用户，但因为它的定位是只能代码提示，这决定了他需要追求响应延时而不是效果，所以他最大的问题是，它无法基于整个项目的源码去做新的代码实现（这样会导致延时增加到不可接受，并且成本太高）。\n\n第二个维度是 Github Copilot 无法模拟人类的开发行为，我们实际做开发的时候，一般都是基于已有功能，并且根据某种“文档”，“第三方代码”和“搜索引擎”来进行开发。\n\n比如 Byzer-LLM 要对接 Qwen-vl 多模态大模型，那么作为一个开发，我至少需要准备三个事情：\n\n1. 首先我们需要了解和参考Byzer-LLM 之前是怎么对接各种模型的代码\n2. 其次我要找到 Qwen-VL的API 文档了解 Qwen-VL 的API\n3. 我可能还需要搜索下参考下别人是怎么对接的，以及如果我使用了第三方SDK，我还需要第三方SDK的文档或者代码。\n\n只有获取了这些信息之后，我们才能写出一个靠谱的代码。但 Github copilot 能做到这些么？显然做不到。\n\n第三个维度是，我没有办法替换模型，也就是只能用 Github Copilot 背后的模型，哪怕我有 GPT-4/Claude3-Opus的 web订阅版，如果是公司在用，如何保证模型的私有化部署呢？\n\n所以 Github Copilot 的产品本质决定了他只是一个更加smart的提示工具，而不是模拟人去编程。这个虽然说不上逆AGI潮流，但确实不够 AI Native, 没有把AI 充分利用起来。\n\n而 Devin 则走了另外一个极端，他是一个AI辅助编程工具，但是他的目标是让AI通过一个简单的需求收集，自动完成为了实现这个需求的所有流程，需求分析，拆解，环境安装，构建项目，编写代码，自动debug,甚至自动运行，这个目标太过于宏大，而且不符合基本的规律，在当前大模型的能力边界上，基本只能作为Demo和科研探索。\n\n为啥不符合基本的规律呢？因为但凡做过研发的人就知道，需求是一个动态变化的过程，是迭代出来的，也是迭代的过程中慢慢想明白的，中间有太多甲方，乙方，商业，技术，人性等等的因素在里面。所以我说 Devin 不仅仅目标过于宏达难以实现，而且也不符合基本的规律，就算AGI实现，也难以完全可行。\n\n正好这两天看到一个star数很高的项目，也是做AI辅助编程的。作者用该工具来辅助自己用JS创建一个聊天程序，基本思路就是通过terminal shell交互来完成沟通，结果演示的时候，在最后关头，AI写的代码出现死循环，因为已经花了大量时间了，他又不想剪辑，就说，反正整体演示出来了。。。很尴尬。。。。\n\n说白了，就算为了完成一个简单的应用，当前最聪明的大模型失败率都太高了。我之前在做 AutoCoder 的时候，也发现，就是根据用户的需求创建一个基本的项目模板，要稳定的重现，对很多大模型来说都是有挑战的。\n\n所以我总结他演示其实有两个大的问题：\n\n1. 他演示整个过程最大的败笔就是不让程序员去修改和调整代码。\n2. 此外，现存需要维护的代码远比新建的代码多，如何帮助用户实现现有代码的迭代，远比重新创建一个新项目更有价值和挑战。\n\n虽然该项目的同学还是很理性的认识到：\n\n我不认为A能够（至少在不久的将来）创建应用程序而不涉及开发人员。因此，GPT Pilot会逐步编写应用程序，就像开发人员在现实生活中一样。\n\n但就像很多人说自己坚信Scaling Law，但在具体实践的时候又容易背离Scaling Law。\n\n所以，我在做AI辅助类编程产品的时候，核心有如下几个理念：\n\n1. 契合现有模型的能力，专注模型当前能够做好的环节，避免好高骛远。\n2. 产品要满足和人一起小步快跑，保证和需求迭代同步。\n3. 要留有足够扩展性，需要保留很多有想象空间的的的功能，这些功能随着模型的提升，一个一个都会有质的变化。\n\n\n根据这么几个理念： AutoCoder 一些特点就出来了。\n\n## 专注在编码环节\n\nAutoCoder专注在编码环节，相比需求分析，拆解，环境安装，构建项目，编写代码，自动debug,甚至自动运行测试，我们只专注在编程这个环节，让这个环节真正做到提效，稳定，可靠。\n\n## 模拟程序员编码环节，和程序员一起小步快跑\n\n我们模拟了程序员在实现代码的一些过程，使得AutoCoder能够阅读已有项目源码，阅读第三方库，参考文档，自动搜索寻找问题参考，在才能让AutoCoder对当前程序员的需求有足够清晰的理解，从而实现更加准确而好用更好的代码。\n\n此外，我们鼓励程序员使用进行小步快跑的方式使用AutoCoder,每次只做tiny 迭代，类似于人类一个大PR 需要拆解成几个小PR才能被提交。\n\n此外，我们还提供了仅生成Prompt,自动执行Prompt,到自动merge代码 三种可选自动化程度，在每一步小迭代中都可以由用户来选择自动化什么程度。由于每一个小迭代，我们都采用了yml配置来描述需求，意味着用户可以回顾自己迭代的整个过程。\n\n## 保留足够的扩展性\n\n我们也提供诸如目标分析，需求拆解，规划，自动生成流程步骤，并且自动执行等等能力，但是这个只是为了保证AutoCoder 在模型能力达到一定程度后，相应的功能能够自动可用，保证AutoCoder 随时能享受模型进步带来的红利。\n\n## 总结\n\n所以，AutoCoder 相比Github Copilot,对研发人员的效率提升会更加上一个台阶，而相比Devin,我们是可以真正落地，给到广大开发人员提升效率，帮助企业降低研发成本的，而非一个Demo或者科研产品，但我们也提供了有竞争力的功能，保证我们在模型进步后，能够快速的对用户提供更多的价值。\n\n如果你对 AutoCoder 感到好奇，那么不妨关注下我们文档: https://github.com/allwefantasy/auto-coder/tree/master/docs", "tag": "", "tokens": 1343, "metadata": {}}], "modify_time": 1711809217.9074173, "md5": "ce84081e8d301450fc9a821904f1db49"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/000-AutoCoder_准备旅程.md", "relative_path": "000-AutoCoder_准备旅程.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/000-AutoCoder_准备旅程.md", "source_code": "# 000-AutoCoder_准备旅程\n\n本篇内容会介绍使用 SaaS API 快速为你的已有项目设置auto-coder。\n\n> auto-coder 是一个基于YAML配置的命令行开发辅助工具，可以根据您的需求自动迭代开发已有项目。\n\n## 安装 auto-coder\n\n```shell\nconda create --name auto-coder python=3.10.11\nconda activate auto-coder\npip install -U auto-coder\nray start --head\n```\n\n如果没有 conda, 可以这么来安装：\n\n```bash\nwget https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-py38_4.12.0-Linux-x86_64.sh\nbash Miniconda3-py38_4.12.0-Linux-x86_64.sh\n```\n\n\n如果你是Windows用户，参考这个：\n可以参考这个[043_AutoCoder_Windows安装特别说明](../zh/043_AutoCoder_Windows%E5%AE%89%E8%A3%85%E7%89%B9%E5%88%AB%E8%AF%B4%E6%98%8E.md)。\n\n## 启动推荐的模型代理\n\n大语言模型(你需要去deepseek官网申请token),然后执行下面的命令。\n\n> 注意要替换 ${MODEL_DEEPSEEK_TOKEN} 和 ${MODEL_QIANWEN_TOKEN} 为你的实际token。\n\n```shell\neasy-byzerllm deploy deepseek-chat --token $MODEL_DEEPSEEK_TOKEN --alias deepseek_chat\n```\n\n运行起来后，你可以通过下面的命令快速验证下：\n\n```bash\neasy-byzerllm chat deepseek_chat \"你好\"\n```\n\n如果遇到异常，打开 [Actors页面](http://127.0.0.1:8265/#/actors)，\n\n![](../images/000-05.png)\n\n点击worker的链接，进去后是这样的：\n![](../images/000-06.png)\n\n切换 STDOUT/STDERR 标签查看是否有什么异常。\n\n如果有异常，可以通过下面命令来删除模型，再重新部署。\n\n```bash\neasy-byzerllm undeploy deepseek_chat --force\n```\n\n更多模型参考： [easy-byzerllm](https://github.com/allwefantasy/byzer-llm/blob/master/docs/zh/004_easy_byzerllm_%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97.md)\n\n\n向量模型(可选，你需要去qwen官网申请token，如果麻烦，可以跳过先),然后执行下面的命令。\n\n```shell\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n## 初始化已有项目\n\n进入你的项目根目录，执行下面的命令。\n\n```shell\nauto-coder init --source_dir .\n```\n系统会自动在当前目录下生成 `.auto-coder`,`actions` 两个目录。\n\n在 actions 目录下会包含一些YAML 模板文件(auto-coder 主要是负责执行这些 yaml文件完成代码相关的工作)\n\n1.  `101_current_work.yaml` 提供中英文解释的参数列表。\n2.  `actions/base` 目录下的提供了一些基础的能力，你可以在你的新建的YAML 文件中引用这些文件从而开启特定的能力。\n\n最佳实践是将你新的 YAML 文件放在 actions 目录，并且以 00x_ 开头，比如我们已经在 actions 提供了一个范例文件:`000_example.yml`。\n\n\n## 使用 chat-auto-coder 为你编程\n\n在项目根目录，执行\n\n```bash\nchat-auto-coder\n```\n\n输入coding指令进行编程：\n\n```bash\n/coding 在 src 目录下创建app.py, 在该文件中实现一个计算器，使用 gradio 来实现。 \n```\n\n然后等待一段时间，系统会自动在你的项目中生成一个文件 `src/app.py`。然后通过 `python src/app.py` 来启动服务，复制终端中的链接到浏览器中，就可以看到一个计算器的页面。\n\n恭喜，你已经使用 chat-auto-coder 完成了第一个编程任务。\n\n欢迎观看 [如何在聊天中完成代码编写](https://www.bilibili.com/video/BV17M4m1m7MW/) 视频，了解更多关于 chat-auto-coder 的使用。\n\n此外我们也提供了文字版本的教程： \n\n1. [046_AutoCoder_Chat-Auto-Coder指南](./046_AutoCoder_Chat-Auto-Coder指南.md)\n2. [047_chat-auto-coder使用实践](./047_chat-auto-coder使用实践.md)\n\n## 使用基础 auto-coder 为你编程\n\n打开 actions/000_example.yml 文件,内容打开是这这样的：\n\n```yaml\ninclude_file:\n  - ./base/base.yml\n  - ./base/enable_index.yml\n  - ./base/enable_wholefile.yml    \n\nquery: |\n  YOUR QUERY HERE\n```\n\n前面我们启动的模型，实际上都配置在 `actions/base/base.yml`中。\n\n你可以在 `query` 字段中填写你的功能或者业务需求，\n并且**关闭 human_as_model模式**,可以让deepseek 模型直接生成代码。如果不关闭，那么会弹出一个输入框，请求会\n转发给用户而不是发送给大模型。具体参看[性感的human_as_model模式](../zh/003-%20AutoCoder%20使用Web版大模型，性感的Human%20As%20Model%20模式.md)。\n\n另外我们还修改了 project_type参数，该参数定义了你的项目类型，方便过滤掉一些无用的文件。\n\n具体支持以下类型：\n\n1. py\n2. ts\n3. 任何文件后缀名组合，使用逗号分隔，比如：.java,.scala\n\n最后看起来是这样的（注意变化点）：\n\n```yaml\n## 变化 1\nproject_type: py\ninclude_file:\n  - ./base/base.yml\n  - ./base/enable_index.yml\n  - ./base/enable_wholefile.yml    \n\n## 变化2\nhuman_as_model: false  \n\n## 变化3\nquery: |  \n  帮我在项目根目录下创建一个 src/server.py, 使用 fastapi ，创建一个 /hello 接口，返回 world.\n```\n\n> 默认开启了 auto_merge, 也就是会直接修改你的项目，auto-coder会检测你的项目是否有git，如果有，会在修改前自动\n> 做一次commit, 以便于你可以方便的回滚。如果没有，会拒绝合并。\n\n运行 auto-coder:\n\n```shell\nauto-coder --file actions/000_example.yml\n```\n\n注意，运行后可能有两种情况:\n\n1. 如果你的项目里面的代码比较多，会自动构建索引，比如800个java文件大概要花10-20分钟，大概2元左右的费用。\n2. 如果你的项目很小，或者是空项目，无需做任何其他操作，按 auto-coder 的提示一直选择Ok或者确认操作即可。\n\n另外，auto-coder 运行后还会询问你要选择哪些文件来作为当前需求的context，你根据需要进行筛选，尽量保持较小的一个数值，减少token的输入。\n\n最后在自己的项目里就可以看到这么一个文件了。\n\n```python\n# src/server.py\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/hello\")\nasync def hello():\n    return \"world\"\n```\n\n> 如果要应对真实项目需求开发，请开启 human_as_model 模式,并结合Web版本的 Claude Opus 模型来获得最佳体验。\n\n## 给自己构建一个本地 auto-coder 小助手\n\n因为 auto-coder 本身是一个命令行+YAML配置文件的编程辅助工具，所以就会涉及到一些配置和用法，然后你如果不愿意自己去一个文档一个文档查。\n这个时候你有两个选择：\n\n1. 使用 auto-coder 的知识库功能，自己构建一个小助手。\n2. 使用 devv.ai 的 github 知识库功能，问询 auto-coder 相关的问题。\n\n\n### 使用 auto-coder 的知识库功能\n\n这一步依赖前面启动的向量模型。\n\n启动知识库：\n\n```shell\nbyzerllm storage start\n```\n\n访问 Ray Dashboard http://127.0.0.1:8265/#/actors 如果看到有三个 Actor 表示检索服务启动成功：\n\n![](../images/000-07.png)\n\n导入 auto-coder 文档：\n\n```shell\ngit clone https://github.com/allwefantasy/auto-coder\ncd auto-coder \nauto-coder doc build --model deepseek_chat --emb_model qianwen_emb --source_dir ./docs/zh --collection auto-coder --description \"AutoCoder文档\"\n```\n\n大概等个几分钟，完工。\n\n注意：若 doc build 失败，请根据上一步检查 Actor 是否启动成功，若 Actor 启动成功，此处构建失败可能与 emb_model 有关，可以更换一个向量模型尝试下。\n\n现在可以和小助手聊天了：\n\n```shell\nauto-coder doc query --model deepseek_chat --emb_model qianwen_emb --query \"如何开启搜索引擎\" --collection auto-coder\n```\n\n你也可以启动一个服务，方便使用一些聊天软件：\n\n```shell\nauto-coder doc serve --model deepseek_chat --emb_model qianwen_emb  --collection auto-coder\n```\n\n下面是一些效果：\n\n![](../images/000-01.png)\n![](../images/000-02.png)\n\n以 [NextChat](https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/releases) 软件为例，配置如下：\n\n![](../images/000-03.png)\n\n密码随意填写。\n\n### 使用 devv.ai 的 github 知识库功能\n\n点击链接： https://devv.ai/search?threadId=dn913itmjchs 直接使用。\n\n![](../images/000-04.png)\n\n## 继续你的旅程\n\n[002- 用 AutoCoder 添加和修改代码](./002-%20%E7%94%A8%20AutoCoder%20%E6%B7%BB%E5%8A%A0%E5%92%8C%E4%BF%AE%E6%94%B9%E4%BB%A3%E7%A0%81.md)\n\n\n\n\n", "tag": "", "tokens": 2192, "metadata": {}}], "modify_time": 1721266317.545382, "md5": "d417e8bf3adb0ea525d2e796d7b2ef95"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/021-AutoCoder初始化项目.md", "relative_path": "021-AutoCoder初始化项目.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/021-AutoCoder初始化项目.md", "source_code": "# 021-AutoCoder 初始化项目\n\n你可以通过如下命令对一个项目进行 auto-coder 初始化：\n\n```shell    \nauto-coder init --source_dir ../test\n```\n\n系统会自动在 `../test` 目录下生成 `.auto-coder`,`actions` 两个目录。\n在 actions 目录下会生成一个 `101_current_work.yaml` 文件，你可以以这个作为模板。", "tag": "", "tokens": 92, "metadata": {}}], "modify_time": 1715214532.3775816, "md5": "5c6036971502f7ab4c57cef82a8469a0"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/018-AutoCoder 索引过滤经验谈.md", "relative_path": "018-AutoCoder 索引过滤经验谈.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/018-AutoCoder 索引过滤经验谈.md", "source_code": "# 018-AutoCoder 索引过滤经验谈\n\n> AutoCoder >= 0.1.29 特性\n\n其实我们前面有好几篇文章都在讲索引的事情：\n\n1. [006-AutoCoder 开启索引，减少上下文](006-AutoCoder%20%E5%BC%80%E5%90%AF%E7%B4%A2%E5%BC%95%EF%BC%8C%E5%87%8F%E5%B0%91%E4%B8%8A%E4%B8%8B%E6%96%87.md)\n2. [017-AutoCoder指定专有索引模型](017-AutoCoder%E6%8C%87%E5%AE%9A%E4%B8%93%E6%9C%89%E7%B4%A2%E5%BC%95%E6%A8%A1%E5%9E%8B.md)\n\n今天我们会主要讲两个事情：\n\n0. 索引过滤的原理\n1. 如何控制过滤出来的文件数量\n2. 有哪些小技巧来弥补没有被过滤进来的文件\n\n## 索引过滤的原理\n\n我们会通过大模型读取所有的项目源码文件（通过project_type来控制），然后对每个文件进行符号(symbols)的提取，然后将符号和文件的关系存储在索引中。这个过程是一个离线的过程，所以你可以通过 `auto-coder index` 命令来构建索引,或者当你通过 `skip_build_index: false` 来开启索引功能时，当你使用AutoCoder的时候系统会自动构建索引或者增量更新索引。\n\n## 如何控制过滤出来的文件数量\n\n接着，当你使用AutoCoder的时候，你可以通过 `query` 来描述你的需求，AutoCoder会根据你的query，从索引中找到符合条件的文件，具体包含了三个步骤：\n\n0. 根据query里提到的文件名，来找到合适的文件。这一步是一个相对准确的匹配过程。比如你可以说，请参考 xxxx.py 文件，此时这个文件大概率会被包含在上下文中。\n1. 根据对query的语义理解，对文件以及文件内部的符号(symbols)进行匹配。这一步是一个相对模糊的匹配过程。比如你可以说，请参考  xxxx 函数，此时这个文件大概率也会被包含在上下文中，当然还有隐式推倒出来的信息，也会对文件和符号进行匹配。\n2. 根据0,1找到的文件，再找到这些文件依赖的文件，该步骤可能导致大量的文件被引入到上下文。\n\n我们通过如下一个参数来控制过滤行为：\n\n- index_filter_level: 0, 1, 2\n\n该参数默认为2, 0表示值开启前面第一个步骤，1，表示开启0,1两个步骤，2表示开启所有步骤。\n\n为了验证这个参数的作用，我们可以通过如下的配置文件：\n\n```yml\nsource_dir: /Users/allwefantasy/projects/auto-coder\ntarget_file: /Users/allwefantasy/projects/auto-coder/output.txt \nproject_type: py\n\nmodel: deepseek_chat\nmodel_max_length: 2000\nmodel_max_input_length: 30000\nanti_quota_limit: 0\n\nindex_model: sparkdesk_chat\nindex_model_max_length: 2000\nindex_model_max_input_length: 10000\nindex_model_anti_quota_limit: 1\nindex_filter_level: 0\n\nskip_build_index: false\n\nquery: |   \n   添加一个新命令\n   \n```\n\n注意，根据索引做文件过滤的模型会使用  model 参数，而不是 index_model 参数。 index_model 参数是用来构建索引的模型。\n\n这里我们设置了 `index_filter_level: 0`，这意味着我们只会根据文件名来过滤文件，我们可以通过下面的命令来验证：\n\n\n```shell\nauto-coder index-query  --file actions/014_test_index_command.yml \n```        \n\n输出结果如下：\n\n```\n+-----------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| file_path                                                                         | reason                                                                                                                                                                                                                                 |\n+===================================================================================+========================================================================================================================================================================================================================================+\n| /Users/allwefantasy/projects/auto-coder/src/autocoder/auto_coder.py               | This file likely contains the main entry point or command handling logic for the 'auto-coder' project. Adding a new command would typically involve modifying this file.                                                               |\n+-----------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| /Users/allwefantasy/projects/auto-coder/src/autocoder/dispacher/actions/action.py | This file seems to define an 'Action' class or module, which might be used to implement individual commands within the project. If commands are implemented as actions, adding a new command may require creating a new subclass here. |\n+-----------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| /Users/allwefantasy/projects/auto-coder/src/autocoder/index/for_command.py        | The file name and location suggest it may contain code related to handling commands, possibly providing functionality or infrastructure for implementing new commands in the project.                                                  |\n+-----------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n```\n\n在比较新的版本里，我们允许用户对过滤出的文件再进行一次确认（对，就是那个丑丑的绿色选择框），移除掉一些不必要的文件。用Tab 移动光标到OK 和 Cancel,然后按回车完成确认。\n\n如果你很烦燥每次都要人工对大模型选择的文件做确认，可以通过参数:\n\n```yml\nskip_confirm: true\n```\n\n来跳过这个确认过程。\n\n如果你认为某些文件应该要被选择，但是大模型没有选择，你可以选择取消后，用 control + C 结束会话，然后重新修改 query, 在query中显示的提到这个文件，这样大模型就会选择这个文件。\n\n## 有哪些小技巧来弥补没有被过滤进来的文件\n\n可以将 index_filter_level 设置为 0, 然后主动在 query 中提到一些文件名，这样可以提高过滤的准确性。\n亦或者将 index_filter_level 设置为 1, 这样你可以在 query 中提到一些函数或者类，系统也能自动识别，你用起来也会更自然一些。\n\n此外，如果你明确知道要改的文件，你可以这么做：\n\n1. index_filter_level设置为0\n2. 在query 最后一行中添加如下语句： 如果需要筛选文件，请从提供的信息中只过滤出xxxx.xx文件\n", "tag": "", "tokens": 1323, "metadata": {}}], "modify_time": 1716937546.3234627, "md5": "ffdf5225abb776a59e1b9c360e18758d"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/019-AutoCoder对本地文档自动构建索引.md", "relative_path": "019-AutoCoder对本地文档自动构建索引.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/019-AutoCoder对本地文档自动构建索引.md", "source_code": "# 019-AutoCoder对本地文档自动构建索引\n\n> AutoCoder >= 0.1.36 特性\n\n请先\n\n```bash\npip install -U byzerllm\npip install -U auto-coder\n```\n\nAutoCoder已经支持对代码构建索引了，参考：\n\n1. [006-AutoCoder 开启索引，减少上下文](006-AutoCoder%20%E5%BC%80%E5%90%AF%E7%B4%A2%E5%BC%95%EF%BC%8C%E5%87%8F%E5%B0%91%E4%B8%8A%E4%B8%8B%E6%96%87.md)\n2. [017-AutoCoder指定专有索引模型](017-AutoCoder%E6%8C%87%E5%AE%9A%E4%B8%93%E6%9C%89%E7%B4%A2%E5%BC%95%E6%A8%A1%E5%9E%8B.md)\n3. [018-AutoCoder 索引过滤经验谈](018-AutoCoder%20索引过滤经验谈.md)\n\n而对文档的使用，我们目前还需要用户通过如下配置来使用：\n\n1. urls: 这里你可以指定本地路径的文档或者网络文档，多个文档可以用逗号分隔。\n2. search_engine/search_engine_token 来开启自动到网络寻找文档\n\n今天我们允许用户对本地文档进行索引来使用了。\n\n## 启动索引服务\n\n还记得我们是怎么启动模型服务的么？类似下面这种：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 30 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\n\n通过运行上面的命令，我们就能得到一个 deepseek_chat 模型。\n\n现在我们可以用相同的命令启动一个索引服务：\n\n```bash\nbyzerllm storage start\n```\n系统会在启动期间下载包括JDK21,以及一些库和文件，请确保网络畅通。\n\n## 对文档进行索引构建\n\n在构建之前，你需要先部署两个模型，一个是 deepseek_chat, 一个是qianwen_emb。 \n\nqianwen_chat:\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 30 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n\nqianwen_emb:\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=text-embedding-v2 \\\n--model qianwen_emb\n```\n\n现在，可以构建你的文档了：\n\n```bash\nauto-coder doc build --model deepseek_chat \\\n--emb_model qianwen_emb \\\n--source_dir 你存放文档的目录\n```\n\n你也可以把上面的参数放到一个 YAML 文件里去：\n\n```yaml\nsource_dir: /Users/allwefantasy/projects/deploy_models\ntarget_file: /Users/allwefantasy/projects/auto-coder/output.txt \n\nmodel: deepseek_chat\nmodel_max_length: 2000\nmodel_max_input_length: 30000\nanti_quota_limit: 0\n\nemb_model: qianwen_emb\n\nindex_filter_level: 0\n\nquery: |   \n   请只给出 qianwen_chat  的启动语句。   \n```\n\n然后这样运行：\n\n```bash\nauto-coder doc build --file actions/019_test_rag.yml \n```\n\n## 使用\n\n### 用来做问答助手\n\n```bash\nauto-coder doc query --model qianwen_chat --emb_model qianwen_emb --source_dir . \\\n--query \"请给出 qianwen_chat  的启动语句\"\n```\n或者把问题和参数写到文件里：\n\n```bash\nauto-coder doc query --file actions/019_test_rag.yml \n```\n\n下面是输出：\n\n```\n=============RESPONSE==================\n\n\n2024-04-17 14:01:50.287 | INFO     | autocoder.utils.llm_client_interceptors:token_counter_interceptor:16 - Input tokens count: 0, Generated tokens count: 0\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=qwen-max \\\n--model qianwen_chat\n\n=============CONTEXTS==================\n/Users/allwefantasy/projects/deploy_models/run.txt\n/Users/allwefantasy/projects/deploy_models/run.sh\n```\n会告诉你结果，以及是根据那些文件得到的结果。\n\n### 用来自动化执行代码\n\n如果是代码，你也可以直接让他执行这个代码：\n\n```bash\nauto-coder doc query --model qianwen_chat --emb_model qianwen_emb \\\n--source_dir . \\\n--query \"请给出 qianwen_chat  的启动语句\" \\\n--execute\n```\n\n下面是输出：\n\n```\n=============RESPONSE==================\n\n\n2024-04-17 13:22:33.788 | INFO     | autocoder.utils.llm_client_interceptors:token_counter_interceptor:16 - Input tokens count: 0, Generated tokens count: 0\nbyzerllm deploy --pretrained_model_type saas/qianwen \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 2 \\\n--infer_params saas.api_key=${MODEL_QIANWEN_TOKEN}  saas.model=qwen-max \\\n--model qianwen_chat\n\n=============CONTEXTS==================\n/Users/allwefantasy/projects/deploy_models/run.txt\n/Users/allwefantasy/projects/deploy_models/run.sh\n\n\n=============EXECUTE==================\n2024-04-17 13:22:45.916 | INFO     | autocoder.utils.llm_client_interceptors:token_counter_interceptor:16 - Input tokens count: 1184, Generated tokens count: 136\nsteps=[ExecuteStep(code='byzerllm deploy --pretrained_model_type saas/qianwen --cpus_per_worker 0.001 --gpus_per_worker 0 --num_workers 2 --infer_params saas.api_key=${MODEL_QIANWEN_TOKEN} saas.model=qwen-max --model qianwen_chat', lang='shell', total_steps=1, current_step=1, cwd=None, env=None, timeout=None, ignore_error=False)]\nShell Command:\nbyzerllm deploy --pretrained_model_type saas/qianwen --cpus_per_worker 0.001 --gpus_per_worker 0 --num_workers 2 --infer_params saas.api_key=${MODEL_QIANWEN_TOKEN} saas.model=qwen-max --model qianwen_chat\nOutput:\nCommand Line Arguments:\n\n--------------------------------------------------\n\ncommand             : deploy\n\nray_address         : auto\n\nnum_workers         : 2\n\ngpus_per_worker     : 0.0\n\ncpus_per_worker     : 0.001\n\nmodel_path          : \n\npretrained_model_type: saas/qianwen\n\nmodel               : qianwen_chat\n\ninfer_backend       : vllm\n\ninfer_params        : {'saas.api_key': '', 'saas.model': 'qwen-max'}\n\nfile                : None\n\n--------------------------------------------------\n\n模型 qianwen_chat 已经部署过了\n```\n\n可以看到他正确的运行了这个代码。\n\n### 用来提供辅助信息生成代码\n\ngenerate.yml:\n\n```yaml\nsource_dir: 你的项目路径\ntarget_file: 你的项目路径/output.txt \n\nmodel: deepseek_chat\n\nenable_rag_search: true\nemb_model: qianwen_emb\n\nindex_filter_level: 0\n\nexecute: true\nauto_merge: true\nhuman_as_model: true\n\nquery: |   \n   请根据 qianwen_chat 的启动代码，封装一个函数 start_qianwen_chat。\n```\n\n我们通过参数 `enable_rag_search` 控制是否启用 Rag 检索来获取一些额外信息给到大模型来生成代码。\n\n然后执行:\n\n```bash\nauto-coder --file generate.yml\n```\n\n## 提供OpenAI兼容接口\n\n```bash\nauto-coder doc serve --model deepseek_chat --emb_model qianwen_emb --source_dir . --port 8000\n```\n\n\n## 小实战\n\n新建一个目录,然后把 auto-coder 的中文文档和 byzerllm 的命令行示例脚本拷贝进去：\n\n```\nll /Users/allwefantasy/projects/doc_repo\ntotal 0\ndrwxr-xr-x    5 allwefantasy  staff    160  4 17 15:19 ./\ndrwxr-xr-x  346 allwefantasy  staff  11072  4 17 15:04 ../\ndrwxr-xr-x    3 allwefantasy  staff     96  4 17 15:07 .auto-coder/\ndrwxr-xr-x   24 allwefantasy  staff    768  4 17 15:18 auto_coder_doc/\ndrwxr-xr-x    5 allwefantasy  staff    160  4 17 15:19 deploy_models/\n```\n\n新建一个  build.yml:\n\n```yml\nsource_dir: /Users/allwefantasy/projects/doc_repo\nmodel: deepseek_chat\nemb_model: qianwen_emb\n```\n\n运行构建:\n\n```bash\nauto-coder doc build --file ./actions/build.yml\n```\n\n现在就可以就有个auto-coder助手了：\n\n```bash\nauto-coder doc query --file ./actions/build.yml --query \"如何设置过滤级别\"\n```\n\n输出：\n\n```\n=============RESPONSE==================\n\n\n2024-04-17 15:40:12.547 | INFO     | autocoder.utils.llm_client_interceptors:token_counter_interceptor:16 - Input tokens count: 0, Generated tokens count: 0\n如何设置过滤级别\n\n在使用AutoCoder进行索引过滤时，可以通过调整`index_filter_level`参数来设定过滤级别的高低。具体设置方法如下：\n\n1. **设置为0**：\n   若要实现较为宽松的过滤条件，允许在查询中直接提及文件名以提升过滤准确性，将`index_filter_level`设置为0。这样，您可以在查询语句中明确指出希望包含的特定文件名，系统将依据这些指示进行文件筛选。\n\n2. **设置为1**：\n   若期望获得更为自然的使用体验，允许在查询中通过提及函数或类名来进行文件过滤，将`index_filter_level`设置为1。在这种级别下，AutoCoder能够自动识别您在查询中提到的函数或类，并据此有效地筛选相关文件。\n\n综上所述，根据您的需求和工作场景，可通过将`index_filter_level`参数分别设置为0或1，来灵活调整AutoCoder的索引过滤级别，以达到期望的文件筛选效果。\n\n=============CONTEXTS==================\n/Users/allwefantasy/projects/doc_repo/auto_coder_doc/018-AutoCoder 索引过滤经验谈.md\n```\n\n其实最重要的是通过 CONTEXTS 快速定位相关文件。\n\n再试一个：\n\n```bash\nauto-coder doc query --file ./actions/build.yml --query \"如何对接ollama\"\n```\n\n输出：\n\n```\n对接Ollama的步骤如下：\n\n1. **模型部署**：\n   使用Byzer-LLM进行模型部署，指定Ollama相关的参数。具体命令如下：\n\n   ```shell\n   byzerllm deploy  --pretrained_model_type saas/official_openai \\\n   --cpus_per_worker 0.01 \\\n   --gpus_per_worker 0 \\\n   --num_workers 1 \\\n   --infer_params saas.api_key=xxxxx saas.model=llama2  saas.base_url=\"http://localhost:11434/v1/\" \\\n   --model ollama_llama2_chat\n   ```\n\n   参数说明：\n   - `--pretrained_model_type saas/official_openai`: 指定使用Ollama模型，遵循OpenAI协议。\n   - `--cpus_per_worker 0.01`: 分配给每个工作进程的CPU资源，由于使用已部署的Ollama，设为较小值即可。\n   - `--gpus_per_worker 0`: 不分配GPU资源，因使用已部署的Ollama。\n   - `--num_workers 1`: 设置并发数为1，此处为测试环境，可根据实际需求调整。\n   - `--infer_params saas.api_key=xxxxx saas.model=llama2 saas.base_url=\"http://localhost:11434/v1/\"`: 配置Ollama的相关参数：\n     - `saas.api_key`: 提供您的Ollama API密钥。\n     - `saas.model`: 指定使用的Ollama模型名称（例如：`llama2`）。\n     - `saas.base_url`: 设置Ollama服务的URL地址。\n   - `--model ollama_llama2_chat`: 自定义给部署模型命名，后续在AutoCoder中使用此名称。\n\n2. **模型测试**：\n   部署完成后，通过Byzer-LLM执行查询命令以测试对接的Ollama模型是否正常工作：\n\n   ```shell\n   byzerllm query --model ollama_llama2_chat --query 你好\n   ```\n\n   参数说明：\n   - `--model ollama_llama2_chat`: 使用上一步部署时定义的模型名称。\n   - `--query 你好`: 输入测试用的查询语句。\n\n   如果对接成功，命令行将输出模型的响应，表明Ollama已成功与Byzer-LLM及AutoCoder对接。\n\n综上所述，通过执行上述Byzer-LLM部署命令并进行模型测试，即可完成对Ollama的对接。后续在AutoCoder中直接使用模型名称`ollama_llama2_chat`即可无缝使用Ollama。\n\n=============CONTEXTS==================\n/Users/allwefantasy/projects/doc_repo/auto_coder_doc/命令行版Devin 来了: Auto-Coder.md\n/Users/allwefantasy/projects/doc_repo/auto_coder_doc/014-AutoCoder使用Ollama.md\n```\n\n\n\n\n\n\n", "tag": "", "tokens": 3270, "metadata": {}}], "modify_time": 1716937681.7747326, "md5": "b364561326e0392cc37767749b1161fe"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/032-AutoCoder_构建auto-coder文档问答助手.md", "relative_path": "032-AutoCoder_构建auto-coder文档问答助手.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/032-AutoCoder_构建auto-coder文档问答助手.md", "source_code": "# 032-AutoCoder_构建auto-coder文档问答助手\n\nAutoCoder 现在文档慢慢开始多起来了，自己查找很多细节其实挺麻烦的。用户可以自己利用AutoCoder构建一个文档小助手，这样可以\n方便自己使用AutoCoder。\n\n我们其实已经在 [019-AutoCoder对本地文档自动构建索引](019-AutoCoder对本地文档自动构建索引.md)中介绍了如何构建索引。\n\n## 启动Byzer Storge服务\n\n```bash\nbyzerllm storage start\n```\n\n## 启动emb/chat模型\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--worker_concurrency 10 \\\n--infer_params  saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=text-embedding-3-small \\\n--model gpt_emb\n\nbyzerllm deploy --pretrained_model_type saas/openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--worker_concurrency 30 \\\n--num_workers 1 \\\n--infer_params saas.base_url=\"https://api.deepseek.com/v1\" saas.api_key=${MODEL_DEEPSEEK_TOKEN} saas.model=deepseek-chat \\\n--model deepseek_chat\n```\n这里你可以自己随意组合，我比较喜欢上面两个模型。\n\n## 构建索引\n\n克隆auto-coder，然后对文档构建索引：\n\n```bash\ngit clone https://github.com/allwefantasy/auto-coder\ncd auto-coder\nauto-coder doc build --model deepseek_chat \\\n--emb_model gpt_emb \\\n--source_dir ./docs/zh \\\n--collection auto-coder \\\n--description \"AutoCoder文档\"\n```\n\n如果你文档有更新，那么再执行一次这条命令就可以了。\n\n## 命令行查询\n\n```bash\nauto-coder doc query --model deepseek_chat --emb_model gpt_emb  --collection auto-coder \\\n--query \"urls 参数怎么使用，有示例么？\"\n```\n\n下面是输出内容：\n\n```\n=============RESPONSE==================\n\n\n2024-05-19 22:06:09.241 | INFO     | autocoder.utils.llm_client_interceptors:token_counter_interceptor:16 - Input tokens count: 0, Generated tokens count: 0\n urls 参数用于配置文档的链接，可以是HTTP(S)链接、本地文件或目录。多个地址可以用逗号分隔。例如，如果你想让AutoCoder参考一个在线文档和一个本地文件，你可以这样配置：\n\n```\nurls: https://example.com/documentation, /path/to/local/file.txt\n```\n\n这样，AutoCoder将同时打开这两个文档供你参考。需要注意的是，urls的内容不会被索引，而是完整地显示在AutoCoder的窗口中，因此需要考虑文件大小以确保不会影响性能。\n\n=============CONTEXTS==================\n/Users/allwefantasy/projects/auto-coder/docs/zh/007-番外篇 AutoCoder里配置的model究竟用来干嘛.md\n```\n\n## 使用聊天软件查询\n\n```bash\nauto-coder doc serve --model deepseek_chat --emb_model gpt_emb  --collection auto-coder\n```\n会开启一个兼容 OpenAI 的API接口，默认端口8000,你可以在你的聊天软件里做下配置,然后就可以有一个 auto-coder 文档小助手了：\n\n![](../images/032-01.png)\n\n\n\n\n\n\n\n\n", "tag": "", "tokens": 768, "metadata": {}}], "modify_time": 1716128244.0362108, "md5": "50686b0e1ca8684c133d96c10e495bff"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/030-AutoCoder_复用YAML配置文件.md", "relative_path": "030-AutoCoder_复用YAML配置文件.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/030-AutoCoder_复用YAML配置文件.md", "source_code": "# 030-AutoCoder_如何复用YAML配置文件\n\n在开发一个需求的时候，我们可能会拆分成多个 yml 文件(实际上每个yml对应一次小迭代),大部分情况下，我们只会变换 query等少数\n几个参数。\n\nAutoCoder 在 YAML 配置文件中提供了 `include_file` 字段，允许你复用其他的 YAML 配置文件。\n\n比如我把一些公共配置放在了 `./actions/common/remote.yml` 文件总，\n然后我新建一个 `041_new_feature.yml` 文件,我可以在 `041_new_feature.yml` 文件中\n这么写：\n\n```yaml\ninclude_file: \n   - ./common/remote.yml\n\nquery: |   \n   AutoCoder 当检测到 include_file 参数（该参数用于指定需要 include 的文件路径，支持yml 数组格式），自动加载该参数，并且\n   优先合并到 args 里去。注意，可能存在递归场景，最大递归深度为 10。\n```\n\ninclude_file 支持数组格式，可以指定多个文件,AutoCoder 会按照数组顺序加载。\n\n\n\n", "tag": "", "tokens": 234, "metadata": {}}], "modify_time": 1716042309.581123, "md5": "e4fd08e1633cb52c247bfc06c6bcfb47"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/047_chat-auto-coder使用实践.md", "relative_path": "047_chat-auto-coder使用实践.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/047_chat-auto-coder使用实践.md", "source_code": "# 047_chat-auto-coder使用实践\n\nChat-Auto-Coder 是一个命令行聊天工具，可以让你以对话的方式与 AI 进行沟通，可以无需打开编辑器，就能完成代码的开发。\n\n> 本文使用 Claude Opus 作为代码生成模型。\n> 你可以通过 /conf code_model: opus_chat 来选择使用该模型。具体参看: [046_AutoCoder_Chat-Auto-Coder指南](./046_AutoCoder_Chat-Auto-Coder指南.md)\n> 最新的 chat-auto-coder 编写代码需要使用 /coding 指令，而不是 /chat 指令。默认不加任何指令则为 /chat 模式，进行闲聊模式。\n\n## 准备工作\n\n首先，进入你的项目，并且初始化\n\n```bash\nauto-coder init --source_dir .\n```\n\n如果你想使用索引功能，你需要先构建索引\n\n```bash\nauto-coder index --file ./actions/base/base.yml\n```\n\n现在可以进入 Chat-Auto-Coder 了。\n\n```bash\nchat-auto-coder\n```\n\n## 实践\n\n进去后，比如我想让他给我创建一个文件（当前我这个文档）：\n\n![](../images/047-01.png)\n\n点击回车后，AI 会自动创建一个该文档。\n\n这篇文章我们会演示如何使用 chat-auto-coder 给 chat-auto-coder 添加排除目录的功能。\n\n### 起手：添加你想修改的文件\n\n我需要修改的文件是 chat_auto_coder.py，我通过 `/add_files` 指令进行添加，系统会自动补全命令和文件。\n\n![](../images/047-02.png)\n\n如果你不知道你的需求会修改哪些文件，可以用下列指令来找文件：\n\n```bash\n/index/query 查找所有调用了 request 库的文件\n```\n\n或者\n\n```bash\n/index/query 项目中命令行入口有哪些文件？\n```\n\n给出列表后，你接着可以用 `/add_files` 来添加你想修改的文件或者需要 chat-auto-coder 关注的文件。\n\n注意该功能需要你先构建索引。\n\n### 查看当前有多少活动文件\n\n我通过 `/list_files` 指令查看当前有多少活动文件。\n\n![](../images/047-03.png)\n\n可以看到，我们成功的添加了 chat_auto_coder.py 文件。\n\n### 对代码进行修改\n\n我通过 `/chat` 指令对 chat_auto_coder.py 进行修改,我现在想新增一个 `/exclude_dirs` 指令，用于排除目录。\n\n下面是我的需求描述：\n\n```\n新增/exclude_dirs 指令, 逻辑和/add_files 指令一样,\n只是保存到 memory 的exclude_dirs 字段下,和 current_files 同级别\n```\n\n![](../images/047-04.png)\n\n点击回车后，AI 会自动帮你修改代码。在终端你可以看到提交后diff 的内容：\n\n![](../images/047-05.png)\n\n此外，你还可以点击 vscode 右侧的 source_control 查看提交：\n\n![](../images/047-06.png)\n\n如果你对这次修改不满意，可以使用 \n\n```bash\n/revert\n```    \n进行回滚。\n\nreview 完代码后，我们发现，这次修改没有修改 help() 方法：\n\n![](../images/047-07.png)\n\n可以看到，没有 `/exclude_dirs` 指令的介绍和描述。没关系，我们现在加：\n\n![](../images/047-08.png)\n\n然后看下执行结果：\n\n![](../images/047-09.png)\n\n代码改好了，我们验证下：\n\n![](../images/047-10.png)\n\n可以看到有了。\n\n接着我们测试下功能：\n\n![](../images/047-11.png)\n\n可以过滤出图片。\n\n然后我们排除images目录：\n\n![](../images/047-12.png)\n\n\n接着可以看到， images 目录已经不会被提示了：\n\n![](../images/047-13.png)\n\n\n## 总结\n\n通过这次实践，我们学会了如何使用 chat-auto-coder 给 chat-auto-coder 添加排除目录的功能。整个过程以对话形式完成，非常方便，真正实现了\n在谈笑间完成代码的开发。\n\n\n\n\n", "tag": "", "tokens": 824, "metadata": {}}], "modify_time": 1718937617.8962588, "md5": "2fa93b24aa863f16276fc95ed84670d1"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/039-AutoCoder_前端开发实战_1.md", "relative_path": "039-AutoCoder_前端开发实战_1.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/039-AutoCoder_前端开发实战_1.md", "source_code": "# 039-AutoCoder_前端开发实战_1\n\n> 本篇承诺，\n> 1. 除了环境准备部分，所有代码全部由 auto-coder 生成,没有做任何人工修改。\n> 2. 所有步骤都是通过 auto-coder 一次性完成的，没有任何反复调整描述语句。(所以大家要勤学苦练。。。。)\n\n当然，因为示例比较简单，所以很容易达成上面的目标，实际项目中，我们还是鼓励大家多练习，多调整描述语句，多尝试不同模型。\n\n本实践提供了完整源码：\n\n> https://github.com/allwefantasy/auto-coder.example_01\n\n在你的笔记本上准备 auto-coder,可以参考[这里](./000-AutoCoder_准备旅程.md)\n\n本实践展示 auto-coder 的前后端协同开发，支持：\n\n1. Figma设计或者网页截图 到前端代码的转换\n2. 前端代码和后端代码的对接\n\n![](../images/039-01.png)\n\n## 前言\n\n本文包含 auto-coder演示，在文章最后也体积了在 chat-auto-coder 中如何完成相同功能。\n\n## 准备一个项目/环境\n\n```bash\nmkdir auto-coder.example_01\ncd auto-coder.example_01\n\nnpx create-react-app frontend --template typescript\ncd frontend\nrm -rf .git\nnpm install -D tailwindcss postcss autoprefixer\nnpx tailwindcss init -p\nnpm install axios\nnpm install react-router-dom@5\nnpm install --save-dev @types/react-router-dom@5\n\ncd ..\nauto-coder init --source_dir .\n```\n\ntailwindcss 需要配置一下，修改 `frontend/src/index.css`,在最前面添加:\n\n```css\n@import 'tailwindcss/base';\n@import 'tailwindcss/components';\n@import 'tailwindcss/utilities';\n```\n\n然后修改`tailwind.config.js`:\n\n```javascript\n/** @type {import('tailwindcss').Config} */\nmodule.exports = {  \n  content: ['./src/**/*.{js,jsx,ts,tsx}', './public/index.html'],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\n```\n\n我也没搞明白为啥tailwindcss不直接在命令里就做掉。\n\n\n现在可以用你喜欢的IDE打开，推荐 vscode.\n\n## 先开发一个 fastapi 后端\n\n复制 `000_examle.yml` 改名为 `001_添加server.yml`，内容如下；\n\n\n```yaml\ninclude_file:\n  - ./base/base.yml\n  - ./base/enable_index.yml\n  - ./base/enable_wholefile.yml    \n\nquery: |  \n    使用fastapi 在 src/server.py 提供\n    1. 一个 /api/add_item 接口。 该接口接受一个json对象，包含一个name字段一个content字段。\n    2. 一个 /api/get_items 接口。 该接口返回一个json对象，包含所有的name字段和content字段。\n    使用一个全局变量来存储这些数据。\n    \n    注意要移除 fastapi CORS 限制。\n```\n\n执行命令:\n\n```bash\nauto-coder --file actions/001_添加server.yml \n```\n\n默认开启了 human_as_model, 这样你就可以复制这个文件里的内容（注意是完整的文本内容，不是代码）到你的命令行里去：\n\n[code_model为Opus模型的输出](./039_code_model_output.txt)\n\n现在，你已经可以得到一个满足你需求的完整的后端代码了：\n\n![server.py](../images/039-03.png)\n\n\n按如下方式启动你的服务：\n\n```bash\nuvicorn src.server:app --reload\n```\n\n## 使用 Figma 设计图或者 网页截图 生成前端代码以及交互\n\n下面是我在figma中设计的一个图片：\n\n![](../images/039-02.png)\n\n这个环节我们需要额外新增一个多模态模型，这里用 GPT4o 模型，启动方式为：\n\n```bash\nbyzerllm deploy --pretrained_model_type saas/official_openai \\\n--cpus_per_worker 0.001 \\\n--gpus_per_worker 0 \\\n--num_workers 1 \\\n--infer_params saas.api_key=${MODEL_OPENAI_TOKEN} saas.model=gpt-4o \\\n--model gpt4o_chat\n```\n\n`gpt4o_chat` 这个名字我们待会会配置到YAML文件中。\n\n搞好了上面这一步之后，就可以在 actions 目录下创建一个 `002_创建web页面.yml`了：\n\n```yaml\ninclude_file:\n  - ./base/base.yml\n  - ./base/enable_index.yml\n  - ./base/enable_wholefile.yml    \n\nproject_type: ts\nenable_multi_round_generate: false\n\nimage_file: /Users/allwefantasy/projects/auto-coder.example_01/screens/add_example.png\nimage_mode: direct\nimage_max_iter: 1\nvl_model: gpt4o_chat\n\nurls: /Users/allwefantasy/projects/auto-coder.example_01/src/server.py\n\nquery: |\n  参考 server.py 中关于解读示例的接口 /api/add_item,\n  使用reactjs + typescript + tailwindcss ，将 html 转换为一张 reactjs 页面,名字为 AddExample.tsx，  \n  放在froontend/src/pages目录下。\n  注意，要严格确保 html 的页面完全一致。\n  与此同时，你需要完成和server.py中的接口对接。\n\n  此外，还要创建一个新的页面，名字为 ListExamples.tsx，对接的接口为/api/get_items，用来展示所有的items。\n  只输出有修改或者新增的文件。\n```  \n\n有几个地方值得注意：\n\n1. `image_file` 指定了我们的设计图，这个设计图是我们在 figma 中设计的。\n2. `vl_model` 指定了我们的多模态模型。\n3. `urls` 指定了我们的后端代码的接口部分实现\n4. project_type 指定了我们的项目类型,因为是前端，所以是 ts\n\n运行yaml文件：\n\n```bash\nauto-coder --file actions/002_创建web页面.yml\n```\n\n中间会有确认环节：\n\n![](../images/039-04.png)\n\n你直接点选 OK 就行。\n\n然后你就可以看到生成的结果了：\n\n![](../images/039-05.png)\n\n你可以在 [这里](./039_code_model_output_002.txt) 查看我使用Opus生成的代码。\n\n现在我们看不到页面，因为我们还没把新的页面加到`frontend/src/App.tsx`。\n\n我们新增一个 `003_设置首页.yml` 文件：\n\n```yaml\ninclude_file:\n  - ./base/base.yml\n  - ./base/enable_index.yml\n  - ./base/enable_wholefile.yml    \n\nproject_type: ts\nenable_multi_round_generate: false\n\nquery: |\n  AddExample.tsx,ListExamples.tsx 添加到 App.tsx 的路由中，默认展示 AddExample.tsx 页面。\n  同时修改 AddExample.tsx 页面，当成功时，跳转到 ListExamples.tsx 页面。\n\n```\n\n运行：\n\n```bash\nauto-coder --file actions/003_设置首页.yml\n```\n\n你依然可以在:\n\n[](./039_code_model_output_003.txt)\n\n现在可以去看页面：\n\n```bash\nnpm run start\n```\n\n备注：\n> 其实被 react-router-dom 的版本坑了一下，因为并不知道 v5和v6不兼容，所以在做例子的时候，为了方便，我直接选择了v5。\n> 如果你要选择v6,你需要在前面YAML需求里明确说下 react-router-dom 的版本，类似，请使用 react-router-dom v6 版本的API。\n\n可以看到页面：\n\n![](../images/039-06.png)\n\n\n因为我们后端是8000端口，前端是3000端口，所以你需要在前端的`frontend/package.json`里加入：\n\n```json\n\"proxy\": \"http://localhost:8000\",\n```\n\n现在你可以点击提交了：\n\n![](../images/039-07.png)\n\n提交后的结果：\n\n![](../images/039-08.png)\n\n可以看到，第一张页面整体遵循了我们的figma设计，相对来说好看很多，\n第二张页面因为我们提供设计，大模型就特别偷懒，搞了一个丑丑的页面。\n\n## 额外福利\n\n你可以通过 auto-coder 一条命令就获得任意网页的完整截图：\n\n```bash\nauto-coder screenshot --urls https://www.baidu.com  --output /tmp/jack3/\n```\n\n## 总结\n\n在本篇实践中，有几个特点：\n\n1. 通过 auto-coder 可以一次性完成前端页面生成，并且同时和后端对接。\n2. 我们通过拆解合适的粒度，让单次auto-coder 交互更加简单。\n3. 如果是已经存在的项目，你可以通过让他参考随意一张页面从而自动获得所使用的技术栈和页面整体风格，组件。\n\n\n## Chat-Auto-Coder 中如何实现\n\n在 chat-auto-coder 中,你可以通过 `/conf` 指令来设置图片：\n\n```bash\n/conf image_file:/Users/allwefantasy/projects/auto-coder.example_01/screens/add_example.png\n```\n\n然后再手动将 project_type 设置为 ts, vl_model 设置为 gpt4o_chat:\n\n```bash\n/conf project_type:ts\n/conf vl_model:gpt4o_chat\n```\n\n接着把后端接口添加到活动文件里去：\n\n```bash\n/add_files /Users/allwefantasy/projects/auto-coder.example_01/src/server.py\n```\n\n现在可以直接用 `/coding` 开始做生成了：\n\n```bash\n/coding 参考 server.py 中关于解读示例的接口 /api/add_item,\n  使用reactjs + typescript + tailwindcss ，将 html 转换为一张 reactjs 页面,名字为 AddExample.tsx，  \n  放在froontend/src/pages目录下。\n  注意，要严格确保 html 的页面完全一致。\n  与此同时，你需要完成和server.py中的接口对接。\n\n  此外，还要创建一个新的页面，名字为 ListExamples.tsx，对接的接口为/api/get_items，用来展示所有的items。\n  只输出有修改或者新增的文件。\n```\n\n这样就可以完成和前面一样的效果了。\n\n记得如果你不需要再转换图片到代码，需要移除图片和重置project_type设置：\n\n```bash\n/conf /drop image_file;\n/conf project_type:py\n```\n\n\n\n\n\n\n\n  \n\n\n", "tag": "", "tokens": 2183, "metadata": {}}], "modify_time": 1719103769.7466633, "md5": "96ea03352d5904df6a027479475af60b"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/009-AutoCoder如何阅读第三方库源码.md", "relative_path": "009-AutoCoder如何阅读第三方库源码.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/009-AutoCoder如何阅读第三方库源码.md", "source_code": "# 009-AutoCoder如何阅读第三方库源码\n\n第三方库对程序员而言，是日常工作的重要部分，通常程序员的工作顺序如下：\n\n1. 阅读已有代码\n2. 阅读第三方库或者对接接口的文档\n3. 使用搜索引擎获得其他人使用第三方库的文档\n4. 自己阅读第三方库源码\n\n一般而言，1,2,3 应该是可以满足需求了的。而且，如果你的第三库是一个比较成熟的库，那么大模型\n对其知识应该也是足够的。通常我们并不需要 AutoCoder 去阅读第三方库的源码。\n\n但是有些库比较新，或者老库的新版本，亦或者你要做更深入的理解和使用，往往就确实阅读其中的源码。\n那么如何让 AutoCoder 阅读第三房库的源码呢？\n\n目前而言有三种方式：\n\n## 在  source_dir 中建立第三方库的软链接\n\n我们知道，在 AutoCoder中，  source_dir 是为了配置我们开发的项目目录的。我们可以在这个项目里创建一个 \n比如叫 pkg 的目录，在这个目录里，链接我们需要的第三方库的源码（或者第三方库部分目录）。这么做的好处是，你可以很好的控制需要 AutoCoder 参考的第三方库源码，同时还有个好处是，这些源码也会被索引（如果你开启了索引的）话。\n\n## 通过 urls 参数来控制\n\nurls 参数本意是让用户配置文档的，但实际上除了可以配置 http(s) 链接以外，也可以配置本地文件或者目录，多个地址可以使用逗号来分割。你可以将指定的第三方库源码文件或者目录配置到这里。\n注意，urls 的内容并不会被索引，会被完整的放到 AutoCoder 的窗口中，所以需要考虑大小问题。\n\n## 通过 py_packages 参数控制\n\n这个是专门正对 Python 项目提供的一个参数，可以指定第三方库的包名，AutoCoder 会自动去找这个包的源码。这个参数是最简单的，但是也是最不灵活的，因为你只能指定包名，而不能指定具体的文件或者目录。\n\n```yml\nsource_dir: /home/winubuntu/projects/ByzerRawCopilot \ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\npy_packages: openai\n\nquery: |\n    阅读 openai 源码，并且在 src/clients/写一个连接到 openai 的演示代码,文件名为 openai_demo.py\n```\n\n不过实际上第三方库往往都有巨大的代码量，而且往往我们只需要其中的一部分。所以建议还是使用第一种方式，通过软链接的方式来控制。\n\n", "tag": "", "tokens": 535, "metadata": {}}], "modify_time": 1711809217.9064243, "md5": "a4186325aa27c807965cb5a9acf7b2b9"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/008-AutoCoder 如何支持各种语言的项目.md", "relative_path": "008-AutoCoder 如何支持各种语言的项目.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/008-AutoCoder 如何支持各种语言的项目.md", "source_code": "# 008-如何支持各种语言的项目\n\n我们之前看到， AutoCoder 最简化的配置是这样的：\n\n\n```yml\n\nsource_dir: /tmp/t-py\ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nquery: >\n  修改 server.py ，在代码 app = FastAPI()后\n  增加 ray 的初始化连接代码。\n```\n\n默认他会只处理 Python 项目。其实显示的配置项是 `project_type`，这个参数可以让 AutoCoder 支持更多的项目类型。上面的配置\n等价于：\n\n```yml\nsource_dir: /tmp/t-py\ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nproject_type: py\n\nquery: >\n  修改 server.py ，在代码 app = FastAPI()后\n  增加 ray 的初始化连接代码。\n\n```\n\n默认我们提供了两种类型：\n\n1.py\n2.ts\n\n那其他类型的项目怎么办？\n\n我们支持后缀模式。\n\n比如如果我要支持Java, 你可以按如下方式配置：\n\n```yml\nsource_dir: /tmp/JAVA_PROJECT\ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nproject_type: .java\n\nquery: >\n  ....\n\n```\n\n如果你是个混合项目，比如同时有 Java, Scala, 那么可以这么配置：\n\n```yml\nsource_dir: /tmp/JAVA_PROJECT\ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nproject_type: .java,.scala\n\nquery: >\n  ....\n\n```\n\n这样， AutoCoder 就会关注项目里的 .java, .scala 结尾的文件。当你开启了索引，也只会对\n这些文件构建索引。\n\n## 额外的过滤\n\n有的时候你肯能希望实现更精细的项目文件过滤，比如要排除掉某个目录，这个时候可以参考 [031-AutoCoder_正则表达式排除目录](./031-AutoCoder_正则表达式排除目录.md)\n\n整体而言就是当你用后缀名来定义项目类型的时候，你可以通过参数 `exclude_files`\n来排除一些目录，可以使用正则或者文字描述（系统会自动转换成正则）。\n\n```yaml\nexclude_files:      \n   - human://任何包含 common 的目录\n```\n\nexclude_files 支持两种模式的配置:\n\n1. human:// 开头的字符串，AutoCoder 会根据后面的文本，自动生成正则表达式。比如上面的例子，会自动生成 `.*common.*` 的正则表达式。\n2. regex:// 开头的字符串，AutoCoder 会直接使用后面的文本作为正则表达式。比如 `regex://.*common.*`。", "tag": "", "tokens": 555, "metadata": {}}], "modify_time": 1716960731.2519758, "md5": "8b6490bbda01dc1fd40a4184fd1a4569"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/034-AutoCoder_word_pdf转换成html.md", "relative_path": "034-AutoCoder_word_pdf转换成html.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/034-AutoCoder_word_pdf转换成html.md", "source_code": "# 034-AutoCoder_word_pdf转换成html\n\n目前Word,PDF 转换工具都会把一些布局信息丢失，比如表格等。AutoCoder 提供了一个工具，可以利用多模态大模型，把Word,PDF 转换成HTML，这样就可以保留较为完整的布局信息，并且可以轻松转换成任意其他文本格式，比如markdown 等。\n\n> 注意，该工具基于多模态大模型，所以会有识别准确率问题。\n\n使用：\n\n```bash\nauto-coder doc2html --model deepseek_chat \\\n--vl_model gpt4o_chat \\\n--urls /Users/allwefantasy/Downloads/xxxxx.docx \\\n--output /tmp/jack2\n```\n下面是一个带表格的 Word:\n![](../images/034-01.png)\n\n下面是转换后的HTML效果：\n\n![](../images/034-02.png)\n\n你也可以把参数都放到配置文件中，然后直接执行：\n\n```bash\nauto-coder doc2html --file xxx.yml\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "tag": "", "tokens": 214, "metadata": {}}], "modify_time": 1716465295.9031568, "md5": "2865a9c8cc92e802eb5e2a5befcd9f10"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/041_AutoCoder_快速生成下一个需求YAML文件.md", "relative_path": "041_AutoCoder_快速生成下一个需求YAML文件.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/041_AutoCoder_快速生成下一个需求YAML文件.md", "source_code": "## 041_AutoCoder_快速生成下一个需求YAML文件.md\n\n在AutoCoder项目中，我们使用以`序号_名称.yml`格式命名的YAML文件来定义具体的需求。这些文件存放在`actions`目录下。当我们想基于之前的需求快速生成一个新的需求文件时，可以使用`auto-coder next`命令。\n\n### 解决的问题\n\n在开发过程中，我们经常会有一些新的需求是基于之前的需求演进而来的。手动创建一个新的YAML文件，然后从之前的文件中复制内容过来，修改，这个过程比较繁琐。`auto-coder next`命令就是为了解决这个问题，提供一个快速生成新需求文件的方式。\n\n### 如何解决\n\n`auto-coder next`命令的实现逻辑如下：\n\n1. 检查当前目录下是否有`actions`目录，如果没有则报错退出。\n\n2. 扫描`actions`目录下所有以`序号_名称.yml`格式命名的文件，找到序号最大的文件。\n\n3. 将上一步找到的序号最大的文件的内容，复制到一个以`新序号_新名称.yml`命名的新文件中。新序号是在最大序号上加1。新名称由命令行参数指定。\n\n4. 如果`actions`目录下还没有任何需求文件，则创建一个空的新文件。\n\n这样，我们就得到了一个基于之前需求的新的需求文件，然后就可以在新文件的基础上进行修改了。\n\n### 如何使用\n\n使用方法非常简单，只需要在AutoCoder项目的根目录下执行以下命令：\n\n```sh\nauto-coder next 新需求名称\n```\n\n其中，`新需求名称`就是你想给新生成的需求文件起的名字。\n\n比如，我们可以执行：\n\n```sh\nauto-coder next 支持图像生成\n```\n\n如果`actions`目录下最后一个需求文件是`023_增加摘要功能.yml`，那么上面的命令就会生成一个新的文件`024_支持图像生成.yml`，它的内容和`023_增加摘要功能.yml`一样。\n\n然后你就可以打开`024_支持图像生成.yml`，修改里面的内容，开始新需求的开发了。\n\n这个小功能虽然很简单，但是可以为我们节省不少重复的工作。给AutoCoder点个赞👍！", "tag": "", "tokens": 447, "metadata": {}}], "modify_time": 1717395436.019594, "md5": "6b292bccc10745721b439c8265d00f88"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/045_AutoCoder_VSCode插件篇.md", "relative_path": "045_AutoCoder_VSCode插件篇.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/045_AutoCoder_VSCode插件篇.md", "source_code": "# 045_AutoCoder_VSCode插件篇\n\nauto-coder 提供了一个 vscode的插件： [auto-coder-copilot](https://marketplace.visualstudio.com/items?itemName=allwefantasy.auto-coder-copilot)，帮助用户更方便的使用 auto-coder。\n\n## 初始化项目\n\n一旦你安装完插件后，你可以使用 vscode 打开一个项目，然后右键单击 Explorer(项目目录栏) 里任何一个位置，\n会弹出两个新选项：\n\n![](../images/045-01.png)\n\n如果这个项目还没有使用 auto-coder 初始化过，那么可以选择 `auto-coder: 初始化项目` 这个菜单。此时会vscode自动帮你打开一个terminal, 然后运行 \nauto-coder 初始化命令。\n\n执行过程中，你很可能在终端看到这样的输出：\n\n![](../images/045-02.png)\n\n提示没有 auto-coder 命令，这个时候你需要用conda 或者pyenv 激活安装有 auto-coder 的环境。\n\n然后再次重复上面的操作即可。\n\n\n## 创建需求\n\n如果你的项目已经初始化过，那么你可以选择 `auto-coder: 创建需求` 这个菜单，如果你的项目没有初始化过，右侧会弹出一个警告框：\n\n![](../images/045-03.png)\n\n你可以点击，立刻初始化，然后初始化完毕后，继续在 Explorer 里右键单击，选择 `auto-coder: 创建需求`,\n\n此时上边会弹出一个输入框，你可以输入需求内容，然后回车，通常还会向你询问一个向量模型，你需要提前准备一个向量模型，然后回车：\n\n![](../images/045-04.png)\n\n为了避免每次输入向量模型名称，你可以在 actions/base/base.yml 里配置一个默认的向量模型：\n\n```yaml\nemb_model: \"your_model_name\"\n```\n\n此时在你的terminal 里会看到输出：\n\n![](../images/045-05.png)\n\n与此同时，自动给你创建好了对应的yaml文件。\n\n## 执行需求\n\n对于创建好了的需求（yaml文件）如果你觉得没啥要更改的了，你可以选择 `auto-coder: 执行` 这个菜单，然后进行执行：\n\n![](../images/045-06.png)\n\n系统也会自动在terminal 里执行对应的命令。\n\n## 创建YAML文件\n\n`创建需求` 菜单是通过agent来完成的，会比较慢以及可能会失败，你可以直接手动创建yaml 文件。\n\n![045-07](../images/045-07.png)\n\n弹出一个需求输入框，实际上会作为文件名称的一部分，然后回车，此时会自动创建一个yaml文件，并且自动打开，你可以修改里面的query。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "tag": "", "tokens": 552, "metadata": {}}], "modify_time": 1718271663.1311164, "md5": "3ca026aaae48d361014dd4ee081f77f7"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/035-AutoCoder_auto_merge详解.md", "relative_path": "035-AutoCoder_auto_merge详解.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/035-AutoCoder_auto_merge详解.md", "source_code": "# 035-AutoCoder_auto_merge详解\n\n通过 auto_merge 用户相当于可以控制三个东西：\n\n1. 是不是要自动合并到现有源码中\n2. 生成/修改代码的格式\n3. 生成修改代码合并的方式\n\nauto_merge 接受两种类型的参数：\n\n1. bool 类型.\n2. 字符串类型. 可选值为 wholefile,diff,strict_diff,editblock\n\n如果设置为 true, 那么等价于 wholefile。\n\n我们详细说明下这几种模式。\n\n## wholefile\n\n这种模式下，AutoCoder 会要求大模型生成修改后的完整代码，合并的时候会把生成的代码直接替换到源码中。\n这种模式对于新文件以及有大量修改的文件非常适用。缺点当修改很小时也需要消费大量Token,并且大部分模型都很难做到\n几乎原模原样输出修改后的完整代码。\n\n## editblock\n\n这种模式下，AutoCoder 会让大模型生成 git 里的 \"合并冲突标记\" 格式，比如：\n\n```tsx\n##File: /Users/allwefantasy/projects/auto-coder.example_01/frontend/src/pages/ListExamples.tsx\n<<<<<<< SEARCH\nimport React, { useEffect, useState } from 'react';\nimport axios from 'axios';\n=======\nimport React, { useEffect, useState } from 'react';\nimport axios from 'axios';\nimport { useHistory } from 'react-router-dom';\n>>>>>>> REPLACE\n```\n\n该模式优点:\n\n1. 可以减少token消耗\n2. 生成格式复杂度也不高，较多大模型能够支持\n\n缺点:\n1. 因为是根据 SEARCH 不分进行替换，有概率一个文件中有多个 SEARCH，导致合并错误\n2. 串行执行，当一个文件有多个 SEARCH 时，会串行自行，期间也可能导致 1 的问题。\n\n## diff\n\n这是一种宽松的diff模式，AutoCoder 会要求大模型直接生成 diff 格式，但是该 diff 格式不要求大模型生成行号，只需要生成 @@ *** @@ 这种格式即可。\n然后通过字符匹配的方式找到修改的点，进行合并。这种模式对模型要求较低，并且大部分情况下都能很好的工作，但是对于一些特殊情况，可能会出现合并错误。\n\ndiff 最大的价值在于，可以减少几百倍的token生成量，并且极大的提升了生成速度。未来边写文字，就可以边看到修改后的效果很快可以来临。\n\n缺点和 editblock 类似。\n\n## strict_diff\n\n这是一种严格的diff模式，AutoCoder 会将源文件带行号发送给大模型，并要求大模型直接生成 unified diff 格式该 diff 格式需要生成@@ -7,10 +7,10 @@ 这种的行号，然后使用 patch 工具进行进行合并。这种模式对模型要求最高，但只要模型生成的diff 是正确的，则一定能够正确合并。\n\n缺点是对大模型要求较高，大模型很难正确生成这种格式的diff。\n\n## 使用经验\n\n目前推荐使用 wholefile 和 editblock 两种模式。editblock 模式兼具 diff 优势又相对来说易于阅读。\n需要程序员有个预判，如果会有较多文件修改，但每个文件修改不大，那么使用 editblock 模式。如果修改较大，那么使用 wholefile 模式。\n\n因为不同的模型支持能力不一致，如果你发现 editblock 经过多次修改描述后依然合并错误，再使用 auto-coder revert 进行回滚，\n可以再次尝试 diff 模式。\n\n## 搭配参数\n\nenable_multi_round_generate 与 auto_merge 搭配使用，可以实现按文件多轮生成（每一次只生成一个文件），最后再做统一合并的效果，但对模型的要求也更高。一般情况下，建议如下组合：\n\n1. diff/editblock + enable_multi_round_generate=false\n2. wholefile + enable_multi_round_generate=true\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "tag": "", "tokens": 807, "metadata": {}}], "modify_time": 1717338993.3557625, "md5": "6098faa6c2d6e56b39bde9e1f5846f53"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/010-AutoCoder 如何在公司级别使用.md", "relative_path": "010-AutoCoder 如何在公司级别使用.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/010-AutoCoder 如何在公司级别使用.md", "source_code": "# 010-AutoCoder 如何在公司级别使用\n\nAutoCoder 如果是程序员用的话，一般 AutoCoder + SaaS 大模型，都会运行在程序员自己的电脑上。程序员还可以通过 human as model 模式，使用 Web 版本的模型。\n\n但是这种方式存在一定泄露源码的风险。\n\n如果公司想给自家的程序员统一使用 AutoCoder，并且对代码隐私有较高的要求，那么我们可以考虑私有化部署 AutoCoder 使用的大模型。\n\n![](../images/client-server.png)\n\n## 私有化部署大模型\n\nByzer-LLM 可以快速的帮助企业部署生产可用的大模型集群，支持主流的SaaS，开源大模型，并且支持多模态类的大模型。具体参考 https://github.com/allwefantasy/byzer-llm。\n\n第一步是把公司的服务器，使用 Ray 组成一个集群,可以参考 https://docs.ray.io/en/latest/。 Ray 可以运行在裸机上，也可以\n运行云上，还可以运行在 k8s 上。这个具体看公司的IT基础设施要求。\n\n接着就可以执行下面命令在 Ray 上启动一个大模型：\n\n```\nbyzerllm deploy --model_path /home/byzerllm/models/openbuddy-llama2-13b64k-v15 \\\n--pretrained_model_type custom/auto \\\n--gpu_gpus_per_worker 4 \\\n--num_workers 1 \\\n--model llama2_chat \n```\n\n然后在 Ray 的dashboard上，可以看到启动的大模型:\n\n![](../images/image12.png)\n\n上面的命令表示，我们使用了一个自定义的模型，模型的路径是 `/home/byzerllm/models/openbuddy-llama2-13b64k-v15`，这个模型是一个 13B 的模型，使用了 4 个 GPU，启动了一个 worker。如果你设置了两个worker,那就相当于使用8块卡。\n\n注意，如果想让AutoCoder 效果好，开源的，推荐至少 72B Qwen 模型，并且需要长窗口版本。\n\n值得一提的事，我们提供了快速在连显卡驱动都没有的服务器上进行环境配置的工具： https://github.com/allwefantasy/byzer-llm?tab=readme-ov-file#raw-machine\n\n实际上，Byzer-LLM 对私有大模型和SaaS模型都一视同仁，都会在Ray 集群启动Worker,区别是 SaaS模型不消耗GPU， 而在连接 SaaS模型也通过 Byzer-LLM的一个重要原因是可以具有更好的可控性，比如审计，同时也可以更好的控制模型的版本，比如需要切换一个模型，只需要服务器做修改即可，而不需要客户端做修改。\n\n## AutoCoder 客户端配置\n\n一旦服务器部署完成后，程序在自己电脑上安装好AutoCoder 客户端后，就可以连接 Byzer-LLM 集群，具体有两种方式：\n\n第一种方式，是按如下方式启动一个客户端代理：\n\n```shell\nray start --address='公司Byzer-LLM集群地址' --num-cpus=0 --num-gpus=0\n```                    \n其中公司Byzer-LLM集群地址地址格式应该是类似这样的类似： x.x.x.x:6379  \n\n然后你就可以正常使用 AutoCoder 了,比如：\n\n```yml\nsource_dir: /home/winubuntu/projects/ByzerRawCopilot \ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nmodel: qianwen_short_chat\nmodel_max_length: 2000\nanti_quota_limit: 5\nskip_build_index: true\n\nproject_type: \"copilot/.py\"\nquery: |\n  优化 copilot 里的 get_suffix_from_project_type 函数并更新原文件\n\n```\n\n这个时候，AutoCoder 会自动连接 Byzer-LLM 集群中部署好的 `qianwen_short_chat` 来进行驱动。\n\n第二种方式，是通过配置文件的方式来指定公司的 Byzer-LLM 集群地址：\n\n```yml\nsource_dir: /home/winubuntu/projects/ByzerRawCopilot \ntarget_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt \n\nray_address: 公司Byzer-LLM集群地址\n\nmodel: qianwen_short_chat\nmodel_max_length: 2000\nanti_quota_limit: 5\nskip_build_index: true\n\nproject_type: \"copilot/.py\"\nquery: |\n  优化 copilot 里的 get_suffix_from_project_type 函数并更新原文件\n```       \n\n此时公司Byzer-LLM集群地址为类似： ray://x.x.x.x:10001\n用户可以根据自己的需求选择其中一种方式。\n\n## 总结\n\nAutoCoder 通过私有化部署大模型，可以满足公司对代码隐私的要求，同时也可以让程序员使用到最新的大模型。", "tag": "", "tokens": 1006, "metadata": {}}], "modify_time": 1716023953.9009407, "md5": "d75f03bdf4a1f3412e7fda75a3a481e4"}
{"file_path": "/Users/allwefantasy/projects/auto-coder/docs/zh/011-AutoCoder最佳实践之组合大模型API,Web订阅 copy.md", "relative_path": "011-AutoCoder最佳实践之组合大模型API,Web订阅 copy.md", "content": [{"module_name": "##File: /Users/allwefantasy/projects/auto-coder/docs/zh/011-AutoCoder最佳实践之组合大模型API,Web订阅 copy.md", "source_code": "# 011-AutoCoder最佳实践之组合大模型API/Web订阅\n\n前面一篇文章，我们介绍了如何在公司级别使用AutoCoder，架构是这样的：\n\n![](../images/client-server.png)\n\n\n而作为研发同学，实际上相当于把 大模型 Server 也放在自己的笔记本上。但受限于笔记本的性能，\n难以解决窗口（上下文）长度以及模型效果问题。\n\n而如果使用 SaaS API的话，这个Token费用在短期内又受不了（在模型厂商没有大规模降价或者AutoCoder 没有提供专有流量的时候）。\n\n那怎么真正把 AutoCoder 给利用起来呢？\n笔者目前的最佳实践是，组合API和Web订阅两种方式，获得效果和成本的平衡。\n\n\n1. Web 版本的优势是按月订阅，对于Token使用量巨大的同学而言，性价比很高，而且他的效果也往往很不错。AutoCoder Token消耗量最大的环节就是生成代码的环节。\n2. API 版本对于 AutoCoder 主要应用于索引构建，HTML 正文抽取等小功能点，相对来说，Token消耗量小一些，但能让 AutoCoder自动化运转起来。\n\n基本思路就是：\n\n1. 给 AutoCoder 配置一个模型，用于索引构建，HTML 正文抽取等小功能点。\n2. 开启 human_as_model模式，使用 Web 版本的大模型，用于生成代码。\n\n具体用法，我们之前在 [human as model 模式](./003-%20AutoCoder%20使用Web版大模型，性感的Human%20As%20Model%20模式.md)有详细的使用介绍。\n\n此外，如果你好奇 AutoCoder 配置的模型主要都会被用在哪些地方，可以参考我们的之前的[番外篇](./007-%E7%95%AA%E5%A4%96%E7%AF%87%20AutoCoder%E9%87%8C%E9%85%8D%E7%BD%AE%E7%9A%84model%E7%A9%B6%E7%AB%9F%E7%94%A8%E6%9D%A5%E5%B9%B2%E5%98%9B.md)\n\n笔者本人的组合配置是：\n\n1. deepseek V2 API\n2. Claude3 Opus Web 订阅\n\n目前实测效果成本最佳。\n\n\n\n", "tag": "", "tokens": 480, "metadata": {}}], "modify_time": 1716937490.15978, "md5": "97cbbffe717ca05ade91545a6ac9da3f"}
